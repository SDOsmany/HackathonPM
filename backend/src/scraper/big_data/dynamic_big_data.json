[
    {
        "url": "https://devpost.com/software/smart-fridge-9d8qyv",
        "content": "Inspiration\n\n\nWe saw the brand new Samsung Family Hub smart fridge at the CES 2017, which require manual data log-in for the goods stored inside. We got inspired to create a smart fridge that can automatically log in what's inside the fridge, enable users to access the data remotely and have information recommended for the users based on what they have in the fridge.\n\n\nWhat it does\n\n\nThis is an IoT-based smart fridge that uses Computer Vision to automatically log in food, informs the users through text messages of what's stored inside and expiration data, and recommend healthier and better use of user's’ current storage through features like checking nutrition and search for recipes related to some items. \n\n\nHow we built it\n\n\nWe used a button on an Arduino board to emulate the action of “closing the fridge door”. The signal created by the button is sent to a PC through a serial COM port. When PC receives that signal, the kinect camera is triggered to capture a photo of the current status in the fridge. The photo is then compressed and sent to our web server. Our web server is coded on Python+Flask and deployed on Google App Engine Flexible Environment. This web server also contains some logics for responding to Twilio messages, which will be mentioned later. When the web server receives that photo, it puts the photo in Google Cloud Storage. It also keeps some basic image metadata in Google Cloud Datastore database. Then the Google Cloud Vision API is called to analyze the photo and label it by what the item is and which category it belongs to. The labels (coming out of cloud vision api) are then passed to Google KnowledgeGraph API to be further narrowed down to things people would normally put in a fridge. The results coming out of Google KnowledgeGraph are then stored in Google Cloud Datastore database. Now the fridge basically identifies the items that were put in it by automatically capturing and analyzing photos. Every time new items are added to the fridge, Twilio would send a notification through SMS to inform user\nUsers are also able to text Twilio some basic commands to:\n\n\n\n\nCheck what is currently in the fridge\n\n\nCheck which item is about to pass its expiration date\n\n\nCheck the nutrition of the food stored\n\n\nSearch for recipes related to some items\n\n\n\n\nChallenges we ran into\n\n\n1) Capture the kinect photo with the least noise and incorporated Arduino-based trigger for the photo \n\n\n2) Integrate the local image capture, python web server, google cloud platform, and twilio together and make them work flawlessly. Specifically, the challenges include the following:\n\n\n\n\nImage format conversion\n\n\nImage compression and processing\n\n\nHandling HTTP POST/GET requests between Local and web servers for images as well as web servers and twilio for sending and receiving texts\n\n\nCreate appropriate database structure to store images and item labels\n\n\n\n\n3) At first, it was really hard to pick the right label from about 10 labels returned by cloud vision api. We used KnowledgeGraph first to narraw the list down to 3-5 labels, and then manually process them according to how “general” or “specific” they are.\n\n\n4) There were some misleading parts in the documentation of cloud vision api in Python. The URI stated in the doc is not the correct format required by the actual function. We finally figured it out by looking into the C# version of that documentation.\n\n\nAccomplishments that we're proud of\n\n\nWe finished it early enough to write this :p\n\n\nWhat we learned\n\n\nLearned so much about technical stuffs and non-technical stuffs along the way of development\n\n\nWhat's next for Smart Fridge\n\n\nComputer Vision System \n\n\n\n\nBetter recognition of photos containing multiple items of different categories\n\n\nMore accurate and systematic labeling of new items\n\n\n\n\nData log-in/Request methods  \n\n\n\n\nUse speech recognition to log in data, complementary to Computer Vision \n\n\n A smarter twilio assistant capable of natural language processing\n\n\n\n\nData Utilization Features \n\n\n\n\nAutomatically refill necessity through Google Express\n\n\n\n\n\n\n\n\nBuilt With\n\n\narduino\nc++\ngoogle-app-engine\ngoogle-cloud\ngoogle-cloud-datastore\ngoogle-cloud-vision\ngoogle-knowledgegraph\ngoogle-ml\nkinect\npython\ntwilio\nwolfram-technologies\n\n\n\n\n\n\nTry it out\n\n\n\n\n\n\n\n\ngithub.com"
    },
    {
        "url": "https://devpost.com/software/hacksat",
        "content": "Top View\n\n\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n\nTesting battery circuit\n\n\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n\nSolar panel\n\n\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n\nBuilding\n\n\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n\nWire connections\n\n\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n\nProject bay\n\n\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n\nWorking team\n\n\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n\nWorking team\n\n\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n\nTesting wire connections\n\n\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n\nTesting battery module\n\n\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n\n\n\n\n\n\n\n \n\n\n\n\nWhat it does\n\n\nHackSat consists of a prototype for a \nCubeSat\n blueprint which will allow anyone who wants to do any experimentation up in outer space to avoid worrying about how to send data or how to provide energy and start thinking about which data will be sent and when they will sent it.\n\n\nIt is also worth noticing that everything will be released under an Open Source License\n\n\nHow we built it\n\n\nWe designed the basic structure, based on the \nCubeSat\n specs provided by \nCalifornia Polytechnic State University\n and used by NASA to send low cost satellites.\n\n\nWe printed the structure by means of a couple 3d printers.\n\n\nWe handcrafted all electronics by using a combination of 3 Arduinos, which required us to search for low consuming components, in order to maximize the battery power, we also work on minimize the energy consumption for the whole satellite.\n\n\nWe opted to use recycled components, like solar panels, cables, battery, converter...\n\n\nWe worked a lot on the data transfer part, so it allows the Sat to be sleeping by the most part, on an effort to increase even more the battery life.\n\n\nAnd almost 24hours of nonstop work and a lot of enthusiasm!!\n\n\nChallenges we ran into\n\n\nWe find mostly challenging the electronics, because our main objective was to get the optimal energy out of our battery and avoid draining it too fast.\n\n\nAnother point worth mentioning was the data transfer between the experiment section and the Sat section, because we wanted to isolate each part as much as possible from the other, so the experiment just need to tell the Sat to send the data and nothing more.\n\n\nAccomplishments that we are proud of\n\n\nWe are very proud to have accomplished the objective of making a viable prototype, even though we have faced some issues during these days, nonetheless we managed to overcome all of those issues and as a consequence we have grown wiser and our vision has become wider.\n\n\nWhat we learned\n\n\nDuring the development for HackSat, we have learned a lot about radio transmission, a huge lot about serial port and how to communicate data between 3 different micros, using 2 different protocols. \n\n\nWhat's next for HackSat\n\n\nThe first improvement that should be made is fix some issues we encountered with the measures of our designs, which have required some on site profiling.\n\n\nAnother obvious improvement is update the case so it is made of aluminium instead of plastic, which is the first blocking issue at the moment for HackSat to be launched.\n\n\nFinally, we would change the hardware so it has more dedicated hardware which most likely will allow us to optimise even the battery consumption and global lifespan for the Sat.\n\n\n\n\n\n\nBuilt With\n\n\n3dprinting\narduino\nc\nenthusiasm\nvirtual-wire\n\n\n\n\n\n\nTry it out\n\n\n\n\n\n\n\n\ngithub.com"
    },
    {
        "url": "https://devpost.com/software/handwashmonitor",
        "content": "Inspiration\n\n\nHand cleanliness is minimum basic requirement, largely not neglected but forgotten by many. Could be professionals in hospitals or Restaurants.\n\n\nWhat it does\n\n\nBased on the accelerometer and gyroscope values of smart watch, It ensures that the WHO standards for hand wash are performed, when person is coming out of unhygienic place or entering a place which should be maintained neatly.\n\n\nHow I built it\n\n\nI used my smart watch, collected data based on different gestures.\nUsed MATLAB to analyze the data, and get my numbers.\nThen integrated into my app, and with a tiny display on which gesture to do next, app is easy to use, and solves the purpose\n\n\nChallenges I ran into\n\n\nLOL!! I couldn't even get the sensor values into my FireBase for five hours, working on sensors for so long sounds terrible.(Restarting my watch did the work, It wasn't able to access WIFI I think.)\n\n\nFiltering data!!! Being a computer science student signals can be confusing at times to understand, yesterday was the perfect day for that. To achieve synchronization, and remove outliers was hectic.\n\n\nCalculating the thresholds, trying to use machine learning, and realizing my data wasn't filtered properly had to repeat the loop lot of times.\n\n\nAccomplishments that I'm proud of\n\n\nIt works!!! It does what it is supposed to. Recognizing all the 5 gestures felt good.\n\n\nWhat I learned\n\n\nIssues with not handling background services properly, debugging firebase issues. Filtering data. (Collecting swag and being awake)\n\n\nWhat's next for HandWashMonitor\n\n\nFine tuning needs to be done, dispensers have to synchronized with the watch, Different people might have different techniques to do. The co-ordinates have to be matched to ground in order to avoid orientation issues. \n\n\n\n\n\n\nBuilt With\n\n\nandroid-studio\nandroid-wear\nbluetooth\nmatlab"
    },
    {
        "url": "https://devpost.com/software/shoebotics",
        "content": "Inspiration\n\n\nWe wanted to incorporate our passion for autonomous robotics with wireless communication, serving the niche for travelling assistance in an increasingly  world.\n\n\nWhat it does\n\n\nCargo robot that connects to a radio transmitter attached to the user (currently integrated in a shoe), using sensor data and programmed logic to follow the user in potentially any environment.\n\n\nHow we built it\n\n\nWe built it in multiple modules, separating the data receiver (robot communications), hardware control (robot outputs), and data transmission (shoe). After each was developed and tested individually, we began the long - and somewhat tedious - process of integration.\n\n\nChallenges we ran into\n\n\nThe initial base processing system (Arduino 101) had built-in accelerometers and gyroscopic sensors, but no driver for our radio transceiver. When switching to a platform with driver support (Arduino Uno), we no longer had such sensors and thus had to make use of a much more limited array. After a poor circuitry connection, we had to design and build a hand-made motor driver to replace our recently Kentucky-fried circuitry.\n\n\nAccomplishments that we're proud of\n\n\nEstablishing transceiver communication between a wearable sensor device (a shoe) and a mobile robotic platform (a cardboard box), both Arduino-powered. Organized approach to testing and debugging various prototype phases. Discovery/learning of algorithms to maximize information obtained from limited sensor input.\n\n\nWhat we learned\n\n\nWe learned two significant lessons: how to ask more experienced individuals for assistance when we were stuck, and to check simple solutions before assuming complex problems (I got 99 problems and not rebooting is about 70 of them).\n\n\nWhat's next for shoebotics\n\n\nIntegration of additional sensor data, including (but not limited to) accelerometers, GPS, and gyros. Construction of a more durable and marketable frame for extended use. Attachment of physical container for cargo and luggage transport.\n\n\n\n\n\n\nBuilt With\n\n\narduino\nc++\nhome-made-motor-drivers\nnrf-transceivers\n\n\n\n\n\n\nTry it out\n\n\n\n\n\n\n\n\ngithub.com"
    },
    {
        "url": "https://devpost.com/software/notificationsapp",
        "content": "Interface\n\n\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n\nNotification Manager\n\n\n\n\n\n\n \n\n\n\n\nInspiration\n\n\nHas your phone ever annoyed you by ringing incessantly from a tirade of notifications you don't even care about that much? Annoying group messages from projects? \n\n\nWhat it does\n\n\nThis app will let you select messaging apps that you want to block notifications when the frequency of notifications exceeds 1 every 3 seconds, allowing you to receive messages without the annoying train of notifications.\n\n\nHow I built it\n\n\nUsed Android Studio, built in java.\n\n\nChallenges I ran into\n\n\nPulling the list of notifications and tracing with app the notification came from on a system-wide basis.\n\n\nAccomplishments that I'm proud of\n\n\nThis was our first project outside of class ever, and sucessfully creating the UI and having skeleton code that hashes out our idea really pushed the boundaries of our knowledge and abilities. We are proud of creating an app that increases the functionality and solves an everyday, yet frequently ignored probem. \n\n\nWhat I learned\n\n\nHow to write Android apps!\n\n\nWhat's next for NotificationsApp\n\n\nWe will refine the notification cancelling function, and adjust it to detect any group messaging app on the phone, including SMS texting and lesser-known group messaging apps so that this functionality can be extended to any user, regardless of what apps they use. \n\n\n\n\n\n\nBuilt With\n\n\njava\n\n\n\n\n\n\nTry it out\n\n\n\n\n\n\n\n\ngithub.com"
    },
    {
        "url": "https://devpost.com/software/resistar",
        "content": "Big Circuit_3\n\n\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n\nBig Circuit\n\n\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n\nBig Circuit_2\n\n\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n\nView For User without AR\n\n\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n\nSimple Circuit\n\n\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n\nSeries Circuit\n\n\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n\nParallel Circuit\n\n\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n\nParallel and Series Circuit\n\n\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n\nTaking a Break\n\n\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n\nEarly Model\n\n\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n\nTesting\n\n\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n\nTesting_2\n\n\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n\nPresentation Boards\n\n\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n\nEarly Concepts_3\n\n\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n\nEarly Models\n\n\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n\nPoster Boards\n\n\n\n\n\n\n \n\n\n\n\nCategories Won\n\n\nTartanHacks Grand Prize \n\nFacebook Company Prize\n\n\nAttempted Prize Categories\n\n\nDuolingo’s Social Impact Prize (Educational) \n\nGoDaddy’s Social Impact Prize (“Best app that improves STEM education”)\n\n\nInspiration\n\n\nLong hours spent on ECE problem sets and frustration visualizing convoluted circuits caused these four CMU undergrads to create a circuit visualization system that would also help them solve circuits.\nA member of the team is currently in the intro ECE course: \"Well it's not \nbad\n, I guess.\" - Team Member\n\n\nWhat it does\n\n\nResistAR is an Augmented Reality Circuit Visualizer and Solver. A user can place down circuit elements in parallel and series configurations and ResistAR will solve the current through and voltage across each element of the circuit. It gives the user an easy way to \nsee\n (sharp) the circuit.\n\n\nHow we built it\n\n\nWe first began with 3D printed chassis for the \nVuMark\n targets. These targets are identified and parsed by the program and cross checked against our cloud database on \nVuforia\n. \nWe then created 3D, textured, models in \nBlender\n that will hover over the VuMark targets.\nWe then wrote the code in \nUnity\n that will calculate voltage and current values using concepts from vector calculus and matrix algebra.\n\n\nChallenges we ran into\n\n\nThe math was very difficult and attempting to rush a 3D printed design was also difficult but there was a rush because 3D printing would be a very time consuming process. \nThus we also had to create a lot of our latter designs around the already 3D printed parts.\nVuMarks were also difficult to create. VuMarks must be very easily distinguishable from each other and non-symmetric along any axis, and therefore took a while to get finely tuned and calibrated.\nFinally the math was a very difficult thing to visualize. We had to go from 3D space to 2D space and there were some difficulties with projections. The coders did end up writing relatively bug-free code, but not before a long, arduous thinking process.\n\n\nAccomplishments that we're proud of\n\n\nThe two logic/algorithm gods that we had on our team solved an extremely complex math problem very quickly. \nAlso our 3D printed parts are actually fire though. Just saying.\n\n\nWhat we learned\n\n\nTwo 5 hour energies in 72 hours is actually not as bad an idea as some might think.\nMath is hard.\n\n\nWhat's next for ResistAR\n\n\nNorton and Thevenin Equivalents. Yikes.\n\n\n\n\n\n\nBuilt With\n\n\n3d-printer\nblender\nc#\nmatrix-algebra\nsolidworks\nunity\nvector-calculus\n\n\n\n\n\n\nTry it out\n\n\n\n\n\n\n\n\ngithub.com"
    },
    {
        "url": "https://devpost.com/software/twosidednews",
        "content": "Website View\n\n\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n\nChrome extension_01\n\n\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n\nChrome extension_02\n\n\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n\nChrome extension_03\n\n\n\n\n\n\n\n\n\n\n\n\n \nGIF\n\n\n\n\n\n\nMeet our team\n\n\n\n\n\n\n \n\n\n\n\nInspiration\n\n\nThe primary inspiration of our project is the growing filter bubbles in our country and in our world. The ability of people to only see posts on social media and news sites that agree with their point of view is a worrying development of the 21st century that we are trying to combat. Our project being two-fold (Chrome Extension and website) means that we can both provide an in depth overview of an issue for those who are actively curious about learning more about an issue through our website, and also passively prompt users with alternative interpretations of news stories for users who are not consistently conscious of filter bubbles.\n\n\nWhat it does\n\n\nTwo Sided News attempts to give two views on any story: liberal and conservative. By searching keywords to a topic the user would like to read about, our website displays articles side by side. The user can then choose to read whichever perspective they please, or both, and come up with their own interpretations of the story.\nThe chrome extension version allows you to directly look at another article from the opposite view. This way, users can continue to browse articles on websites that they are comfortable with, but have the option to read the another side of the story through this extension.\n\n\nHow we built it\n\n\nWe used HTML, CSS, and JavaScript (all the classic web development basics!) for creating our website. We hosted through firebase, and we also used JavaScript for the Chrome extension. The querying was handled with a custom google search engine that searched for articles from selected sources.\n\n\nChallenges we ran into\n\n\nWe all had to learn how to use Github for group version control, and had to teach each other web development basics. In addition, our collective experience with Chrome extensions was that one of our members had attended the Chrome extension Hack@Brown workshop. Yet we still managed to make our thing!\n\n\nAccomplishments that we're proud of\n\n\nWe are extremely happy to have been able to complete this project in less than a day. Our website lets users read online news articles, but we are especially proud that our product can allow them to become more informed about other perspectives. The portable version of our product, the Chrome extension, can even be used on the go for readers who don't use our website.\n\n\nWhat we learned\n\n\nWeb Development (HTML, CSS, jQuery, JavaScript, Bootstrap), Google Custom Search API, Chrome Extension Development, Hosting, Friendship\n\n\nWhat's next for TwoSidedNews\n\n\nFuture features would be to upgrade the news search. One way to do this is implement Machine Learning that will take in a database of articles and learn which articles are liberal or conservative leaning. This way, the article returned from a query would not be restricted to a particular news outlet. For example, if one news outlet has both left and right leanings, the same news outlet can be displayed on both sides.\nAnother feature would be a website format that can be its own sustainable news website. This would include tags like \"Popular\" or \"Health\" where the trending stories for each would be displayed instead of having to specifically querying for an article/subject. This would also include updating the design of the website to best accommodate users' experience.\n\n\nTry it out\n\n\nVisit the website \nhere\n\n\nCheck out the Chrome Extension in the Google Chrome Store \nhere\n\n\nSee the full project at \nthis GitHub repo\n\n\n\n\n\n\nBuilt With\n\n\ncss\ngoogle-cse-api\nhtml\njavascript\njquery"
    },
    {
        "url": "https://devpost.com/software/selfie-activism",
        "content": "Inspiration\n\n\nYoung people aren't communicating with their government and are not aware of political issues. We help Selfie Activism will inspire them to become more politically active.\n\n\nWhat it does\n\n\n\n\nPick an issue you're interested in\n\n\nTake a selfie for the postcard\n\n\nWrite a message on your stance on the issue\n\n\nUse your location to find your local representatives\n\n\nWe send your postcard (physical!) to them using the Lob API!\n\n\n\n\nHow we built it\n\n\nWe made it in Objective-C with Lob API to send the postcard. We scrape issues and their descriptions from 5calls.org\n\n\nWhat's next for Selfie Activism\n\n\nWe're launching a social campaign as well as improving the app."
    },
    {
        "url": "https://devpost.com/software/virtual-real-estate-agent",
        "content": "Refining your style with your answers\n\n\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n\nProposing apartments\n\n\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n\nBook it!\n\n\n\n\n\n\n \n\n\n\n\nInspiration\n\n\nMy friend and I needed to find an apartment in New York City during the Summer. We found it very difficult to look through multiple listing pages at once so we thought to make a bot to suggest apartments would be helpful. However, we did not stop there. We realized that we could also use Machine Learning so the bot would learn what we like and suggest better apartments. That is why we decided to do RealtyAI\n\n\nWhat it does\n\n\nIt is a facebook messenger bot that allows people to search through airbnb listings while learning what each user wants. By giving feedback to the bot, we learn your \ngeneral style\n and thus we are able to recommend the apartments that you are going to like, under your budget, in any city of the world :) We can also book the apartment for you.\n\n\nHow I built it\n\n\nOur app used a flask app as a backend and facebook messenger to communicate with the user. The facebook bot was powered by api.ai and the ML was done on the backend with sklearn's Naive Bayes Classifier.\n\n\nChallenges I ran into\n\n\nOur biggest challenge was using python's sql orm to store our data. In general, integrating the many libraries we used was quite challenging.\n\n\nThe next challenge we faced was time, our application was slow and timing out on multiple requests. So we implemented an in-memory cache of all the requests but most importantly we modified the design of the code to make it multi-threaded.\n\n\nAccomplishments that I'm proud of\n\n\nOur workflow was very effective. Using Heroku, every commit to master immediately deployed on the server saving us a lot of time. In addition, we all managed the repo well and had few merge conflicts. We all used a shared database on AWS RDS which saved us a lot of database scheme migration nightmares. \n\n\nWhat I learned\n\n\nWe learned how to use python in depth with integration with MySQL and Sklearn. We also discovered how to spawn a database with AWS. We also learned how to save classifiers to the database and reload them.\n\n\nWhat's next for Virtual Real Estate Agent\n\n\nIf we win hopefully someone will invest! Can be used by companies for automatic accommodations for people having interviews. But only by individuals how just want to find the best apartment for their own style!\n\n\n\n\n\n\nBuilt With\n\n\nchatbot\nmachine-learning\nnaive-bayes\npython\nsklearn\n\n\n\n\n\n\nTry it out\n\n\n\n\n\n\n\n\nbot.api.ai"
    },
    {
        "url": "https://devpost.com/software/giffiti",
        "content": "GIF\n\n\n\n\n\n\nDemo\n\n\n\n\n\n\n\n\n\n\n\n\n \nGIF\n\n\n\n\n\n\nWorking\n\n\n\n\n\n\n\n\n\n\n\n\n \nGIF\n\n\n\n\n\n\nBest GIF submission\n\n\n\n\n\n\n \n\n\n\n\nInspiration\n\n\nCreate an immersive way to experience and interact with gfycats in your environment.\n\n\nWhat it does\n\n\nGfycats in AR. Search for gfycats and place them in the environment. Geolocation added to gfycat so you can see gfycats placed in the same area by other users.\n\n\nHow we built it\n\n\nUnity, C#, Hololens Toolkit\n\n\nChallenges we ran into\n\n\nCurrent version of unity isn't friendly with streaming most video formats (OGG only). For live streaming of new gfycats we need either updates to unity to support more video formats, or OGVs from gfycat, or our own server to convert them on the fly. Lots of fine tuning necessary to keep the pictures in the interactable environment.\n\n\nAccomplishments that we're proud of\n\n\nSuccesfully placing gfycats into the environment in AR, moving them, and placing them on walls. \n\n\nWhat we learned\n\n\nPlaying with gifs in AR is fun and has a lot of potential for social interaction.\n\n\nWhat's next for Gfytti\n\n\nTweak search and stream process to be more fluid and add ability to browse and select from search results. Incorporate official tagging of geolocation on images that user chooses\n\n\n\n\n\n\nBuilt With\n\n\nc#\nmicrosoft-hololens\nunity\n\n\n\n\n\n\nTry it out\n\n\n\n\n\n\n\n\ngithub.com"
    },
    {
        "url": "https://devpost.com/software/hacktube",
        "content": "The Chrome popup for HackTube.\n\n\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n\nHackTube in effect. Notice the \"CENSORED\" counter at the top of the page.\n\n\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n\nA comparison between HackTube enabled and no HackTube.\n\n\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n\n\n\n\n\n\n\n \n\n\n\n\nInspiration\n\n\nYouTube is a place for millions of people to share their voices and engage with their communities. Unfortunately, the YouTube comments section is notorious for enabling anonymous users to post hateful and derogatory messages with the click of a button. These messages are purely meant to cause anger and depression without ever providing any constructive criticism. For YouTubers, this means seeing the degrading and mentally-harmful comments on their content, and for the YouTube community, this means reading negative and offensive comments on their favorite videos. As young adults who consume this online content, we feel as though it is necessary to have a tool that combats these comments to make YouTube a safer place.\n\n\nWhat it does\n\n\nHackTube automatically analyzes every YouTube video you watch, targeting comments which are degrading and offensive. It is constantly checking the page for hateful comments, so if the user loads more comments, the extension will pick those up. It then blocks comments which it deems damaging to the user, listing the total number of blocked comments at the top of the page. This process is all based on user preference, since the user chooses which types of comments (sexist, racist, homophobic, etc) they do not want to see. It is important to note that the user can disable the effects of the extension at any time. HackTube is not meant to censor constructive criticism; rather, it combats comments which are purely malicious in intent.\n\n\nHow we built it\n\n\nHackTube uses JavaScript to parse through every YouTube comment almost instantly, comparing its content to large arrays that we made which are full of words that are commonly used in hate speech. We chose our lists of words carefully to ensure that the extension would focus on injurious comments rather than helpful criticism. We used standard HTML and CSS to style the popup for the extension and the format of the censored comments.\n\n\nChallenges we ran into\n\n\nWe are trying to use cookies to create settings for the user which would be remembered even after the user closes the browser. That way anyone who uses HackTube will be able to choose exactly which types of comments they don't want to see and then have those preferences remembered by the extension. Unfortunately, Chrome blocks the use of cookies unless you use a special API, and we didn't have enough time to complete our implementation of that API at this hackathon.\n\n\nAccomplishments that we're proud of\n\n\nWe are proud of making a functional product that can not only fight online harassment and cyberbullying but also appeal to a wide variety of people. \n\n\nWhat we learned\n\n\nWe learned how to dynamically alter the source code of a webpage through a Chrome extension. We also learned just how many YouTube comments are full of hate and malicious intent.\n\n\nWhat's next for HackTube\n\n\nRight now, for demo purposes, HackTube merely changes the hateful comments into a red warning statement. In the future, HackTube will have an option to fully take out the malicious comment, so users’ YouTube comments feed will be free of any trace of hateful comments. Users won’t have to worry about how many comments were flagged and what they contained. Additionally, we will have a way for users to input their own words that offend them and take the comments that contain those words out of the section.\n\n\n\n\n\n\nBuilt With\n\n\nadobe-illustrator\ncss\nhtml\njavascript\njson\n\n\n\n\n\n\nTry it out\n\n\n\n\n\n\n\n\ngithub.com"
    },
    {
        "url": "https://devpost.com/software/go-alexa-go",
        "content": "Inspiration\n\n\nWe were inspired by Millennials' dangerous texting and driving habits, so we developed a driving system to allow them to text and still drive at the same time.\n\n\nWhat it does\n\n\nOur HTC Vive virtual reality experience allows the user to issue commands to our taxi driver, Alexa, and explore Sponsorville.\n\n\nHow we built it\n\n\nWe built our HTC Vive VR experience in Unity using C# and our Amazon backend with node.js and the Alexa skillset. The Amazon Alexa is able to take a user's directional input voice command through Amazon's unique browser-based web services built with node.js, and notifies Unity of the user's input with a web API hosted on Microsoft Azure.\n\n\nChallenges we ran into\n\n\nThe first challenge we ran to was division of work. Charlie became our Unity/C#/HTC-Vive programmer, Randy became our impromptu Scrum Master/Front-End Designer/3D-modeler, and Caleb and Colin worked on node.js/Azure-IoT/Amazon Web Services. After we had a better sense of everyone's skill-set and strengths, we were able to snowball each other consistently throughout the course of the hackathon. Regarding Unity and C#, we ran into rigidbody and trigger debugging issues early on. With Alexa, we had troubles getting the browser based web service to work with node.js/Azure but by the middle of the second day, we were able to create a working prototype.\n\n\nAccomplishments that we're proud of\n\n\nGetting an Amazon Alexa to take voice commands and convert them to directional output in a Unity VR environment.\n\n\nWhat we learned\n\n\nMake sure you go into a hackathon with your division of work ready between your teammates. Additionally, make sure you teammates actually have a solid background in coding the work that is handed to them. Get together with your teammates every few hours, AGILE style, and see what progress has been made and if anyone needs help. Make sure everyone on your team can at some point handle paperwork because there will be a good amount of it throughout the course of the hackathon from the gathering of your teammates, to the final 12 hours before showtime. There needs to be a HUGE sense of trust between you and your teammates. Without some form of solid workflow (we used 2-hour scrums), you can run into problems like people just going off and coding who knows what for 3-4 hours of your hackathon before you realize you have issues.\n\n\nWhat's next for Go Alexa Go\n\n\nWe plan on buying our own private islands and moving there with our solid-gold rocket ships from the amount of sponsorship money we've made from our amazing SponsorVille sponsors at Spartahack 2017.\n\n\n\n\n\n\nBuilt With\n\n\nadobe-dreamweaver\nadobe-illustrator\nagile\namazon-alexa\namazon-web-services\nasp.net\nazure\nc#\nhtc-vive\niot\njavascript\nnode.js\nphotoshop\nredbull\nunity\n\n\n\n\n\n\nTry it out\n\n\n\n\n\n\n\n\ngithub.com"
    },
    {
        "url": "https://devpost.com/software/sumantha",
        "content": "/catchup command\n\n\n\n\n\n\n \n\n\n\n\nTable: 11\n\n\nInspiration\n\n\nWe all love Slack, but sometimes in big teams it is impossible to keep up with all the messages.\n\n\nWhat it does\n\n\nSumantha works with a Slack command '/catchup' or by contacting her as with other bots. Sumantha is also accessible through a Echo Skill, which makes it really convenient to use.\n\n\nHow we built it\n\n\nWe are using a AWS Lambda function that gets POSTS requests from the Bot and answers back with the most important messages. In order to calculate the relevance of the messages, we take into account several parameters:\n\n\n\n\nNumber of reactions\n\n\nLength of the message\n\n\nMentions to the user and/or channel\n\n\nFrequency of the messages, to know which messages triggered a discussion\n\n\n\n\nChallenges we ran into\n\n\nDebugging a Lambda function and formatting data so the Echo is able to read it. Also, parsing the input for the Slack bot.\n\n\nAccomplishments that we're proud of\n\n\nDeveloping a working product of an idea with a useful outcome.\n\n\nWhat we learned\n\n\nUsing the Slack API, Bot SDK, Alexa SDK and AWS Lambda functions to create a voice application. We've also learnt to value which are the important factors in a conversation and how to gather these data.\n\n\nWhat's next for Sumantha\n\n\n*Improving the algorithm to make it work with high volumes of messages.\n*Being available in the Echo Store and as a Slack app. \n\n\n\n\n\n\nBuilt With\n\n\namazon-alexa\namazon-web-services\nlambda\nnode.js\n\n\n\n\n\n\nTry it out\n\n\n\n\n\n\n\n\ngithub.com"
    },
    {
        "url": "https://devpost.com/software/gbtlc",
        "content": "See how useful it is on a day-to-day basis\n\n\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n\nThe home menu\n\n\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n\nYou can even change user! (*feature coming soon)\n\n\n\n\n\n\n \n\n\n\n\nInspiration\n\n\nI love Game Boys! I own quite a lot of them, and I've been wanting to write a game for one for quite a while. I have this flash cartridge that lets you run whatever you want on the Game Boy.\n\n\nBecause I'm really bad at coming up with game ideas, I ended up making something 'useful' instead!\n\n\nWhat it does\n\n\nIt's a todo list that runs on a Game Boy! Because the Game Boy is such a versatile platform, this software is compatible with the original GB, the Color, and all models of the Advance.\n\n\nHow I built it\n\n\nI used the GBDK - it's a set of compilers and linkers for targeting the Game Boy (and other z80-like processors). The last release was in 2002, and even trying to compile it for Mac was a \nhuge\n pain. I eventually found some binaries that somebody had uploaded to a forum in 2006 and used them.\n\n\nGBDK has a load of helper functions for doing stuff on the Game Boy. The means rendering text and using cout is pretty simple! But it doesn't have enough helper functions to handle something like displaying a menu. I had to write that myself (and it's horrible).\n\n\nChallenges I ran into\n\n\nI had all kinds of problems with missing header files. This problem was compounded by the fact that \nI literally have no idea when it comes to C\n. The biggest blocker was not being able to clear the screen. The way all the >10 year old docs said it should be done just didn't work. It still doesn't work, and I have no idea why.\n\n\nAccomplishments that I'm proud of\n\n\nI got a working rom, and it runs on real hardware! (but not in an emulator for some reason)\n\n\nWhat I learned\n\n\nC isn't all that bad, but it is mostly. I'm motivated to keep learning it and get better, then eventually I'll work on a proper game!\n\n\nWhat's next for GBTLC\n\n\nI keep reading everywhere that the only way I'm truly going to be able to get fine control over what I want to do is with assembly programming. The docs and tutorials around asm on the Game Boy are actually really good - much better than with GBDK.\n\n\nI really just need to find the time to learn now!\n\n\n\n\n\n\nBuilt With\n\n\nc\ngameboy\ngbdk\n\n\n\n\n\n\nTry it out\n\n\n\n\n\n\n\n\ngithub.com"
    },
    {
        "url": "https://devpost.com/software/aeye-space",
        "content": "A Eye\n\n\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n\nUX Flow\n\n\n\n\n\n\n \n\n\n\n\nInspiration\n\n\nMy aunt has an eye disease called Glaucoma which damaged the optic nerves in her eyes. The damage has resulted in severe vision loss, meaning she struggles to find everyday items around her. With advancements in computer vision, it seemed reasonable that we could replace her eyes with an artificial one. \n\n\nWhat it does\n\n\nAEye is an artificial eye. Using a combination of the microphone and camera on the device, someone is able to ask where an object is and be guided to it. Moving around the room will result in a \"hot\" or \"cold\" reading. \n\n\nHow we built it\n\n\nSplit into two teams: 3 people concentrating on the mobile app & the UX design; 2 concentrating on the backend and visual recognition training.\n\n\nWe have trained multiple image classifiers on the Watson visual recognition service. This can give a reliable match when the image contains the object we're searching for. This service is wrapped by a Python web service, which maps the desired object class to the associated classifier. This is hosted on a Radix domain, \napi.aeye.space\n.\n\n\nThe iOS app uses the Watson Speech to Text service to translate voice input from user. Once the user selects a class to search for, we give aural feedback (\"warmer\", \"colder\", \"found it\", etc) as the user moves the phone's camera around the room or surface.\n\n\nChallenges we ran into\n\n\n\n\nWatson's free tier for classifying images is limited to 250 calls\n\n\nKeyword detection in the speech to text service\n\n\n\n\nAccomplishments that we're proud of\n\n\nGot it working early enough that we could go home for a few hours sleep!\n\n\nWhat we learned\n\n\nIt was our first time using the Watson APIs. There was a bit of a learning curve getting around the account structure and documentation.\n\n\nWhat's next for Aeye.space\n\n\nLook into celebrity voice expansion packs in order to monitize this product!\n\n\n\n\n\n\nBuilt With\n\n\nibm-watson\npython\nradix\nswift\n\n\n\n\n\n\nTry it out\n\n\n\n\n\n\n\n\ngithub.com\n\n\n\n\n\n\n\n\naeye.space"
    },
    {
        "url": "https://devpost.com/software/easydial",
        "content": "Alexa - Ask EasyDial to give DT a ring about my broadband...\n\n\n\n\n\n\n \n\n\n\n\nInspiration\n\n\nCalling customer support is painful, you have to navigate a menu then wait on hold for 30mins...  When all you want to do is go live your life without your phone to your ear!\n\n\nWhat it does\n\n\nWe created an Alexa Skill that calls a company's customer service team on your behalf.  It then chooses the correct menu option, waits on hold until finally you reach a human customer service agent and at that point forwards the call onto you.\n\n\nHow we built it\n\n\nAlexa connects to a AWS Lambda function, this makes a request to a Flask server.  This then uses Twillio to create a conference call, adding the company and our AI agent.  The agent then selects the appropriate option for the users query with IBM Watson's Speech to Text and Alchemy Language tools.  When a customer representative answers, the user is then connected!\n\n\nThe way we do it with Watson is to take Taxonomy, Sentiment and Keywords, which are then analysed and weighted to determine the best matching response.\n\n\nChallenges we ran into\n\n\n\n\nPhone calls have a surprisingly low sampling rate, this meant Watson had a hard time with the classification\n\n\nConference Calls with Twilio have a few nuances!\n\n\n\n\nAccomplishments that we're proud of\n\n\n\n\nAll of it coming together!  Was awesome to use three great technologies, really feels like the future\n\n\n\n\nWhat we learned\n\n\n\n\nThe Twillio API is pretty good!\n\n\nIBM Watson struggles to classify sentences with the word 'query'!\n\n\nWe know way more about the ins and outs of Alexa\n\n\nThere is no company that will get you through to a person on a Sunday in less than 60 seconds!\n\n\nIt is all to easy for someone to ask Alexa to buy you a PS4 for next day delivery...\n\n\n\n\nWhat's next for EasyDial\n\n\n\n\nExpanding to cover any customer service number, ie by looking up the number for you\n\n\n\n\n\n\n\n\nBuilt With\n\n\nalexa\naws-lambda\nibm-watson\npython\ntwilio\n\n\n\n\n\n\nTry it out\n\n\n\n\n\n\n\n\nbitbucket.org"
    },
    {
        "url": "https://devpost.com/software/popmidi",
        "content": "Overview of the machine\n\n\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n\nDetail of the dabbing machine\n\n\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n\nDetail of the servo layout\n\n\n\n\n\n\n \n\n\n\n\nPopMidi\n\n\nThis project is created during Cipher Major League Hacking \nLocal Hack Day 2016\n in Brampton, Ontario. This is awarded with \nBest Project with Mentor\n.\n\n\nInspiration\n\n\nWe essentially had three plans: a muchanical music box, a dab machine, and a water bottle flipper.\n\n\nSeeing and listening to the ever popular \"floppy disk music\" \nImperial March\n on YouTube inspired us to create our own mechanical music box.\n\n\nWhat it does\n\n\nOur hack is able to play music by hitting the cans.\nIt has 8 cans for 8 notes (C, D, E, F, G, A, B, C), and 8 servos to hit the cans.\nIn addition, it is able to take a MIDI file (a form of audio files) and play the notes.\nThat means you can import whatever music you and the machine will play it!\nIt also have a cool dabbing machine which dabs all the time.\n\n\nHow we built it\n\n\nMechanical\n\n\nThree things needed to be done for the mechanical part of the hack:\n\n\n\n\nStructure - Building the frame with the cans and the boxes.\n\n\nServos - Attaching 9 servos and their respective stick/coin arms.\n\n\nTuning - Filling the cans with water to tune them to their appropriate pitch.\n\n\n\n\nElectrical\n\n\nElectrical consisted mainly of wiring 9 servos to the Arduino board and mapping out the contraption.\nWe use Arduino UNO to control servos and 9V batteries.\n\n\nProgramming\n\n\nThere were two main parts to the programming:\n\n\n\n\nArduino to Servo - Outputting information the servos to the Arduino, fine tuning the parameters to hit the cans correctly.\n\n\nMIDI to Serial - Parsing MIDI files and sending them to the Arduino code as easy to read information in hitting the cans (done in Python).\n\n\n\n\nChallenges we ran into\n\n\nThere were many problems in the gathering of the materials, as we were initially planning to make an elastic band guitar but could not find strong elastic bands to pluck.\n\n\nThe serial communication between the Python code and the Arduino was not working properly and as such the servos were not hitting the cans in the correct order.\n\n\nWater was hard to get into the cans (without spilling), and it was extremely easy to overfill the cans.\n\n\nAccomplishments that we're proud of\n\n\nWe successfully communicate Arduino and a audio file from our laptop!\n\n\nWhat we learned\n\n\nOur project revolved mainly around using Arduino, something that some of us have never used before in the past!\nWe learned a lot about how to program/use hardware and how to connect software/hardware.\nWe also learned how to parse MIDI files into useful information.\nWe learned how to manipulate liquid and cans to make sounds.\nLastly, for some of us, it was our first hackathon! We had tons of fun!\n\n\nMentor and Mentee Relation\n\n\nFor the mentees, it was their first hackathon, and the entire event seemed fairly daunting. It is also their first hardware hack. Fortunately, the entire process was very smooth, with the mentor guiding the learning and hack creation very well. They successfully learned how to use microcontrollers and circuits, and was able to develop a successful project. \n\n\nFor the mentor, it was his first time mentoring others. He tries to guide the mentees to self-learn and The mentor learned a lot about communication skill, leadership, and project management. The mentor was really impressed by what the mentees have learned and created.\n\n\n\n\n\n\nBuilt With\n\n\narduino\ncardboard\niot\nmidi.js\npop-cans\npython\nservo\n\n\n\n\n\n\nTry it out\n\n\n\n\n\n\n\n\ngithub.com"
    },
    {
        "url": "https://devpost.com/software/get-well-holiday",
        "content": "Child View - Intro\n\n\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n\nChild View - Stroyline\n\n\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n\nChild View - Task\n\n\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n\nHospital Adventures Login\n\n\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n\nParent View - Dashboard\n\n\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n\nParent View - Live Surgery Updates\n\n\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n\nParent View - Packing List\n\n\n\n\n\n\n \n\n\n\n\nDemo\n\n\nInspiration\n\n\nA child's hospital experience is often frightening, exhausting and overall unpleasant. We change that by creating a unique storyline for a child's hospital stay.\n\n\nUsing the GE Adventure Series as inspiration, we wanted to imbed the awesome settings into a comprehensive use case. \n\n\nWhat it does\n\n\nOur application allows us to portray a child's hospital stay in a unique story, while simultaneously informing the parents about all necessities and the status of the treatment.\nParents and children are able to login into different views, using the child's unique patient ID.\n\n\nChild's view\n: The child receives a virtual guide through his journey, mapping the upcoming treatment steps to a fun storyline. By integrating the hospital's environment and equipment, the child looses its fear of upcoming procedures. An avatar accompanies the child on his journey and explains every step in a simple and visual manner.\n\n\nParents' view\n: By including the GE Opera API, parents receive live updates about the treatment progress and scheduled examinations.\nFurthermore, parents have a dashboard, allowing them to support the child during the whole affair, as well as being prepared for all eventualities.\nIn stress situations, things tend to get overlooked. By providing extensive checklists, the parents are able to focus on more important aspects.\n\n\nHow we built it\n\n\nUsing react-native, we simultaneously built one app for IOS and Android devices. It connects to our simulated GE Opera API, allowing us to display realtime status updates.\n\n\nChallenges we ran into\n\n\nHaving no example data or mocked API from GE Healthcare, we built an API simulation based on the data given.\nOn a non-technical level, we were forced to translate medical terms to easy-to-understand descriptions. The mapping of different treatments to colourful storylines required a lot of imagination and creativity.\n\n\nAccomplishments that we're proud of\n\n\nWe hope to make an impact in the lives of future patients and strengthening their trust in modern medicine.\nBy digitizing a major part of a hospital's overhead, we take stress from the parents and allow for a smoother workflow.\n\n\nWhat we learned\n\n\nA hospital does not have to be an exhausting, frustrating or even frightening experience. Many people have put a lot of thought into creating a patient friendly experience, especially for young people.\n\n\nWhat's next for Hospital Adventure\n\n\nSharpen our prototype and include more storylines for different treatment plans.\nDoing user studies in order to get feedback from our small patients.\n\n\n\n\n\n\nBuilt With\n\n\nge\njavascript\nlots-of-love\npython\nreact-native\nredux\nruby-on-rails\n\n\n\n\n\n\nTry it out\n\n\n\n\n\n\n\n\nappetize.io\n\n\n\n\n\n\n\n\ngithub.com"
    },
    {
        "url": "https://devpost.com/software/room-service",
        "content": "4 spades team\n\n\nInspiration\n\n\nSami and hotels, and Mihai and hotels, and Whitbread challenge.\n\n\nWhat it does\n\n\nYou can check in hotel without interacting with human being. You can do everything by speaking with Amazon Echo. Also you do not have to call or go downstairs to order room service. You can do it using text messages.\n\n\nHow we built it\n\n\nWe used Amazon Echo and AWS lambda to host Alexa skill. We also used Twilio API to handle text messages. \n\n\nChallenges we ran into\n\n\nAsynchronous JS and setup of Amazon Echo\n\n\nAccomplishments that we're proud of\n\n\nWe finished and it works!\n\n\nWhat's next for Room service\n\n\nAdd more available options and interactions, connect everything to hotel booking backend.\n\n\n\n\n\n\nBuilt With\n\n\nalexa\namazon-web-services\nclaudia.js\necho\ngithub\nlambda\nnode.js\ntwilio\n\n\n\n\n\n\nTry it out\n\n\n\n\n\n\n\n\ngithub.com"
    },
    {
        "url": "https://devpost.com/software/test-submission-d5ubit",
        "content": "Sales Pitch\n\n\nEver felt too lazy to pick up your stuff from across the room? Ever dreamt of having your own personal butler? Are you tired of robots having complex interfaces with scary buttons and crave something simpler?  Well, dream no more – the future is here! With the help of our eyeServant you can grab something and have it delivered to you with just a glance of an eye. Look at the object you need and then look again at the place where you want it delivered – and the eyeServant will do all your work for you. You will never need to leave your comfy couch again!\n\n\n\n\nInspiration\n\n\nWe were inspired by the eternal human laziness. Nothing to be ashamed of, we just say it as it is. Humans go the great lengths to save themselves some unwanted effort – and we aim to help them to do so. \n\n\nBut jokes aside, the main purpose of the solution is to help people with limited mobility in their everyday lives in a truly simple and unintimidating way.\n\n\nWhat it does\n\n\nThe goal was to control the Baxter robot with the Tobii EyeX eyetracker and make it move objects from one place to another. The user would look at the object and then look at the destination where the object should be placed – and consequently the robot would do all the work for the user and move the object.\n\n\nChallenges we ran into\n\n\nHowever, we ran into some challenges while trying to communicate with the robot. It turned out that the robot we were using came with the wrong firmware (the manufacturing firmware instead of the research firmware) and ultimately we could not control the robot via commands from the computer. \n\n\nHow we built it\n\n\nInstead we pre-programmed sequence of movements via the robot interface. Then we created the Unity program that reads the data from the eyetracker and checks what is the user looking at. If the user looks at one of the marked spots (marked specially in the Unity app for the purpose of the presentation), the signal is sent from the computer, via the Arduino board, to the Baxter robot, and as a result Baxter performs the next movement from the sequence. \n\n\nAccomplishments that we're proud of\n\n\nWe’re very proud of the effects of our work – even though we ran into some technical difficulties while connecting to the robot, we managed to find a workaround allowing us to create the demo of the system. Those dirty hacks are what you are usually the most proud of at the end of any hackaton. :)\n\n\nWhat we learned\n\n\nWe learned that if you see the biggest, the most expensive toy/hardware on the hackaton, always dare to ask to work with it – someone might just say yes. But seriously, we learned a lot about the Baxter robot (and about its different firmware variants) and how it can be programmed. Even though we didn’t get to actually write any software in ROS, there’s still a lot we learned about the Baxter and what are its capabilities.\n\n\nWhat's next for eyeServant\n\n\nWe already have the Unity program that properly reads user’s gaze and transforms it onto the 2D plane of the table. The next step would be to actually connect the program with other Baxter robot – the one that has research firmware preinstalled. Although, this might not happen for us in the nearest future. ;)\n\n\n\n\n\n\nBuilt With\n\n\narduino-uno\nbaxter-robot\ntobii-eyex\nunity\n\n\n\n\n\n\nTry it out\n\n\n\n\n\n\n\n\ngithub.com"
    },
    {
        "url": "https://devpost.com/software/intellisense",
        "content": "Inspiration\n\n\nPrescription labels are not accessible to everyone. Confusing the dosage, frequency, and timing of prescription drugs can result in unnecessary expenditure of resources, increased morbidity, and ultimately patient fatality. Current solutions are too expensive, bulky, or cumbersome to use. \n\n\nThere are more than 285 million visually impaired people around the world. While the idea was motivated by concern for blind and visually impaired patients, it has high potential for application to wider audiences. According to a study performed at University of College London, a third of the elderly population (age 65 and older) is in danger of premature death due to misunderstanding medicine labels. A study of low-income patients carried out at Northwestern University showed that nearly half of even visually able participants misinterpreted at least 1 out of 5 prescriptions labels presented. With the disparate education and literacy levels across the world, Pharmassist has the potential to improve the lives of billions of people. \n\n\nWhat it does\n\n\nPharmassist takes advantage of using low-cost conductive ink to print distinct patterns onto labels, which can be placed directly onto pill bottles. These patterns simulate multi-touch inputs and, in combination with our smartphone application, can be recognized to identify and access a patient’s specific prescription information, which will be presented audibly. Additional capabilities include being able to track a patient’s usage and offer on-demand services, such as options to refill prescriptions, request side effect information, and directly contact their pharmacist.\n\n\nHow we built it\n\n\nThe Pharmassist smartphone application was built using Java and Android Studio. We used conductive copper tape in place of conductive ink in the development of our prototypes. By using the tape to create different patterns on the bottoms of pill bottles, we were able to simulate recognizable touch patterns that could be identified by our application and used to relay specific prescription information. \n\n\nWhat's next for Pharmassist\n\n\nWith the advancement of mobile phone technology and the enhancement of multi-touch resolution, there will be endless possible conductive ink patterns for prescription drug labeling. Evolution of the mobile application will include features to increase patient interaction, lower noncompliance rates, and incentivize patients to follow their healthcare providers' instructions.\n\n\nMoving forward, we would like to optimize and construct a conductive ink label as a final product. In addition, we would like to integrate voice recognition for direct interactions and multilingual capabilities for increased accessibility and global application.\nUltimately, we would like to introduce our product to overseas markets to significantly improve prescription label accessibility to patients worldwide.\n\n\n\n\n\n\nBuilt With\n\n\nandroid-studio\njava\n\n\n\n\n\n\nTry it out\n\n\n\n\n\n\n\n\nbitbucket.org\n\n\n\n\n\n\n\n\ndocs.google.com"
    },
    {
        "url": "https://devpost.com/software/wind-power",
        "content": "Inspiration\n\n\nThe idea was to make a low cost wind tunnel that can be used in a wide variety of applications. This is especially beneficial to educational institutions who do not necessarily need a research level wind tunnel.\n\n\nWhat it does\n\n\nThis will measure wind speed, temperature, and humidity of the air in addition to allowing you to view the air around the object in question. \n\n\nHow we built it\n\n\nWe gathered our strong minds and found resources from various everyday objects around Georgetown. We constructed the actual tunnel from goldfish boxes. We used a motor and propeller to simulate wind. Cups with a magnet are used to detect wind speed.  \n\n\nChallenges we ran into\n\n\nA major challenge for us was getting the cups to spin on an axis.\n\n\nAccomplishments that we're proud of\n\n\nCompleting a functional wind tunnel was nice. The supplies for the tunnel cost under $50.\n\n\nWhat we learned\n\n\nHow to be resourceful.\n\n\nWhat's next for Wind Power\n\n\nPublish the project online so that science teachers around the world will be able to use our idea in their classrooms.\n\n\n\n\n\n\nBuilt With\n\n\narduino\ngoldfish\nheart\nlove\nnode.js\nraspberry-pi\nsoul\nwire"
    },
    {
        "url": "https://devpost.com/software/selfie-stick-golf",
        "content": "Forrest playing golf\n\n\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n\nFiltering phone gyro and accelerometer data\n\n\n\n\n\n\n \n\n\n\n\nInspiration\n\n\nSelfie stick looks like a golf club\n\n\nWhat it does\n\n\nconnects your phone to your computer so you can use it as a controller for a golf video game\n\n\nHow I built it\n\n\nphone talks to crossbar server talks to python server talks back to crossbar talks to web app for display\n\n\nChallenges I ran into\n\n\ncompass on phone did not work, had to integrate gyro measurements over time\n\n\nAccomplishments that I'm proud of\n\n\ngot a hole in one\n\n\nWhat I learned\n\n\nhow to write code that talks fast over a network\n\n\nWhat's next for Selfie Stick Golf\n\n\nwindmills?\n\n\n\n\n\n\nBuilt With\n\n\njavascript\nphonegap\npython\n\n\n\n\n\n\nTry it out\n\n\n\n\n\n\n\n\ngithub.com"
    },
    {
        "url": "https://devpost.com/software/hack-the-accessibility-pgfc9a",
        "content": "Cover\n\n\n\n\n\n\n \n\n\n\n\nInspiration\n\n\nWe were looking for inspiration and someone said \"do something a CEO can use\". We decided to create a fintech app, using CapitalOne and BlackRock APIs to generate the best three options to invest, looking at different ratios and returns (peRatio, pbRatio, returnonEquity, returns from year 1 and year 5). Furthermore, we wanted to make it more useful, so, we had the idea to implement this project for projects with disabilities. \n\n\nWhat it does\n\n\nUsing American Sign Language, you can ask Alexa to do different tasks for you: get your account balance, make payments for you, generate the first three best investment options and interact with Alexa during executing those tasks. \n\n\nHow we built it\n\n\n\n\nLeap-Motion: we created a machine learning program, which \"reads\" the signs and generate a string, which is sent to Alexa\n\n\nCapitalOne: we used their API to access a set of accounts, in order to make a payments and to generate the balance for the user's account \n\n\nAlladin: we used their API to access different investment portfolio and through an econometrics model, it returns your best three options for investment \n\n\nAlexa: we integrate everything as an Alexa's skill \n\n\n\n\nChallenges we ran into\n\n\nWe had some problems Amazon Web Services and we had to get use with their platform, in order to learn how to program Alexa. Furthermore, AWS Lambda did not work. \n\n\nAccomplishments that we're proud of\n\n\nWe learnt node.js! Furthermore, one of our team member was at his first Hackathon and another one is with a non-programming background, so, we are proud of the whole program! \n\n\nWhat's next for Hack the Accessibility\n\n\nDevelop the program, add more financial features and signs. \n\n\n\n\n\n\nBuilt With\n\n\nalexa\nalladin\njavascript\nleap-motion\nnessi\nnode.js\n\n\n\n\n\n\nTry it out\n\n\n\n\n\n\n\n\ngithub.com"
    },
    {
        "url": "https://devpost.com/software/project-firefly",
        "content": "Inspiration\n\n\nLight Painting\n is an photographic technique which is made by taking a long exposure photograph on moving handheld sources of light. There are several problems with this approach:\n\n\n\n\nThere is no feedback system. The artist is unable to see exactly what they're drawing\n\n\nThere is a limited amount of time to take a long exposure shot, and on top of that there are no room for mistakes\n\n\nIt's a difficult technique; creating complex art requires practice and skill\n\n\nAll artwork made with conventional light painting techniques seem hand-drawn. It looks like someone just drew their artwork with a marker\n\n\n\n\nWhat it does\n\n\nSo in traditional light painting the artist just sets the camera to long exposure and waves a flashlight around at it to create their artwork. So, what if you had 288 LEDs on a two meter long stick? One can upload an image to this stick, then hold it upright, and as they move it through 3D space, the stick scans through the image and shoots out each column of pixels in light. It's essentially a 3D light printer.\n\n\nHow We built it\n\n\nWe used a Raspberry Pi 3, a two-meter-long strip of 288-LED tape we soldered together, a breadboard, and various electrical components for our circuits. The Raspberry Pi intakes images through a USB drive and processes them with code in Python and C. \n\n\nChallenges We ran into\n\n\nDue to the nature of the hack and how we were building custom circuits for it, we were not able to afford any mistakes in our circuitry because that could potentially damage the Raspberry Pi or the LED tape or any other of the irreplaceable components we used. A lot of these components are expensive and thus any error could be very very very very very catastrophic. As a result, precision soldering and an insane amount of testing/sanity-checking was of upmost importance. We would test a circuit a couple times before soldering/assembling, then test it a couple times while soldering, and then test it three times after soldering.\n\n\nAccomplishments that we're proud of\n\n\nIt works\n\n\nWhat I learned\n\n\nOne of our member who rarely do hardware projects learned about connecting circuits, how to use a soldering iron, and how to connect hardware to the raspberry Pi. As a team we also learned how to do hardware unit and integration testing for hardware.\n\n\nWhat's next for Project Firefly\n\n\nWe are thinking of using a bigger stick or smaller LEDs for higher resolution picture. We are also thinking of mounting a small camera on the stick to record video while taking pictures.\n\n\n\n\n\n\nBuilt With\n\n\ncamera\nled\npython\nraspberry-pi"
    },
    {
        "url": "https://devpost.com/software/braille-printer",
        "content": "Picture of our Braille Printer\n\n\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n\nDiagram of how each solenoid is wired\n\n\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n\nDiagram of how the servo is wired\n\n\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n\nA page from the printer reading \"pork\". The punctures have been circled.\n\n\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n\nThe braille alphabet/numbers for reference.\n\n\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n\nA work in progress of the printer GUI.\n\n\n\n\n\n\n\n\n\n\n\n\n \nGIF\n\n\n\n\n\n\nA close up of the printer's solenoids.\n\n\n\n\n\n\n \n\n\n\n\nInspiration\n\n\nSmall scale braille printers cost between $1800 and $5000.  We think that this is too much money to spend for simple communication and it has acted as a barrier for many blind people for a long time.  We plan to change this by offering a quick, affordable, precise solution to this problem.\n\n\nWhat it does\n\n\nThis machine will allow you to type a string (word) on a keyboard.  The raspberry pi then identifies what was entered and then controls the solenoids and servo to pierce the paper. The solenoids do the \"printing\" while the servo moves the paper.\n\n\nA close-up video of the solenoids running: \nhttps://www.youtube.com/watch?v=-jSG96Br3b4\n\n\nHow we built it\n\n\nUsing a raspberry pi B+, we created a script in python that would recognize all keyboard characters (inputted as a string) and output the corresponding Braille code. The raspberry pi is connected to 4 circuits with transistors, diodes and solenoids/servo motor.  These circuits control the how the paper is punctured (printed) and moved.\n\n\nThe hardware we used was: 4x 1n4004 diodes, 3 ROB-11015 solenoids, 4 TIP102 transistors, a Raspberry Pi B+, Solarbotic's GM4 servo motor, its wheel attachment, a cork board, and a bunch of Lego.\n\n\nChallenges we ran into\n\n\nThe project initially had many hardware/physical problems which caused errors while trying to print braille. The solenoids were required to be in a specific place in order for it to pierce paper. If the angle was incorrect, the pins would break off or the paper stuck to them. We also found that the paper would jam if there were no paper guards to hold the paper down. \n\n\nAccomplishments that we are proud of\n\n\nWe are proud of being able to integrate hardware and software into our project. Despite being unfamiliar with any of the technologies, we were able to learn quickly and create a fun project that will make a difference in the world. \n\n\nWhat we learned\n\n\nNone of us had any knowledge of python, raspberry pi, or how solenoids functioned. Now that we have done this project, we are much more comfortable in working with these things.\n\n\nWhat's next for Braille Printer\n\n\nWe were only able to get one servo motor which meant we could only move paper in one direction. We would like to use another servo in the future to be able to print across a whole page.\n\n\n\n\n\n\nBuilt With\n\n\nblood\nhardware\npython\nraspberry-pi\nsweat\ntears\n\n\n\n\n\n\nTry it out\n\n\n\n\n\n\n\n\ngithub.com"
    },
    {
        "url": "https://devpost.com/software/imashare-7zv4hn",
        "content": "Image selection\n\n\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n\nBlur detection\n\n\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n\nOrientation detection\n\n\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n\nBad expossition detection\n\n\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n\nNoise detection\n\n\n\n\n\n\n \n\n\n\n\nInspiration\n\n\nWhen you return from a trip, you usually have a lot of repeated photos. There are lots of blurry photos, noise ones, and even some of them have awful exposition. Selecting the best ones is a very time consuming task, so we wanted to solve that issue.\n\n\nWhat it does\n\n\nIt's a computer software that detects similar images from a input set and it selects the best ones automatically.\n\n\nHow I built it\n\n\nThe application is built with matlab and it uses different image processing technics such as filtering, frequency transformations, hierarchical clustering and colour feature extraction.\n\n\nI build 4 algorithms to detect the independent features of a bad quality image such as blur, noise, bad exposition and desaligments.  The main application detects similar images and discards the worse ones based on the features above.\n\n\nChallenges I ran into\n\n\nThe design of every feature was a big challenge as it required some advanced signal processing concepts. The clustering application was also a bit tricky because it required some algorithms to process every cluster individually.\n\n\nAccomplishments that I'm proud of\n\n\nThe performance of the individual algorithms, they run very well.\n\n\nWhat I learned\n\n\nI learned new different ways to retrieve important information from an image, and expanded my clustering algorithms knowledge in matlab.\n\n\nWhat's next for ImaShare\n\n\nA mobile app build in react-native is in the way, with the intention of using the algorithm with the purpose of facilitating the image sharing. The app would select automatically the best photos of an event and then share it with the selected people. The app will have integration with social platforms like Facebook.\n\n\n\n\n\n\nBuilt With\n\n\nmatlab\n\n\n\n\n\n\nTry it out\n\n\n\n\n\n\n\n\nmega.nz"
    },
    {
        "url": "https://devpost.com/software/surfshield",
        "content": "Here is a screenshot of the chrome extension with the four scores, the overall score, and with a yellow-green background since it's  2.3/5.\n\n\n\n\n\n\n \n\n\n\n\nInspiration\n\n\nOur inspiration for SurfShield was the Hack Harassment initiative. The internet can be very useful, but due to its anonymity, it can also be used for cyberbullying and for posting vulgar and offensive content. We wanted to develop a way to let online users know whether the content of the page they are visiting is considered offensive. To do this effectively, we incorporated various apis and aggregated metrics that help to better inform the user.\n\n\nWhat it does\n\n\nSurfShield is a chrome extension. When a user visits a site, the SurfShield icon will change colors to display the level of offensive content on the page with green representing little to no offensive content and red representing a high level of offensive content. When the user clicks on the SurfShield icon, the extension will display an overall score, which is between 1 and 5. This score is averaged from from our four main metrics, which are anger, cyberbullying, profanity, and if users have voted on that particular site, an audience score. \n\n\nHow we built it\n\n\nWe built SurfShield by creating a Chrome Extension using javascript, html5, and css3. The API calls are made in Python. Using Amazon Web Services, we process the results and then update the UI of the chrome extension with the anger, cyberbullying, profanity, audience score, and overall score. The SurfShield icon also changes colors based on the overall score. \n\n\nChallenges we ran into\n\n\nOne main challenge we had was determining where the API calls would be made and the results processed. It was also challenging to integrate the separate parts that we all worked on to create a functioning Chrome Extension. \n\n\nAccomplishments that we're proud of\n\n\nWe're proud that we've created an extension that will help those who are worried about viewing offensive content on the web. We imagine that many parents would like to have this extension for Chrome, so that there kids can surf the web in a safer environment. The extension is useful for anyone that wants a better and safer experience on the web. \n\n\nWhat we learned\n\n\nWe learned a lot about the Watson Tone Analyzer as well as the Bark API, which greatly assisted us in our goal of determining the level of offensive content on a website. We all collaboratively worked together and used our own knowledge to make the Chrome Extension.  \n\n\nWhat's next for SurfShield\n\n\nSurfShield has great potential. We believe that in the future, it could allow parents to set it up so that if a site has a score above a threshold, it will block that site from use, so that children can surf the web without their parents having to look over their shoulder. In this case, the service could be monetized and sold on a monthly subscription basis. \n\n\n\n\n\n\nBuilt With\n\n\namazon-ec2\namazon-rds-relational-database-service\namazon-route-53\namazon-web-services\naws-elastic-beanstalk\nbark.us-api\ncss3\nflask\nhtml5\njavascript\npip\npython\nvirtualenv\nwatson-tone-analyzer\n\n\n\n\n\n\nTry it out\n\n\n\n\n\n\n\n\ngithub.com"
    },
    {
        "url": "https://devpost.com/software/trojan-automated-diabetic-retinopathy-detection",
        "content": "Automated  Diabetic Retinopathy Detection\n\n\n\n\n\n\n \n\n\n\n\nInspiration\n\n\nDiabetic retinopathy (DR) is a disease with an increasing prevalence and the main cause of blindness among working-age population. The risk of severe vision loss can be significantly reduced by timely diagnosis and treatment.\nThe current process of testing for diabetic retinopathy is laborious and often inefficient and the scope of detecting the diseases in the early stages are completely unexploited. There exists a need for a powerful Automated retinal image analysis for early stage Diabetic retinopathy detection\n\n\nWhat it does\n\n\nThe tool analyzes the retinal image of a patient to detect whether the patient is affected with Diabetic Retinopathy and if yes, how severe (on a medical scale of 1 to 4; 4 being highest). It is the ultimate tool to aid in precisely detecting the early stages of diabetic retinopathy. .\n\n\nHow I built it\n\n\nThe entire application was developed using the Predix infrastructure . The application was build in different layers UI using JavaScript  angular js, html  , Postgre sql database, Data science  algorithm  in python.  We leveraged powerful deep learning algorithms inspired from the way the humans learn.  Convolutional Neural Networks were leveraged to learn from retinal images of several hundred of different severity levels .\n\n\nChallenges I ran into\n\n\nThe Training  process for the deep learning algorithms were extremely resource intensive, our codes kept running for over 15 hours with no intermediate outputs from predix analytics runtime  . With limited visibility into the analytics processing and time we  ran in to challenges of optimizing  the resource intense algorithms. Our attempt to improve computational efficiency by increasing instances and RAM was also not  successful . \n\n\nAccomplishments that I'm proud of\n\n\nMaking  end to end predix  Micro application In the short time  using  various predix components such as \nPostgreql,Predix seed application(UI),Predix UAA,Predix analytics catalog, Predix Analytics Runtime . \ndeploying in to the Predix platform. Building the deep learning algorithm for the computer vision  in a day .\nFalling early and finding out alternatives quickly to achieve the goal .\n\n\nWhat I learned\n\n\nHow to plan end execute the things in short cycles . How to cut the corners and still make wonderful applications in 24 hours .  How to work collaboratively to get things done as team . Constant motivation and  energy to achieve the goal .\n\n\nWhat's next for Trojan - Automated Diabetic Retinopathy Detection\n\n\nEnhancing the algorithm as a real-time service using  Analytics micro service to be monetized in the predix platform .\nImproving the algorithm into self-learning algorithms that can be completely automated and improved incrementally without human interventionExtending the scope to other industries like Manufacturing to learn to predict and detect faults. Eg - detecting  manufacturing defects using real-time image analysis\n\n\n\n\n\n\nBuilt With\n\n\nangular.js\ncloud-foundry\ncss\nflask\ngrunt.js\nhtml\njavascript\njquery\njson\nmachine-learning\nnode.js\npostgresql\npredix\npython\n\n\n\n\n\n\nTry it out\n\n\n\n\n\n\n\n\nautomated-diabetic-retinopathy-detection.run.aws-usw02-pr.ice.predix.io"
    },
    {
        "url": "https://devpost.com/software/bob-s-ramen",
        "content": "App Interface\n\n\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n\nApp Interface\n\n\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n\nApp Interface\n\n\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n\nHot plate switch\n\n\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n\nSpice dispenser\n\n\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n\nRamen dispenser\n\n\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n\nMicrocontroller\n\n\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n\nFinal Product\n\n\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n\nWater Mechanism\n\n\n\n\n\n\n \n\n\n\n\nInspiration\n\n\nSometimes in college, you don't have time to cook, and you want food ready for you when you get home.\n\n\nWhat it does\n\n\nBOB'S Ramen is a combination of an iPhone application and and automatic ramen cooker. Through the app, a user can specify a specific time for ramen to be cooked and a spice level and the ramen cooker prepares it completely autonomously.\n\n\nHow we built it\n\n\nThe iOS application was built using swift in Xcode, and the ramen cooker was built using the NodeMCU wireless micro controller, Arduino IDE, servo motors, cardboard, duct tape, innovation, grit, and twelve cups of coffee.\n\n\nChallenges we ran into\n\n\nMany. The valves for water we purchased weren't working, the NodeMCU couldn't connect to the CMU-Secure wifi, we ran out of epoxy glue, and much more.\n\n\nAccomplishments that we're proud of\n\n\nWe managed to make an automatic ramen cooker and iOS app in 24 hours. We did not eat too much ramen.\n\n\nWhat we learned\n\n\nWe learned a lot in swift, hardware, arduino, networking, teamwork, and everything we did.\n\n\nWhat's next for Bob's Ramen\n\n\nBringing smiles and full stomachs to college kids all around the world.\n\n\n\n\n\n\nBuilt With\n\n\narduino\ncardboard\nduct-tape\ninnovation\nmsg\nswift"
    },
    {
        "url": "https://devpost.com/software/project-open-heart",
        "content": "Inspiration\n\n\nDoctors, Students, and upcoming surgeons are striving to better themselves in private practice or for general surgical tasks. Wether for academic or industrial reasons, upcoming surgeons have barely any information to look at when learning how to do a specific surgery. We made Project Open Heart to provide an opportunity to develop a better understanding of what techniques an methods to use in a surgery and ultimately lock in the concepts/surgical steps through VR.\n\n\nWhat it does\n\n\nWe provide a full system to a legitimate surgery. The first part of the system is our \"Nurse,\" Alexa. Alexa helps us retrieve specific tools necessary for our surgery. Through simple requests such as, \"bring me the scalpel,\" Alexa responds by adding a scalpel to your hand in the virtual reality atmosphere. The second part is connecting with the oculus and actually being hands-on with the entire surgery experience. We provide recordings of the surgery so other people can analyze mistakes and show others how the specific surgery is conducted. There is also the feature to record your actions and save them to a file. This file can then be played back at a later date for you or others to see how you performed and what you did. Lastly, you can stream your surgery to any others who want to watch it with our setup. Using firebase, we can sync data between the host and the clients with minimal effort.\n\n\nHow we built it\n\n\nThe system has two parts: an Alexa app and a Unity app. The Alexa app leverages the firebase api, amazon lambda, and the Alexa API. Once a certain task is called, we call a function in our Alexa app to send an update to our Firebase Database. This allows for live communication between both the Nurse and Surgeon. Now moving over to the surgeon-side. We use Oculus Rift to allow a full hands on experience of an open heart surgery we used 3d models to show a human body: including skin, flesh, bone, and organs. We attached a Leap Motion to the oculus to allow us to track the hand movements when performing a certain gesture in the surgery. When eventually synced with Firebase, we allow for more accessibility of the surgery performed and that is also done through the Unity application.\n\n\nChallenges we ran into\n\n\nSending http requests through alexa was a pain, huge shout out to the Google employees helping us out through that. Acquiring 3d models to use was not an easy task, most of them are extremely expensive. \n\n\nAccomplishments that we're proud of\n\n\nWe were excited when we knew that we had a cohesive entire system, from recordings to surgical simulations, rather than just building one part. We were also proud of creating something that we both know will be used one day. There has been a problem for centuries of knowledge between hospitals and surgeons not being sent to each other. Project Open Heart, bridges the gap and truly allows others to learn about the surgery and practice it hands-on\n\n\nWhat we learned\n\n\nWe learned how to program for Alexa, though it was a complete pain. We had to learn how to also efficiently utilize our threads in the Unity app. Pushing data from Alexa to firebase was also a great thing we learned and debugged to get right.\n\n\nWhat's next for Project Open Heart\n\n\nWe want to implement a machine learning layer on top of the Unity application to allow for robust completion of the surgery without humans. We also want to leverage the fact that this application is very unique. Since this project expands to a vast amount of market, we would love to take advantage of that and present this application accordingly. We also want to provide more robust Alexa features such as \"What's the next step\" or \"How does _____ surgeon do this.\"\n\n\n\n\n\n\nBuilt With\n\n\namazon-alexa\nc#\njavascript\nnode.js\nunity\n\n\n\n\n\n\nTry it out\n\n\n\n\n\n\n\n\ngithub.com"
    },
    {
        "url": "https://devpost.com/software/theia-tsmj24",
        "content": "Inspiration\n\n\nProviding low cost and eyecare in third world countries can be expensive and require professionals. We want to enable as many people to receive eye care even if they can't afford it or do not have access to an optometry test.. \n\n\nWhat it does\n\n\nt walks a user through an vision acuity exam and provides an approximate prescription for corrective lenses.\n\n\nHow we built it\n\n\nWe developed an iOS application which takes the user through the test, as well as a cardboard housing that holds the phone in the correct lighting and distance. The box is simple to make and can be built by anyone simply with scrap cardboard, scissors, and a ruler (the template is available on our site).\n\n\nChallenges we ran into\n\n\nWith zero knowledge of optometry prior to development, the main challenge was understanding how visual acuity tests correlate with spherical measurements of the eye (prescription numbers). From there, we had to convert all sizes of the test to correctly display 20 inches from the user rather than 20 feet (the distance required for a Snellen test). We converted all of the numbers to find a correlation, drew out each test to avoid pixel blurring, and developed the test on the application, running through the test and determining how mistakes and correct answers correlated with determining a user's prescription.\n\n\nAccomplishments that we're proud of\n\n\nWe did it. Through demos with random users, our application had +-.05 margin of error.\n\n\nWhat we learned\n\n\nOptometry. Years worth of Ophthalmology in two days. Please, feel free to ask us about eyes and how they work. We have too much knowledge about it now.\n\n\nWhat's next for Theia\n\n\nFind funding, increase accuracy with more tests, work for astigmatism and other prescriable eye problems, find a distributor, and ship it.\n\n\nVC funding baby!\n\n\n\n\n\n\nBuilt With\n\n\nios"
    },
    {
        "url": "https://devpost.com/software/pepperpay",
        "content": "Inspiration\n\n\nA few nights ago, as Dave was wondering home after a late-night hack session at the office, he wanted to stop by Walgreens to pick up a fresh tube of toothpaste. He picked up one single item from their easily laid out shelves and got in line. His stay was only beginning. After two arguments between patrons and cashiers and a very slow person, Dave finally walked out of the store 30 minutes after he arrived. He knew there had to be a better way to purchase simple goods. It's 2016 for god-sakes!\n\n\nFor years, stores such as Target, Banana Republic, and CVS have been pouring millions of dollars into research about ways to better layout their stores. However, there is one common problem that has yet to be solved: the lag experienced when it comes time to check out.\n\n\nWhat it does\n\n\nPepperPay makes checking out of your favorite store easier. It starts at the store, by being asked if you need any help with your store visit. It then allows you to buy the goods you've picked out, without having to deal with a cashier. All you have to do is hold up each item in front of the camera. It will then use image recognition (courtesy of IBM Watson) to figure out which item it is. It will then match that against a database to let you know the price. Without even taking out your wallet, you can then press a button to log into PayPal and pay. This process reduces the average checkout time by over 50%. Built with love by Team \nCroissant\n.\n\n\nHow we built it\n\n\nWe started by designing a simple flow for checking out different items. We then built a customized web page for the tablet display on the Pepper robot (courtesy of SoftBank). After learning the different gesture and other functionality using Choregraphe, we decided to go with the JavaScript wrapper for the Python API. This enabled us to quickly iterate on different robot control ideas. While that was being worked on, Adam tapped into the Watson Image Recognition API for taking the pictures from Pepper and converting them to specific items that we could put a price on. Lastly, we integrated PayPal via Braintree into the web page and back end, for completing the transaction.\n\n\nChallenges we ran into\n\n\nOne major challenge was the fact that the team had little to no robotics experience before this. We had to interface with SoftBank representatives who quickly and efficiently helped us learn the tools for the job. A big challenge with the tablet web page was that is was opened as a single file in the tablet web browser. This presented an issue with PayPal, which requires a hosted web page in order to open up the pop-up payment checkout window. We ended up adding a simple button on the front end to bypass this issue.\n\n\nAccomplishments that we're proud of\n\n\nWe're proud of what we've managed to make this robot do in such a short time, and of having worked with so many cool APIs in the process. Getting the arms to move and the robot to roll forward, backward, and in circles was the \"ah ha\" moment that really got us excited.\n\n\nWhat we learned\n\n\nWe now know the difference between \"yaw\", \"pitch\", and \"roll\"! We also know about the challenges behind getting a robot to do what you want. The latest robotics technology today is kin to the latest mobile tech of ~10 years ago. It is only a matter of time before it improves and becomes omnipresent.\n\n\nWhat's next for PepperPay\n\n\nWe don't know too much about the retail game, but we imagine a future where something like this changes how we shop. We invite any and all to take a look at our code on github and see where they can take it!\n\n\n\n\n\n\nBuilt With\n\n\nangular.js\nbraintree\nibm-watson\njavascript\nnode.js\npaypal\npepper\nsoftbank\n\n\n\n\n\n\nTry it out\n\n\n\n\n\n\n\n\ngithub.com"
    },
    {
        "url": "https://devpost.com/software/hinder",
        "content": "Hinder\n\n\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n\nRejected\n\n\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n\nMatches\n\n\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n\nHouse listing\n\n\n\n\n\n\n \n\n\n\n\nInspiration\n\n\nThe Housing Bubble Crash in 2008 impacted many Americans who did not know what houses they could afford. We wanted a fun way for people to browse through houses but also learn what houses they can buy within their means.\n\n\nWhat it does\n\n\nThe app is like Tinder. Swipe right or left to like or dislike a house. For the houses you like, you can see which houses you have matched with and have been rejected by. For your rejections, you can see why you did not qualify for a house, based on your inputted income and credit score.\n\n\nHow I built it\n\n\nWe used a third-party library to build swipeable cards and Firebase to store our data.\n\n\nChallenges I ran into\n\n\nThe third-party library had its faults. Android Studio was slow. Not enough time. \n\n\nAccomplishments that I'm proud of\n\n\nA working app. We worked well as a team. Learning Firebase.\n\n\nWhat I learned\n\n\nSetting up Firebase. It takes a lot more time to make a solid app\n\n\nWhat's next for Hinder\n\n\nMove beyond prototyping. Change the world of house-buying.  Go global. Make millions.\n\n\n\n\n\n\nTry it out\n\n\n\n\n\n\n\n\ngithub.com"
    },
    {
        "url": "https://devpost.com/software/beat-the-box-fipowr",
        "content": "The UI\n\n\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n\nSticky Note Detection\n\n\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n\nHand Detection\n\n\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n\nFace Detection\n\n\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n\nEye Detection\n\n\n\n\n\n\n \n\n\n\n\nInspiration\n\n\nMy inspiration came from learning how to use Cascade Classifiers which allowed me to do cool things such as hand, face, and eye detection. Over the past few months, I have really plunged deep into OpenCV and the different applications of it.\n\n\nWhat it does\n\n\nBeat the Box! is a simple concept. Use your \"controller\" to clear the box from the screen by overlapping your \"controller\" with the box. The \"controller\" is can be your hand, eye, face, or a yellow sticky note. The boxes can move if desired and there are different times you can choose from(i.e. 30, 40, 50 seconds). After time runs out, a score is calculated based on your \"Boxes Per Minute\". If you happen to earn the top score in your controller, the high score will be updated on the description page. See the YouTube video to see it in action!\n\n\nHow I built it\n\n\nI aimed to combine my two favorite libraries, OpenCV and wxPython. I first created the detection algorithms for hand, face, eye, and the sticky note. Once I had detection of the controller, I needed to create a random box that would pop up so the user could hit it. Then, I needed to have the box disappear once the user hit it and up the score. After that, the OpenCV was pretty much was done. For wxPython, I aimed to make a simple interface with a couple options to make the game more challenging. Those options ended up being \"Box Movement?\" and \"Time\". Box movement gave a random speed to each box and then the box would move across the screen. I added a dropdown to choose between controllers, added a description box of each controller, and then finally used a big Start button to run the options. \n\n\nChallenges I ran into\n\n\nThe main challenge of this project was the controller detection. Fine tuning the hand algorithm was especially painful until I figured out that it was much easier to tune if you had a closed fist instead of an open hand. Another problem was how to figure out if the box was inside the controller. This problem was easier because plenty of other people online had asked about this problem. A physical problem that stackoverflow.com couldn't fix was the slow computer I had. I couldn't run the program at the ideal resolution of 640x480 but rather had to settle for 320x240 which certainly lost some accuracy with the controller detectors. \n\n\nAccomplishments that I'm proud of\n\n\nI am happy that the whole application worked out as I had envisioned it in the beginning. Sometimes I hit a major roadblock that would stop all progress, but this time, everything fell into place! I am happy to have extended my OpenCV skills to include hand detection. \n\n\nWhat I learned\n\n\nI learned how to detect hands with OpenCV and also learned how to properly setup OpenCV and wxPython together. I learned how to quickly build an application by first breaking down all the parts into small pieces that I could do like figure out how to detect faces. Once I finished one small problem, I just went on to the next until the app was finished.\n\n\nWhat's next for Beat the Box!\n\n\nI plan on using a more powerful computer to run this program at a higher resolution and see just how accurate my algorithms are capable of. I also plan on cleaning up the code and giving some nice comments on what is going on for easier future reference. I think this will make for a fun demo app to friends and family of what computer vision is capable of doing with just some code!\n\n\n\n\n\n\nBuilt With\n\n\nopencv\npython\nwxpython\n\n\n\n\n\n\nTry it out\n\n\n\n\n\n\n\n\ngithub.com"
    },
    {
        "url": "https://devpost.com/software/3dmodelresizer",
        "content": "Do the DEW\n\n\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n\nWorking with Blender is hard. We're here simplify things.\n\n\n\n\n\n\n \n\n\n\n\nWhat it does\n\n\nUsing Blender's API and a whole lot of math, we've created a service that allows you to customize and perfectly fit 3D models to your unique dimensions. No more painstaking adjustments and wasted 3D prints necessary, simply select your print, enter your sizes, and download your fitted prop within a few fast seconds. We take in specific wrist, forearm, and length measurements and dynamically resize preset .OBJ files without any unsavory warping. Once the transformations are complete, we export it right back to you ready to send off to the printers.\n\n\nInspiration\n\n\nThere's nothing cooler than seeing your favorite iconic characters coming to life, and we wanted to help bring that magic to 3D printing enthusiasts! Just starting off as a beginner with 3D modeling can be a daunting task -- trust us, most of the team are in the same boat with you. By building up these tools and automation scripts we hope to pave a smoother road for people interested in innovating their hobbies and getting out cool customized prints out fast.\n\n\nNext Steps\n\n\nWith a little bit of preprocessing, we can let any 3D modeler upload their models to our web service and have them dynamically fitted in no time! We hope to grow our collection of available models and make 3D printing much easier and more accessible for everyone. As it grows we hope to make it a common tool in every 3D artists arsenal.\n\n\nSpecial shoutout to Pepsi for the Dew\n\n\n\n\n\n\nBuilt With\n\n\nblender\ncss\nhtml\njavascript\njquery\nmath\npython\n\n\n\n\n\n\nTry it out\n\n\n\n\n\n\n\n\ngithub.com"
    },
    {
        "url": "https://devpost.com/software/leapdrop",
        "content": "LeapDrop is a (Leap Motion + Chrome Extension + Android) hack that allows the geek to drag a webpage from one computer and drop it to another computer, where it opens \nalmost\n instantly.\n\n\nWhat started out as a FPS VR game, soon turned out into a cool hack emulating gesture control that we've seen in science-fiction movies. While looking for documentation online, as none of us had an iota of experience with VR development, we ran into almost ten situations every two minutes where we had to share links with each other.\n\n\nSo, we thought about how we can make sharing webpages cool, and came up with LeapDrop.\n\n\n\n\n\n\nBuilt With\n\n\nandroid\nchrome\njava\njavascript\nleap-motion\nnode.js\nsocket.io\n\n\n\n\n\n\nTry it out\n\n\n\n\n\n\n\n\ngithub.com"
    },
    {
        "url": "https://devpost.com/software/wwworld-j9047y",
        "content": "WWWorld is a Chrome extension that allows you to react to videos in real time with the world\n\n\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n\nA full screen viewing experience with reactions in real time\n\n\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n\nExit screen viewing\n\n\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n\nAdd friends to current video viewing\n\n\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n\nSearch for friends to watch with\n\n\n\n\n\n\n \n\n\n\n\nInspiration\n\n\nWWWorld's inspiration came from the possibilities of video, twilio's sync api, and the power of iconographic reactions to create an engaging and immersive user experience. Our inspiration drew from sources such as Periscope's live viewing feature and Facebook Live's reaction series (love, wow, haha, sad, angry) in the creation of WWWorld, a Chrome Extension. \n\n\nWhat it does\n\n\nWWWorld is a chrome extension that allows users to react to the same video in real time. It utilizes Twilio's Sync API to synchronize video play time between users. Users can then express reactions playfully and intuitively as the video plays with WWWorld's emoji's. \n\n\nHow we built it\n\n\nWWWorld was built with rapid prototyping using Sketch App and Adobe Illustrator, and was then further implemented on the Front-End using HTML, CSS, and JavaScript.\nThe Front-End is supported by the Twilio Sync Javascript SDK for syncing live video and live reactions. Authentication is handled by a node backend issuing JWT tokens.\n\n\nChallenges we ran into\n\n\nSynchronizing video \nHaving no network is not great for a real-time networking application.\nHaving no power is not great for a real-time networking application that needs a network.\n\n\nAccomplishments that we're proud of\n\n\nEnabling synchronization of video\n\n\nWhat we learned\n\n\nTwilio Sync API\n\n\n\n\n\n\nBuilt With\n\n\nadobe-illustrator\nchrome\ncss\nhtml5\njavascript\njquery\njson\nsketch-app\ntwilio-sync\n\n\n\n\n\n\nTry it out\n\n\n\n\n\n\n\n\ngithub.com"
    },
    {
        "url": "https://devpost.com/software/petcontrol",
        "content": "Inspiration\n\n\nAdam had the dream to create this application for months and as his last hackathon, he decided to peruse it.  It started as a simple way to use QR codes on dog tags to get alerted when someone finds your lost dog, it turned into much more.\n\n\nWhat it does\n\n\nThere's two goals of PetControl, the first is the original idea of using a QR code to keep track of pets. Current dog tags can hold small amounts of information, maybe a name and an address.  By using PetControl's QR tags pet owners can set large amounts of information.  The owner can tell the person who finds it the dogs name, the owners name, the phone number, address, and special notes about the pet.  By doing this it allows the pet to get home to the owner faster and safer.\n\n\nThe second part of PetControl came after, instead of only being able to be used when a pet is lost, PetControl can be used any time of the day to help manage and take care of your beloved animal friend.  With widgets in a dashboard on our website, pet owners can log in and view a live video of their pet with their Nest Camera, order more food for their pet using the eBay API, or mark their dog as lost so they are immediately notified whenever someone scans their dog's tag.\n\n\nNot everyone has a QR code reader, or a smartphone, that's why we used \nTwilio\n to send a picture of the tag to a number and it act the same as the app.\n\n\nHow we built it\n\n\nThe back end was all developed using Python, combined with Flask we were able to create a fully functioning website complete with accounts, pets, dashboard, and numerous other features (including Twilio support). The mobile app was developed for Android and is complete with tag scanning functionality, pet lookup, and pet management.\n\n\nChallenges I ran into\n\n\nIn the beginning we had planned to use the JavaScript framework Meteor, however parsing a QR code using Javascript proved difficult and we moved to Python where we felt more comfortable.\n\n\nAccomplishments that I'm proud of\n\n\nI (Tristan) am proud of learning Python and Flask to create a Python web application, I am also proud of the frontend work I did to create the online dashboard.\n\n\nWhat I learned\n\n\nAs above, Tristan learned how to create a web application in Python by using Flask.\n\n\nWhat's next for PetControl\n\n\nPetControl has many use cases, and we believe that it can play a substantial role in helping many people have cost effective, modern pet tags.\n\n\n\n\n\n\nBuilt With\n\n\nandroid\ncss\nflask\nhtml5\njavascript\nmaterializecss\npython\n\n\n\n\n\n\nTry it out\n\n\n\n\n\n\n\n\ngithub.com"
    },
    {
        "url": "https://devpost.com/software/chess-ev68np",
        "content": "Inspiration\n\n\nChess is a game that has been around for centuries, and remains widely popular even in today's digital age. Whereas games today are utilizing increasingly sophisticated technology in order to provide players, both experienced and inexperienced, with a more modern look and feel to them, the chessboard has remained largely the same throughout its history. As a result, we decided to combine the traditional game of chess with a modernized interface in order to increase its appeal to a new, modern generation. \n\n\nWhat it does\n\n\nThe Chess++ board has the functionality of a traditional chess board, and can be used to play a traditional game of chess. However, the Chess++ board is also connected to a website which displays the current positions of the pieces on the board. Not only does the Chess++ board send information to the website, but it also takes in information from the website and displays it on the board. The Chess++ board includes LED lights below each square, and these lights can be used to display to inexperienced users the possible actions that a certain piece could perform. Once a player picks up a piece from the Chess++ board, it registers that a user has picked the piece up and displays the possible actions the piece can take. This functionality is aimed to assist new users in quickly grasping the rules of chess and to allow them to enjoy the game faster.\n\n\nHow we built it\n\n\nThe Chess++ board utilizes cloud technology in conjunction with the Raspberry Pi and the Arduino Uno in order to accomplish its functions. The board itself is built with conductive tape, which completes a circuit when a piece is placed on the grid  The grid is connected to the Arduino, which is the conduit between the website and the board itself. In addition, the Raspberry Pi is used to turn on the proper LEDs, which are situated underneath the board.\n\n\nChallenges we ran into\n\n\nThe main challenge was the lack of hardware to completely finish our design. We had to cut back on certain features that were planned, such as real-time analysis of the gameplay on the website. In addition, another challenge was figuring out the algorithm for possible moves. \n\n\nAccomplishments that we're proud of\n\n\nWe are very proud of the fact that we were able to develop a working prototype, and prove that this design and functionality is viable and can be further polished in the future for others to develop as well. \n\n\nWhat we learned\n\n\nWe learned that communication is very important, as we had difficulty in communicating between team members and figuring out what tasks needed to be done.\n\n\nWhat's next for Chess++\n\n\nWe hope to be able to implement an analysis of gameplay at the end of a game, in order to teach new players where they might have been able to make a better move. In addition, we hope to streamline the design, and make it more visually appealing. Finally, we also hope to make a realtime move analysis in addition to the analysis at the end of a game.\n\n\n\n\n\n\nBuilt With\n\n\narduino\nc-sharp\ncss3\nhardware\nhtml5\njavascript\njoint.js\npython\nraspberry-pi"
    },
    {
        "url": "https://devpost.com/software/scan-n-grab",
        "content": "The blank list before running a scan against a host.\n\n\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n\nBy selecting and double clicking a port in the list, you can view the banner that was sent back from the server.\n\n\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n\nThe list fills up quickly after starting to scan a host for open ports, and the program automatically grabs the banner that are available.\n\n\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n\nThe program comes with instructions on how to understand the output of this program.\n\n\n\n\n\n\n \n\n\n\n\nInspiration\n\n\nIn cyber security, there are many well know tools that exist. I decided that I wanted to understand the underlying mechanics of making a scanner from the ground up, and add more functionality. Hence, banner grabbing was included, an the Scan 'n' Grab was born.\n\n\nWhat it does\n\n\nThe Scan 'n' Grab is a program that knocks on the \"doors\" of a computer looking for open \"doors\" and \"doors\" that respond with information. A network administrator can use this information to audit what services a server is running, and what version some of the software is running. It can even catch the occasional backdoor.\n\n\nHow I built it\n\n\nI built it using Python, with very few imported libraries.\n\n\nChallenges I ran into\n\n\nI ran into issues with setting up the gui side of this application, while I had the lower levels of this project running very quickly, getting the \"back end\", and the \"front end\", of the project working together took a lot of time and debugging.\n\n\nAccomplishments that I'm proud of\n\n\nI'm proud that I was able to build this from scratch within the time frame that was given.\n\n\nWhat I learned\n\n\nI learned a lot of Python: sockets, and tkinter gui.\n\n\nWhat's next for Scan 'n' Grab\n\n\nAn improved gui, and adding more advanced port grabbing for services that is more accurate.\n\n\n\n\n\n\nBuilt With\n\n\npython"
    },
    {
        "url": "https://devpost.com/software/the-magic-hand",
        "content": "Structure\n\n\n\n\n\n\n \n\n\n\n\nInspiration\n\n\nNowadays, people cannot live without mobile phones and we cannot interact with people without touching a capacitive screen. However, imagine you are a disable person with no hand or paralysis and have to use the huge, ugly and special mobile devices which makes you feel special already, how would you feel? Therefore, our design is trying to help those disable people to use modern capacitive screen mobile phone with a simple and convenient phone cover film which can touch any points of the screen instead of human fingers.\n\n\nWhat it does\n\n\nThe Magic Hand is a film which can be attached to any capacitive screen to help disable people to use the phone. It works as a output and takes instructions from any sort of input sensors, eg, LDR, Muscle sensors, BCI and Voice Control.\n\n\nHow we built it\n\n\nWe built a prototype of our design to interact a game called piano tiles which has high requirement on dexterity and reaction time. We used Light-Dependent Resistors (LDR) to detect the brightness of the tiles at certain points. Arduino board was used to process and control the signals at outputs. Furthermore, the to simulate human fingerprints, we used relay, resistors, aluminium foil and 3D-printing to trigger the capacitive screen. \n\n\nChallenges we ran into\n\n\nThe most significant challenge we faced was the calibration of the actuator. Since we used an iPhone for demo, the specific requirement of Apple screen are unknown to us, for example, the surface area of actuator. \n\n\nAccomplishments that we're proud of\n\n\nOur prototype can achieve a score that higher than human reactions which means that its able to achieve a dexterity and reaction time that users required, especially for disable people. That also means our idea about the capacitive screen film is achievable.  \n\n\nWhat we learned\n\n\nHow to build a high dexterity demand devices and trouble shooting under high pressure and time limit.\n\n\nWhat's next for The Magic Hand\n\n\nBuild the full version of The Magic Hand, capacitive dexterity engine, by increasing the control area of our current version of The Magic Hand. To achieve this, we need to built a PCB board which contains the main control circuit and try different types of material to improve the performance of device.\n\n\n\n\n\n\nBuilt With\n\n\n3dprinting\narduino\ncircuit-design\nldr\nsimplify-3d\nsoildworks\nsoldering"
    },
    {
        "url": "https://devpost.com/software/pyraminx-scheme",
        "content": "3D representation of pyraminx\n\n\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n\nAnimation of 3D pyraminx\n\n\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n\nOpenCV image recognition of pyraminx configuration\n\n\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n\nRecognition of triangles on a face\n\n\n\n\n\n\n \n\n\n\n\nInspiration\n\n\nWe found that there were no apps available for the particular type of puzzle that we were trying to solve, so we decided to make one for ourselves! The idea is the make it educational to allow people to learn how to solve the puzzle, and not just mindlessly rotate things around.\n\n\nWhat it does\n\n\nYou show the app each side of your pyraminx and it will automatically read in the colours and determine the configuration of the puzzle. It will then show you the moves necessary to solve the puzzle.\n\n\nHow we built it\n\n\nWe used OpenCV for the image processing and Unity for the 3D rendering.\n\n\n\n\n\n\nBuilt With\n\n\nc#\nc++\nopencv\nunity\n\n\n\n\n\n\nTry it out\n\n\n\n\n\n\n\n\ngithub.com"
    },
    {
        "url": "https://devpost.com/software/idiomatic",
        "content": "Mock up\n\n\n\n\n\n\n \n\n\n\n\nInspiration\n\n\nIdioms are common sayings that native languages often use. For English language learners and students with autism, idioms are very challenging. Idioms often have no clear relation to their meaning and are not literal, and thus hard to learn and understand. English-as-a-second-language learners and students with autism can have better conversations and more deeply engage with native English speakers when they have idioms in their vocabulary.\n\n\nWhat it does\n\n\nThe learner can search for a word or phrase. Idiomatic will return an idiom, its meaning and (will include) a relevant YouTube video. This is valuable for the learner, as they will find the meaning and have a visual reference for the idiom.\n\n\nHow we built it\n\n\nSet up involved:\n\n\n\n\nCreate Heroku project\n\n\nCreate Github repo\n\n\nSet up Ruby on Rails \n\n\nInstall React\n\n\nCreate database in SQLite 3\n\n\n\n\nOur project involved:\n\n\n\n\nWireframing\n\n\nResearch on idioms\n\n\nDesign process\n\n\nCompiled idiom data into a spreadsheet (idiom, meaning).\n\n\nPut compiled idioms into database\n\n\nLots of coding!\n\n\n\n\nChallenges we ran into\n\n\nWe ran out of time to add in the YouTube API in order for the learner to view a relevant video. Most of day was spent creating the framework. If this was a two-day hackathon, we would have had time.\n\n\nWe were unable to find an API that would allow us to only search for or use idioms, so data entry was more manual than we would have liked. \n\n\nAccomplishments that we're proud of\n\n\n\n\nLearning: Team members took the time to teach each other.\n\n\nCollaboration: Each team member contributed and each contribution was valued. \n\n\nAgility: We moved quickly to build Idiomatic, all while a number of us were attending workshops.\n\n\n\n\nWhat we learned\n\n\nFor two of us, this was our first hackathon. We learned how quickly we could develop a working app, which was very exciting for us. Many of us worked with new technologies for the first time. \n\n\nWhat's next for Idiomatic\n\n\nFuture versions of Idiomatic can include a number of great new features:\n\n\n\n\nPronunciation of the idiom\n\n\nTranslation of idiom into learner's language using a dictionary API\n\n\nUse of sentiment analysis in order to provide additional details on the emotions that are associated with the idiom\n\n\nAbility for learner to find random idiom\n\n\nAdditional grammar information on idiom used in a sentence\n\n\n\n\n\n\n\n\nBuilt With\n\n\ncss\nheroku\nhtml\njavascript\nlove\nreact\nruby-on-rails\nsketch\nsqlite\n\n\n\n\n\n\nTry it out\n\n\n\n\n\n\n\n\nwww.idiomatic.website\n\n\n\n\n\n\n\n\ngithub.com"
    },
    {
        "url": "https://devpost.com/software/feedpakistan",
        "content": "App Launch\n\n\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n\nNew Donor/Taker\n\n\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n\nDonor Profile\n\n\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n\nNew Donation\n\n\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n\nAll Donations\n\n\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n\nTake Donation\n\n\n\n\n\n\n \n\n\n\n\nNo Food Wasted\n\n\nPROBLEM STATEMENT\n\n\nAccording to the UN Food and Agriculture Organization (FAO), developing countries waste 40 percent food items. Around 1.3 billion tons of food is wasted annually all over the world. By trimming waste and diverting food loss we can alleviate hunger, create jobs, combat climate change, conserve natural resources, and cultivate more sustainable communities.\n\n\nFACTS\n\n\nFood waste ranks as a top emitter of greenhouse gases estimated at 3.3 billion tons of carbon dioxide released into the atmosphere each year. 3.5 billion acres of land, 28 percent of the world’s agricultural area, is used annually to produce food that’s never eaten. More than 20% of all cultivated land, 30% of forests and 10%of grasslands are undergoing degradation.\n\n\nWHAT WE DO\n\n\nWasting food means not only money down the drain, but also all of the resources needed to grow and distribute that food in the first place. Our mission is to make fresh, nutritious food accessible to everyone regardless of their location or socio-economic status.\n\n\nHOW IT WORKS\n\n\nWe believe our platform can end hunger by connecting restaurants and people that have excess food to different neighborhood and charities in dire need of food to feed the underprivileged. This moves nutritious food away from the waste stream and onto the plate of someone in need. Hunger is a distribution problem, not a supply problem. Together, we can solve it.\n\n\n\n\n\n\nBuilt With\n\n\njava\nmysql\nphp\n\n\n\n\n\n\nTry it out\n\n\n\n\n\n\n\n\ngithub.com"
    },
    {
        "url": "https://devpost.com/software/bagh-bagh",
        "content": "Overview\n\n\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n\nHomepage\n\n\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n\nMenu\n\n\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n\nEvergreen\n\n\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n\nShare Sample\n\n\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n\nHomepage\n\n\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n\nPlant information\n\n\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n\nTrees\n\n\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n\nGul Mohar\n\n\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n\nHomepage\n\n\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n\nCommunity Activities\n\n\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n\nShare a plant\n\n\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n\nSubmission sample\n\n\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n\nSubmissions\n\n\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n\n\n\n\n\n\n\n \n\n\n\n\nProblem Statement\n\n\nResidents of big cities are leading increasingly isolated and mechanical lives. Also, the stripping of green belts in cities like Islamabad, Lahore and Karachi has resulted in environmental degradation as well. \n\n\nSolution\n\n\nThe Bagh Bagh phone app seeks to create communities that will engage in gardening activities together. These communities will be formed through the following ways:\n\nPlant Share:\n Exchange and sharing of plant clippings and seeds by users. These are those extra clippings and bulbs that would normally go to waste after pruning and cutting that takes place several times every year. Now, through this app, users can post pictures of the plants they will be clipping and others can take them for them, either for free or in exchange for any other extra plant that they have.\n\nInformation Share:\n A large database of gardening information for new gardeners will bring more interested people into gardening. Moderated community submission of content will further build the \n\nSpace Share:\n People and organizations can set up events to invite people to garden in public spaces and their own yards. \n\n\nOnce a community is developed it will lead to plantation drives in the city, more planting in users' residences through the shared clippings and increased interaction among city residents that is purposeful and positive. \n\n\nGrowth\n\n\n\n\nConnect with existing groups such as \nMASF \n who are involved in plantation in Islamabad (we have contacted them and they are ready to work with us) and other existing gardening groups.\n\n\nLeader boards and gamification possibly using shared plants as suprise rewards\n\n\nInitial gardening and plantation events curated and managed by the team. \n\n\nOnce a community is developed leaders will emerge who will take the groups forward\n\nSustainability\n\n\nStage 1\n Self run \nOnce small groups have been formed through this interaction, community activities for plantation will be planned, for which funding from International donors and local NGOs known to the team members can be obtained. After a momentum has been built for the interaction in the users through these events and exchange of plants, leaders from the group of users will emerge and sustain the momentum. \n\nStage 2\n Small grants International donors and partnering with local NGOs to arrange community events to grow the community. \n\nStage 3\n Once the community is developed, partner with gardening suppliers for direct marketing to a large dedicated community. Other avenues can include specialized delivery services for seeds, cuttings, supplies etc. \n\n\n\n\n\n\n\n\nBuilt With\n\n\n.net\nandroid\nasp.net\nbootstrap\ncommunity\ncsharp\ncss3\nhtml5\njquery\nmockup.io\n\n\n\n\n\n\nTry it out\n\n\n\n\n\n\n\n\ngithub.com"
    },
    {
        "url": "https://devpost.com/software/locatio",
        "content": "Overview of selected Shipment\n\n\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n\nHead Part of selected Shipment\n\n\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n\nBottom Part of selected Shipment\n\n\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n\nLanding Page With Shipments Overview and Upload\n\n\n\n\n\n\n \n\n\n\n\nParticipated Challenges\n\n\n\n\nKuehne & Nagel\n\n\nHere Maps\n\n\n\n\nInspiration\n\n\n\n\nKuehne & Nagel's real problem of planning huge amounts of shipments all around the world\n\n\nproblem of sending different couriers to the same location to ship goods to customers which is a waste of resources\n\n\nproblem of inconsistency when handling location data provided by customers\n->Huge Optimizations Possible! \n\n\n\n\nWhat it does\n\n\n\n\nnormalize location data from different customer sources\n\n\nprovide additional meta information about courier routes and destinations (minimal street width, maximal curviness, building information, opening times, ...) using the here maps API\n\n\nhelps Kuehne & Nagel to plan shipments with route information and offers consolidations instead of using multiple half empty Excel files with sometimes 200.000\n\n\n\n\nHow we built it\n\n\n\n\nThe data was first cleaned and normalized using Python (with numpy and pandas).\n\n\nAt the same time we developed a Web application to simplify the way the end user interacts with our data.\n\n\nThe Web app is served via a Flask Server with a beautiful Front-End.\n\n\nWe used an informal Scrum approach by having multiple iterative sync meetings and brainstorming sessions.\n\n\n\n\nChallenges we ran into\n\n\n\n\nThe Data Cleaning and Data Integration task was tough, since every customer uses another data format.\n\n\nUnfortunately, we were not able to receive an extended here API key. Thus, it wasn't possible to retrieve other information than location data from the here API. In production, of course, the data can simply be retrieved using the here API and the production key. \n\n\n\n\nAccomplishments that we're proud of\n\n\nIT WORKS!!!\n\n\n\n\n\n\nBuilt With\n\n\najax\nbig-data\ncss\nflask\nhere-api\nhere-maps\nhtml\njavascript\njqeury\njupyter\nmaterialize\nnumpy\npandas\npython\n\n\n\n\n\n\nTry it out\n\n\n\n\n\n\n\n\ngithub.com"
    },
    {
        "url": "https://devpost.com/software/oneapp",
        "content": "Home Screen\n\n\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n\nTwitch!\n\n\n\n\n\n\n \n\n\n\n\nInspiration\n\n\nEver since Google demo'd Instant Apps at IO this month, we've been intrigued by the possibilities of this technology and curious whether it could work on iOS. Many people have told us that this would be difficult or even impossible, but with some knowledge of react native hot-reloading, we were convinced that this was not only possible, but also the perfect project for a16z.\n\n\nWhat it does\n\n\nBased on the user's context, such as GPS location, OneApp prompts the user about apps that could be useful and allows the user to use them immediately. OneApp grabs the relevant app from a server, compiles the code and sends the result directly to the user's phone.  Theoretically, OneApp could process any React Native app in this way.\n\n\nHow we built it\n\n\nWe modified React Native by intercepting communication to its server and inserting our own code. We also built a number of modules.\n\n\nChallenges we ran into\n\n\nHacking a framework like React Native is difficult. You can very easily get deeply lost in some stack trace which will not give you the functionality that you want.\n\n\nAccomplishments that we're proud of\n\n\nBuilding something said to be impossible! We managed to get Twitch working in some degree. Our entire a16z hack last year was Twitch for mobile phones, and we can get similar functionality, and much more, with OneApp.\n\n\nWhat we learned\n\n\nWhat's next for OneApp\n\n\n\n\n\n\nBuilt With\n\n\nmongodb\nnode.js\nreact\nreact-native\n\n\n\n\n\n\nTry it out\n\n\n\n\n\n\n\n\ngithub.com"
    },
    {
        "url": "https://devpost.com/software/falcon-m5zehr",
        "content": "Mac Client\n\n\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n\nChrome Omnibox Client\n\n\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n\nMac Client: Images\n\n\n\n\n\n\n \n\n\n\n\nWe designed Falcon with an all-too-familiar scenario in mind: remembering snippets of content or meaning from websites but not the exact title or URL. Unfortunately, modern browser histories are only indexed by these two properties. To this end, we designed Falcon, a multi-platform application that allows users to search their browsing history by page content and the semantic meaning of the embedded images. \n\n\nAs the user browses the web on Chrome or their smartphone, Falcon keeps track of the pages they visit, and extracts both text and image content. We index the text in an elasticsearch database, and feed the images through a deep recurrent neural network to infer and articulate what is going on in the image or gif. We use a server written in golang to index and search all the data received from users. \n\n\nA Chrome extension and an Android app feed the data to the server, and users can search through a native popup-style Mac client or an omnibar extension for Chrome.\n\n\n\n\n\n\nBuilt With\n\n\nandroid\nelasticsearch\ngolang\njava\njavascript\nmachine-learning\nobjective-c\n\n\n\n\n\n\nTry it out\n\n\n\n\n\n\n\n\nfalcon.kim"
    },
    {
        "url": "https://devpost.com/software/tic-tac-fail",
        "content": "Inspiration\n\n\nHaven't you ever wanted to have something that is more of a failure than you are, well I have one now :P\n\n\nWhat it does\n\n\nIt will purposely loose at tic tac toe by chosing the moves that have the most winning outcomes for you -  how considerate :)\n\n\nHow I built it\n\n\nIt is built entirely using the functional programming language Racket and a module for http rest apis. It uses a depth tree search to determine the rating of each move\n\n\nAccomplishments that I'm proud of\n\n\nIts built in freaking racket\n\n\n\n\n\n\nBuilt With\n\n\nracket\n\n\n\n\n\n\nTry it out\n\n\n\n\n\n\n\n\ngithub.com"
    },
    {
        "url": "https://devpost.com/software/swip",
        "content": "UX Markup\n\n\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n\nExtendedScreen\n\n\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n\nPong Clone\n\n\n\n\n\n\n \n\n\n\n\nswip\n\n\nPong & Photoshare Demo\n\n\nInspiration\n\n\nThe idea behind this tool was to use 2 or more different devices (e. g. Smartphones) to create a bigger screen, quickly exchange Data and create new game pattern mechanics (like pong with shifted screens). So we decided to try it out in the 24 hours Inno-{Hacks} hackathon at the DHBW Karlsruhe.\n\n\n\n\nWhat we build\n\n\nWe created 2 example applications to show the capabilities of such a system. The first is a pong clone which let's you play with 2 different devices. Yes, even with different sizes! You just hold the devices next to each other an *swip* over both, pinching the thumb and indexfinger to the edge. That's it, you're now ready to play.\n\n\nThe second example is \n\n\n\n\nHow we built it\n\n\nTo provide a wide coverage of all the different devices out there, our choice was javascript. For the connection websockets were the best choice, so we used Socket.io.  For the pong game, the canvas Element was the perfect choice.\n\n\n\n\nChallenges we ran into\n\n\nThe Time synchronization between the devices was a real big problem, because we wanted a smooth transition from one screen to the next. The different pixel-density was another quite big problem, since the different manufacturers (Apple, Samsung, HTC) have different resolutions and density, we had to find a way to scale the content according to the devices density.\n\n\n\n\nBuilt With\n\n\nJavascript, HTML, CSS, Socket.io, Express, Node.js\n\n\n\n\n\n\nP.S. it's a prototype\n\n\n\n\n\n\nBuilt With\n\n\ncss\nhtml\njavascript\njquery\nnode.js\nsocket.io\n\n\n\n\n\n\nTry it out\n\n\n\n\n\n\n\n\ngithub.com"
    },
    {
        "url": "https://devpost.com/software/time-traveling-for-dummies",
        "content": "Input\n\n\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n\nOutput\n\n\n\n\n\n\n \n\n\n\n\nInspiration\n\n\nRecently, I traveled back to 1776 to see Washington cross the Delaware. Bad idea – I nearly died of hypothermia since I, as a shorts-wearing Californian, was not prepared for the New England winter. Even worse, no shops would accept my $3 to buy Doritos! \nThey didn't even have Doritos!!!\n Who could have possibly known this would happen?!? If only I had thought of those annoying details beforehand...\n\n\nWhat it does\n\n\nEnter the day and place to which you plan to travel and the amount of money you will be bringing along, and Time Traveling for Dummies will provide you useful information about weather conditions and the value of your money.\n\n\nHow I built it\n\n\nFueled by my anger that 1776 Delaware lacked Doritos, I retrieved and displayed historical data on my website with the WolframAlpha API.\n\n\nChallenges I ran into\n\n\nI had difficulty figuring out how to handle and properly display the API data. I also struggled with constantly reminding myself of my humiliation after showing up to the Crossing of the Delaware in shorts and a t-shirt.\n\n\nAccomplishments that I'm proud of\n\n\nWorking with WolframAlpha's API (well, any API) for the first time! Developing a website that solves a real, highly pressing problem!\n\n\nWhat I learned\n\n\nI learned how to work with an API and how to parse through XML files. Also, I realized that Doritos didn't exist in the 1770s.\n\n\nWhat's next for Time Traveling for Dummies\n\n\nMore useful tools* for your next adventures!\n\n\n*Probably determined by further embarrassing problems I experience in my travels through time.\n\n\n\n\n\n\nBuilt With\n\n\ncss\nhtml\njavascript\nwolfram-technologies\n\n\n\n\n\n\nTry it out\n\n\n\n\n\n\n\n\ngithub.com"
    },
    {
        "url": "https://devpost.com/software/aros",
        "content": "Loaded Model From Web\n\n\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n\n\n\n\n\n\n\n \n\n\n\n\nAROS - distributed possibilities \nAugmented reality, when you blend the physical and digital world,  has a projected market size of 150 Billion Dollars by 2020.\n \nThe leading drivers for this projection are full immersion devices\n    like Holosens or the meta\n\n\nAt their current price points, these devices are still targeted at the higher end of the market \nEven harder to find than these devices, is the content  \nThe Content required from Augmented Reality very unique \n    To load a single model you need to build a full app,\n       built with its own AR possessor and provisioning profiles.\n \nAROS shows how to open up the emerging market and make it more accessible to developers by using current technology\n\n\nAROS is complete with its own file processing service, AR processor and provisioning profiles.\n \nAllowing you access any file and load it Augmented Realty.\n—————\nDEMO\n    — I am now an architect with William,  he has completed the house model and wants me to review it for approval.\n        previously he would have needed to build a physical model, costing both of u money and time.\n    — All I have to do is copy the link,  Past the link, and press start.\n    — AROS Then pulls the model from online, then converts it to AR content.\n    \n—————\n \nWith AROS,  Designers and developers can produce AR content without jumping through any of the rigorous hoops.\n \nFor Consumers, this means that they only have to install ONE small app and can consume any file in Augmented Reality.\n \nAROS - distributed possibilities \n\n\n\n\n\n\nBuilt With\n\n\nunity\nvuforia"
    },
    {
        "url": "https://devpost.com/software/bookbot",
        "content": "Messaging our Chat Bot \n\n\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n\nOur GupShup Code\n\n\n\n\n\n\n \n\n\n\n\nInspiration\n\n\nThe system of going to a library, checking out books, returning books, and placing holds on old fashioned websites is too tedious, so we thought of a way to modernize and automate a library with a chat bot.\n\n\nWhat it does\n\n\nThe product, BookBot, lets users and librarians access the library from their phones and search for books, view holds, and more.\n\n\nHow I built it\n\n\nWe used GupShup, a bot platform, to build our bot with javascript and nodejs. This bot would be deployed onto Facebook Messenger to use. We also used the Google Books API for the books and NLP to make the bot more user-friendly.\n\n\nChallenges I ran into\n\n\nDifferent messaging platforms behaved differently and handled images differently.\n\n\nAccomplishments that I'm proud of\n\n\nWe managed to create our own chat bot with natural language processing and give it a practical application in society.\n\n\nWhat I learned\n\n\nWe learned the ins and outs of GupShup, especially how its database works. We also learned the algorithms behind NLP.\n\n\nWhat's next for BookBot\n\n\nWe plan to integrate the 3m, the eBooks platform that libraries use today, into BookBot and perhaps make a virtual library with only bots.\n\n\n\n\n\n\nBuilt With\n\n\nfacebook-messenger\ngoogle-book-api\ngupshup\njavascript\nnode.js\nslack\n\n\n\n\n\n\nTry it out\n\n\n\n\n\n\n\n\ngithub.com"
    },
    {
        "url": "https://devpost.com/software/taz",
        "content": "Inspiration\n\n\nRemove the key frustration of Slack users: Tired of micro-managing unread messages in multiple channels in Slack.\nRecap/digest messages intelligently for users.\n\n\n\n\nWhat it does\n\n\nUsing a slash command, Taz creates a digest of the unread messages that are:\n\n\n\n\nTrending based on emoticons\n\n\nDirectly or undirectly mentionned to you\n\n\nMost talked about topics\n\n\n\n\n\n\nHow we built it\n\n\nHaving the problem clearly defined, our first priority was to brainstorm a backlog of features and figure out which were part of our minimal viable product (MVP) and which were nice to have that we could squeeze in at the end to wow the crowd. \n\n\nWe then went on an epic journey to find the brand we would give our bot.  This would in turn give us a better idea how to design, market and bring it some appeal. \n\n\nOn a technical note, a Slack command sends data to our Azure web API endpoint.  This endpoint then crunches your Slack data and figures out the unread messages and direct/undirect mentions.  This formatted data is sent as a beautiful digest in the taz bot instant messaging channel.\n\n\nChallenges we ran into\n\n\n\n\nGenerating usable dummy data\n\n\nSplitting tasks equally to leverage the team\n\n\nFormatting and rendering the Slack post\n\n\nSlack throttling our messages\n\n\nDon't always trust a Slack API wrapper\n\n\n\n\nAccomplishments that we're proud of\n\n\n\n\nEPIC value deliver in less than 24h\n\n\nWorking MVP\n\n\nAwesome sales pitch\n\n\nGreat landing page\n\n\n\n\nWhat we learned\n\n\n\n\nDon't cheap out on marketing/design strategies\n\n\nOrganizing on a tight deadline\n\n\nPrioritizing features for an MVP\n\n\nSleep is a luxury\n\n\n\n\nWhat's next for taz\n\n\n\n\nMachine learning; intelligently process message data\n\n\nActivity heatmap\n\n\nNatural language recognition\n\n\nMultlingual support\n\n\n\n\n\n\n\n\nBuilt With\n\n\nasp.net\nazure\nc#\ncss\nhtml\njavascript\nslack\n\n\n\n\n\n\nTry it out\n\n\n\n\n\n\n\n\ngithub.com"
    },
    {
        "url": "https://devpost.com/software/groot-72a9tw",
        "content": "Inspiration\n\n\nPlants have an amazing impact on worker happiness a healthy workplace. Apart from improving air quality, they've been proven to increase productivity by up to 15% and help reduce stress and anxiety by up to 35%.\n\n\nBut plants die. It's also pretty expensive to hire someone to take care of what your employees perceive as a glorified piece of furniture. So we made GROOT.\n\n\nWhat it does\n\n\nGROOT suggests plants that are well suited for your specific workspace. It finds an owner for the plants, which they notify when they need to be maintained. It allows your employees to really take control of their environment, form a bond with their workspace, and ultimately end up happier and more engaged.\n\n\nBest of all, this happens through natural language discussions through your favorite team messaging platform: Slack! (Almost) No forms to fill. Never boring.\n\n\nHow we built it\n\n\nWe've built GROOT on the proven Django/Python web stack. It's deployed on cloud VMs, making horizontal scaling as easy as 1-2-3.\n\n\nChallenges we ran into\n\n\nIt turns out plant care is one of the few remaining things that don't have an API in 2016. We've had to manually gather, curate, and organize a bunch of data from disparate sources to populate GROOT with valuable information.\n\n\nA team of 4 developers doesn't make for easy design, UX, or marketing. We definitely dabbled outside our comfort zone!\n\n\nAccomplishments that we're proud of\n\n\nGROOT is awesome and super easy to use. We're genuinely confident that it has the potential to increase happiness in the workplace, which is no easy task.\n\n\nWhat we learned\n\n\nApparently\n, plants require more care than just watering. Some need to be trimmed, others need to be rotated. Who knew?!\n\n\nWhat's next for GROOT\n\n\nWe're experimenting with sensor integration so that GROOT can stop guessing. Being able to measure soil humidity and sunlight would be invaluable data that could save a plant's life!\n\n\n\n\n\n\nBuilt With\n\n\ndjango\npython\n\n\n\n\n\n\nTry it out\n\n\n\n\n\n\n\n\nplantbot.cloudapp.net\n\n\n\n\n\n\n\n\ngit.mightygeeks.ninja"
    },
    {
        "url": "https://devpost.com/software/solarmaps",
        "content": "Landing\n\n\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n\nVerified view\n\n\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n\nReferrer view\n\n\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n\nEstimate view\n\n\n\n\n\n\n \n\n\n\n\nInspiration\n\n\nThe pain of selling solar and the sorrow of people not believing that solar can save them real money.\n\n\nWhat it does\n\n\nSolarMaps automates the solar sales process so that property owners interested in going solar can self qualify and self refer themselves through an open, information rich, and social process.  SolarMaps also allows existing solar customers to share the real savings that they have received from their installations and automatically refer more people to their developer, creating an additional revenue stream for them, while they help more people understand the dollars and cents value of solar.\n\n\nHow we built it\n\n\nAs a team!\n\n\nChallenges we ran into\n\n\nAccurately generating valid solar values.\n\n\nAccomplishments that we're proud of\n\n\nCreating a true demo that communicates the product value.\n\n\nWhat we learned\n\n\nWe learned about potential cost and market limitations to the product, but we believe that we found true solutions.\n\n\nWhat's next for SolarMaps\n\n\nContinuing to improve the product and start to generate real market values that will lead to more solar on more roofs.\n\n\n\n\n\n\nBuilt With\n\n\nbrains\ngoogle-maps\ngoogle-streetview\ngoogle-sunroof\npython\nruby-on-rails\nsketch\n\n\n\n\n\n\nTry it out\n\n\n\n\n\n\n\n\nsolarmaps.herokuapp.com\n\n\n\n\n\n\n\n\ngithub.com"
    },
    {
        "url": "https://devpost.com/software/alexabot",
        "content": "AlexaWebBot Team\n\n\n\n\n\n\n \n\n\n\n\nInspiration\n\n\nI want to order flowers for my mom for Mother's Day.  Straightforward.  I don't need to sit down, and I don't need to run though login on my phone.  I can give all the necessary information just by answering questions.  Apps have been mobilized.  They should also be \"Alexized.\"  We'd like to facilitate this by automatically generating the intents from typical workflows.\n\n\nWhat it does\n\n\nTell your Amazon Echo what page you want to visit.  Using Amazon's Alexa, Amazon Lambda's are executed to call servers that have pre-processed website actions.  Voice commands are translated to actions that the server executes through a browser to navigate, browse, log in, and submit other actions to the website.  \n\n\nIdeal website candidates are those that don't already have APIs, effectively turning these sites into a modern version.  \n\n\nChallenges we ran into\n\n\nWeb pages are built using many different patterns, some clear and others obscure.  Determining the flow of a web page can be assisted with natural language processing, and machine learning algorithms can be trained to make intelligent guesses.  But, a seamless approach could be attained by adding Open Graph attributes (or say \"ASK Graph\" attributes) for voice navigation.\n\n\n\n\n\n\nBuilt With\n\n\nalexa\namazon-alexa\njavascript\nnode.js\npython\nradix\nruby\nselenium\n\n\n\n\n\n\nTry it out\n\n\n\n\n\n\n\n\nalexabot.tech"
    },
    {
        "url": "https://devpost.com/software/stripr",
        "content": "Inspiration\n\n\nPower strips today have practical problems that make them severely inefficient; put in two laptop chargers and suddenly a 6-plug strip can be completely used up. There are some variations on the market that attempt at a more flexible structure, yet those are still restrictive in design, expensive, difficult to mount, and don't respond to specific user needs, such as USB ports or international travel.  So we decided to build an efficient and intelligent power strip that has a more flexible design, provides usage information, and responds to wireless control.\n\n\nWhat it does\n\n\nStructurally, the power strip, stripr, is divided into modules that can extend and rotate, giving more room for diverse plug designs.  An LCD display shows real time voltages across each module, and this information is also sent to the cloud where it can be analyzed and the results sent wirelessly to the user's computer or phone.\n\n\nHow we built it\n\n\nThe structural parts of the power strip were built by 3D-printing ABS filament, and the modules are connected by adjustable metal rods which house the wires connecting the modules.  We then built a circuit connecting an Arduino with a LCD display, the system also serving as a digital multimeter that measures voltage drops.  The Arduino then sends the voltage numbers to a computing system where it can be stored as a text file in the cloud.\n\n\nFor the sake of the prototype and due to limitations in size, parts, and time, the Arduino, breadboards, and circuit wiring are attached to the power strip on the outside.  \n\n\nChallenges we ran into\n\n\nDue to fire hazard regulations, we couldn't actually plug our power strip into a power source, so in order to demonstrate the function of the prototype, we compromised by wiring 9V batteries, with a small rotating camera appliance, internally through the power strip.  We recognize that this isn't exactly how our product will function, but due to the challenge of testing, we decided that using a battery driven motor was the best substitute to mimic a real power supply and appliance.\n\n\nIn addition, we faced difficulty in transmitting the voltage data over the WiFi network.  While the system properly provides real time voltage values, it was only able to properly send it to a computer using a direct USB connection.  After hours of debugging and reconfiguration, we attribute this setback to the security of the University of Chicago's wireless networks, since the WiFi requires additional security information the Arduino cannot provide and it probably also has other firewall and security features that would affect connectivity.  Therefore, the current strip only provides real time information on the LCD display.\n\n\nAccomplishments that we're proud of\n\n\nWe went through an iterative design process to ensure that we had a quality finished product with complex ideas.  We were able to embed a microcontroller to make a \"smart\" power strip, and we were able to minimize the disadvantages of 3D printing in regards to tolerance.\n\n\nWhat we learned\n\n\nThrough this project, we learned a lot about the vast abilities and functions of microcontrollers as well as how to develop advanced Arduino programming functions.  We discovered better ways to wire circuits and design hardware, paying special attention to structural integrity.  We also got a chance to dive into the world of internet connected devices and the analytics one could potentially unveil using it.\n\n\nWhat's next for stripr\n\n\nOur next steps for stripr would be to formally flesh out the modular concept.  We would develop detachable modules, which would allow the product to be customized to each customers needs, such as more USB ports or international outlets.  Additionally, we would create an application interface to complement the device.\n\n\nDomain Name: stripr.tech\n\n\n\n\n\n\nBuilt With\n\n\n3dprinting\narduino\nautodesk\ninventor\n\n\n\n\n\n\nTry it out\n\n\n\n\n\n\n\n\nwww.github.com"
    },
    {
        "url": "https://devpost.com/software/airena",
        "content": "In-game screenshot\n\n\n\n\n\n\n \n\n\n\n\n(Note: if the heroku server is down, it's probably because it exceeded quota. You can still run it by cloning the git repo)\n\n\nInspiration\n\n\nI've always loved the idea of writing an AI bot to challenge or fight others. I think it's a super fun way to learn to program and feels empowering. I always wanted to host a \"St. Olaf vs Carleton Bot Wars\" sort of thing, but none of the available tools really make it easy. That's mainly because:\n\n\n\n\nYou can't challenge individual people or groups. You can only upload your code to fight in the global leaderboards.\n\n\nYou can't easily play the game as a human, or play against your own bot as a human player.\n\n\nMost games are grid-based. (Which is ideal for developing AI, but is often not what the real world looks/feels like)\n\n\n\n\nSo I set out to make something that should be: simple, flexible and tangible. I wanted something that \ndoes not obscure the process of writing code\n. No one writes a perfect algorithm from scratch on their first try. Programming is an iterative process. It's very important for beginners to see that to understand how talented programmers do what they do and how they got to where they are. * My tool will be highlighting, rather than obscuring, the process of writing code.*\n\n\nWhat it does\n\n\nAirena (pronounced Eye-reen-a) is a multiplayer game, a teaching tool and a (prototype of a) platform. It's a space shooter that you can play as a human. You can immediately click on the code button to write up your bot and watch it take over. Joining the game is as easy as opening the link.\n\n\n\n\n\n\nHow I built it\n\n\nI used CreateJS for manipulating the canvas and vector graphics. I used CodeMirror for the in-browser code editor. I'm using NodeJS with Socket.io for the networking. The game runs the Javascript code you supply it in an isolated container so you can't interfere with the game itself. All you can do is return which key to press next. You have access to some data about the world, such as your position and the positions of other players. \n\n\nChallenges I ran into\n\n\nNot having really done a proper multiplayer/networking game before definitely made it a challenge to figure out how to connect and sync multiple users across the world. It was even more challenging to figure out how to allow anyone to run their own AI bot on the fly. \n\n\nI also spent a lot of time thinking about a game that would be simple enough to implement but still provide enough interesting challenge/strategy to be played by humans and AI.\n\n\nAccomplishments that I'm proud of\n\n\n\n\nThis is definitely the easiest and fastest way to write code for an AI bot and see immediate results (all within a browser!)\n\n\nVariables and state information persist even though the code gets recompiled at every keystroke!\n\n\nThe server keeps track of every player's code and state in its own isolated container, separate from the global state.\n\n\nThe live-coding-server logic is more or less decoupled, making it easy to repurpose for any other game or simulation for teaching purposes. (Imagine it being as easy as: import the plugin, define which function you want users you to be able to live-edit, and a code button pops up that allows users to play with the code in real time!)\n\n\n\n\nDespite how simple this game is, this would actually be pretty close to a real setting where you might implement AI because of the game state is continuous (not grid based). If we wanted to teach people about techniques such localization, we could omit sending the current player's position and have them use the common techniques to locate themselves on the map.\n\n\nWhat I learned\n\n\nI learned a lot about writing networked code, and thinking about decisions such as what logic goes on the server and what can be kept on the client. I also learned about how Javascript can be executed as a separate process within Nodejs (using the VM built in library) and about all the different ways of capturing and isolating an environment. (I'm currently taking a Programming Languages class so it felt highly relevant here!) \n\n\nI had also never used CreateJS before. I could have used Phaser, which is a library specifically built for games, and it probably would have been a lot easier, but I decided to use CreateJS because I hadn't used it before and because it's built to be more general (to support any rich interactive content on the web, not just games) I thought it would be a good tool to know. \n\n\nWhat's next for Airena\n\n\nI want to do two main things:\n\n\nOne is to further develop the game to make it more fun and interested to write AI for. One thing I'd love to add is the ability to release \"minions\", so you're no longer able to play very effectively as a single human. Then you can feel/see the power of programming. Perhaps a way to store and share code snippets that do various kinds of behaviors.\n\n\nThe other is to clean up and publish the live coding server as a standalone, to encourage more people to experiment with this idea, because I very much believe in learning by doing, I think by making it easy to implement this we could improve learning materials and show more people how fulfilling and exciting writing code really is!\n\n\nSource: \nhttps://github.com/OmarShehata/Airena\n\n\n\n\n\n\nBuilt With\n\n\ncreate.js\nnode.js\nsocket.io\n\n\n\n\n\n\nTry it out\n\n\n\n\n\n\n\n\nairena.herokuapp.com\n\n\n\n\n\n\n\n\nomarshehata.me\n\n\n\n\n\n\n\n\ngithub.com"
    },
    {
        "url": "https://devpost.com/software/alexa-jarvis",
        "content": "Inspiration\n\n\nWith the ever growing danger of distracted driving related incidents, our goal was to increase safety for motorcyclists and prevent dangerous accidents, while still maintaining a good user experience.\n\n\nWhat it does\n\n\nAlexa JARVIS is an intuitive voice controlled smart helmet. The goal was to create a completely voice controlled interactive system to assist a motorcyclist on the road, while leaving the other four senses to focus on the safety of the user. This is different than other smart helmets on the market because it does not impede the sense of sight, it is easily accessible and can be adapted to many different helmets, and it is a cheaper alternative to competitors.\n\n\nSome examples of the commands you can run are:\n\n\n“Navigate home”\n\n\n“Navigate to work”\n\n\n“Where am I?”\n\n\n“How long will it take me to get to work?”\n\n\n“Text Mike I’ll be there in 5 minutes”\n\n\n“Send my location to mom”\n\n\n“Help” – Sends out a distress signal with your location to a list of emergency contacts\n\n\nHow we built it\n\n\nWe incorporated Amazon’s Alexa voice service into a Raspberry Pi 2 and attached it to the helmet. We built the Alexa skill to communicate with the rider, using node.js and firebase for the backend. The Raspberry Pi is powered by an external USB battery pack and the electroluminescent lights are controlled using a relay which is controlled by the Raspberry Pi.\n\n\nChallenges we ran into\n\n\nControlling the electroluminescent lightson the helmet using Alexa commands, securing the Pi to the back of the helmet, and putting the microphone and speaker inside the helmet. Getting the Alexa voice service to work with the Raspberry Pi.\n\n\nWhat we learned\n\n\nWe learned how to function without sleep.\n\n\nWhat's next for Alexa JARVIS\n\n\nMore voice commands and a smaller and more mobile system that can be adapted to other helmets.\n\n\n\n\n\n\nBuilt With\n\n\namazon-alexa\nangular.js\napache\nfirebase\ngoogle-directions\nlambda\nnode.js\npython\nraspberry-pi\nrelay\ntwilio\n\n\n\n\n\n\nTry it out\n\n\n\n\n\n\n\n\ngithub.com"
    },
    {
        "url": "https://devpost.com/software/ar-watch",
        "content": "Trying it out!\n\n\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n\nAn early test of AR Watch\n\n\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n\nA screenshot of the near completed project in Unity\n\n\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n\nFinal version of the AR Watch made at Wearhacks\n\n\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n\nThe physical part of the AR Watch that the user wears\n\n\n\n\n\n\n \n\n\n\n\nInspiration\n\n\nWe play video games like Fallout and Mass Effect. Both games have wrist devices which display data. The team decided to try to mimic it.\n\n\nWhat it does\n\n\nAR Watch acts as a digital watch through the Google Cardboard. Four functions can be selected by the user while one functions runs at all times. The first function tells the time like a normal watch. The second function is a weather function which tells the current temperature for the city the AR Watch is in. The third function tells the date like a calendar. The last user selected function is to display a Bing map centered on the user's location. The function which runs at all times is the battery indicator.\n\n\nHow we built it\n\n\nThe AR Watch is built in Unity with Vuforia and Google Cardboard SDKs. Unity is the main platform where everything is put together. Vuforia is used to detect a image which determines where Unity displays the AR Watch. In order to display data across the watch faces, AR Watch made REST calls to various APIs. The map data came from Bing while the weather data came from OpenWeatherMap. \n\n\nWant to try?\n\n\nYou can download the .apk below for android. It will work with this image:\n\nhttps://drive.google.com/file/d/0B-LMvLFomXHHNGJNUFRGX2ZGT2s/view?usp=sharing\n\n\n\n\n\n\nBuilt With\n\n\nc#\ngoogle-cardboard\nunity\nvuforia\n\n\n\n\n\n\nTry it out\n\n\n\n\n\n\n\n\ndrive.google.com"
    },
    {
        "url": "https://devpost.com/software/secret-set",
        "content": "Artist analytics\n\n\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n\npay wall\n\n\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n\nset chatroom\n\n\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n\nSecret Set annoucement\n\n\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n\nUser Journey\n\n\n\n\n\n\n \n\n\n\n\nInspiration\n\n\nI want more intimate experiences with my favorite artists! Is that too much to ask? Little creepy yes, but how awesome would it be to have an actual conversation with them!\n\n\nWhat it does\n\n\nArtists create a \"secret set\" which includes when the set goes live, what content they want to include (audio, video, live-stream), and how to structure the set (free for first 1000, $.99 after). They then send out a special link/code through their social media platforms for people to use to connect to the secret set that a specific time. Once connected consumers can interact with the artist, listen, view, and enjoy with like minded people and the artist themselves. When the artists leaves the set is over.\n\n\nHow we built it\n\n\nReact Native mobile app deployed to iOS, built on-top of Firebase for real-time messaging and Twilio SMS for auth.\n\n\nSponsor API's employed\n\n\nMusicGraph for artist music metrics \n\n\nChallenges we ran into\n\n\nProbably spent too much time on polish, doh! Couldn't complete the global audio/video syncing so had to default to client specific instances.\n\n\nAccomplishments that we're proud of\n\n\nA deployed iOS app in 24hrs, a pretty wicked on-boarding flow (I like the little things)\n\n\nWhat we learned\n\n\nReact Native is still behind and we needed a few Object C bridges to fill the gaps, Artists are really hard to get a hold of... like seriously. However, talking with Alex was nice!\n\n\nWhat's next for Secret Set\n\n\nFinish up the chat features, polish some more, and get ready to deploy to iOS. However, I really am going to start working on outreach to artists, because they are really the key to making Secret Set great!\n\n\n\n\n\n\nBuilt With\n\n\nreact\nreact-native\n\n\n\n\n\n\nTry it out\n\n\n\n\n\n\n\n\ngithub.com\n\n\n\n\n\n\n\n\ngithub.com\n\n\n\n\n\n\n\n\nsecretset.club"
    },
    {
        "url": "https://devpost.com/software/bolterizer",
        "content": "Problem Description\n\n\nHex head nuts and bolts are almost universally the symbol of mechanical engineering, and their presence is everywhere. Unfortunately, every shop has a pile of fasteners that no one wants to sort, and while these fasteners are valuable, they aren’t worth the time to categorize them. Scrap companies also have large amounts of fasteners that are too expensive to sort, so they are simply sold as scrap steel.\n\n\nSolution\n\n\nBolterizer is an automated system for categorizing nuts and bolts. The system uses a webcam to see a selection of nuts and bolts, and then identifies the dimensions of every nut and bolt in the image. Once these nuts and bolts are categorized, conceivably they could be passed to a pick-and-place system to physically sort the fasteners. This would be incredibly valuable for a scrap company, since they could pour fasteners into the machine, turning them from scrap metal into sorted fasteners, which is a steep \nincrease in value from approximately $0.015 per pound\n (Rockaway Recycling, 3/13/16) \nto $1.80 per pound\n (at Fazzio’s Surplus, Glassboro NJ). The machine would pay for itself quickly.\n\n\nTechnical Details\n\n\nBolterizer uses a Microsoft Kinect as a webcam to capture the image of the nuts and bolts, on top of a sheet of paper. The corners of the sheet of paper are mapped to an 8.5” x 11” image at 60 DPI, which accounts for perspective and paper position. A contour finder is used to find the profile of the bolts, and an algorithm was developed to pick out key nut and bolt properties from the nut or bolt contour. All of this was completed in 450 lines of C++ and runs at approximately 30 Hz single threaded on a Core i5.\n\n\n\n\n\n\nBuilt With\n\n\nc++\nkinect\nopencv"
    },
    {
        "url": "https://devpost.com/software/the-phone-magnet-d84gir",
        "content": "Inspiration\n\n\nDuring your lovely trip in abroad, finding accesses to the data/internet connections is such a pain. With many unexpected events and tight schedules, you are more likely to be careless and lose your properties. This is one of the critical issues that most of the people should have experienced before. The Phone Magnet is an absolute solution for those are looking for your lost smartphone with no data/internet connections.\n\n\nWhat it does\n\n\nAfter installing a mobile application and registering for The Phone Magnet, the smartphone can be tracked its location via any browsers as long as that smartphone has a cellular connection to receive and send SMSs. The application will monitor all incoming SMSs and filter out the messages from the specific number which is the Twilio number integrated into The Phone Magnet tracking system. Once the application receive the SMS from our server, it will trigger and send another SMS with the geolocation information attached back to the browser. Additionally, the smartphone can capture an image automatically and send it along with the geolocation just to add a little more information for finding your phone!\n\n\nHow we built it\n\n\nWe used JavaScript for the website front end, node.js for the server running on AWS that manages all the work and an Android app on the phone, which listens for and replies with SMS after carrying out all the necessary actions. The server uses Twilio to send and receive SMS.\n\n\nChallenges we ran into\n\n\nThe most challenging thing we faced was to send the image from a phone to the server. Moreover, the capturing itself is another difficult challenge because it has to capture without manually using the phone.\n\n\nAccomplishments that we're proud of\n\n\nWe brainstormed the ideas from 'why' we make it. And then 'how' we it would help, and to ‘whom’. Therefore we proudly came up with the idea of ‘what’ we would be making, with a clear purpose and a strong belief that it would be used for good.\n\n\nWhat we learned\n\n\nWe learned that sending information such as image is not as simple as we thought. As well as the making of a functional mobile application of auto running when it receives a contact from a specific number.\n\n\nWhat's next for The Phone Magnet\n\n\nWe will definitely develop more additional functions increase the possible of users to find their phones. (ex. voice, dark screen in case of stolen, etc)\n\n\n\n\n\n\nBuilt With\n\n\namazon-web-services\nandroid\ndomain.com\njavascript\nnode.js\ntwilio"
    },
    {
        "url": "https://devpost.com/software/wake-9eqwob",
        "content": "1 - Intro\n\n\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n\n2 - Application Home\n\n\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n\n3 - Initial Blank Screen\n\n\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n\n4 - Alarm Edit Screen\n\n\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n\n5 - Shake Alarm Screen\n\n\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n\n6 - Run Alarm Screen\n\n\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n\n7 - Settings Screen\n\n\n\n\n\n\n \n\n\n\n\nUI Demo Video (Scenes shortened for brevity)\n.\n\n\nInspiration & What it does\n\n\nIf you've ever hit the snooze button again and again and again only to realize hours later that you should have been up long ago, then you've felt the pain that we have felt. Driven by the need for an alarm that genuinely got us up, we set out to create wake. \n\n\nThe app itself is simple. Instead of tapping or sliding a button to disable it, the user must get out of bed and walk a certain distance away to disable the alarm or vigorously shake the alarm for twenty-seconds. These physical triggers engage the user in a way that alarms have not done in the past. Leveraging the GPS and Accelerometer features of modern smartphones we created wake. Additionally, typical snooze features enable users to form bad habits by continuously snoozing an alarm. Instead, wake provides a unique form of negative reinforcement. Users can choose to enable or disable the snooze feature, but there's a catch. The snooze feature must be linked to the user's bank account, and every snooze donates a pre-set amount of money to an entity of the user's choice, such as a charity or political campaign.\n\n\nHow we built it\n\n\nThe application was built in Objective-C for iOS devices. We leveraged iOS’s NotificationCenter, AudioToolbox, AVFoundation, CoreLocation, and CoreMotion frameworks to implement all the features for this application. We integrated \nPlaid’s\n API for payment processing.\n\n\nChallenges we ran into\n\n\nDue to some of iOS’s restrictions, setting the alarm to trigger at a certain time and forcing the user to only use the app was technically impossible. So we could only display a notification for the user to open the app in order to disable the alarm. Additionally, the UILocalNotifications that we used for the event triggers were uncooperative and sometimes would not fire — so we also built a standard timer to alleviate this issue.\nUnfortunately, due to the lack of an Objective-C wrapper/library for Plaid, we were unable to complete integrating their payment processing system.\n\n\nAccomplishments that we're proud of\n\n\nWe were proud of the UI animations, flow, and functionality of the app. We wanted to make the app not only be functional but also to look/feel great. (Watch \nthe video demo\n! (Some interactions shortened for brevity.))\n\n\nWhat we learned\n\n\nWe learned that making a alarm application was not as simple as we thought and that working with AutoLayout (through code) could be very painful at times.\n\n\nWhat's next for wake\n\n\nWe will attempt to finish integrating the Plaid payment processing system and releasing wake to the app store.\n\n\n\n\n\n\nBuilt With\n\n\nobjective-c\nsketch\n\n\n\n\n\n\nTry it out\n\n\n\n\n\n\n\n\ngetwakeapp.com"
    },
    {
        "url": "https://devpost.com/software/sing-me-a-dress",
        "content": "Rapunzel\n\n\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n\nJasmine\n\n\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n\nAriel\n\n\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n\nPresenting at Closing Ceremonies\n\n\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n\nTesting the Lights\n\n\n\n\n\n\n \n\n\n\n\nInspiration\n\n\nWho doesn't want to be a princess?\n\n\nWhat it does\n\n\nAs you sing, we parse the lyrics of the song and match them to a Disney princess's song. The LEDs then take on the pattern of the dress of the princess you are singing.\n\n\nHow we built it\n\n\nMagic and Fairy Dust!\n\n\nKidding. The LEDs are individually programmable Neopixels (by Adafruit), and what we've done to dressify them is map them into different regions on the body-- so we control bust, waist, and sections of the skirt. We can customize per princess following that, but we assign colors to each section once the princess has been identified.\n\n\nTo parse the song lyrics, we use an API called \nDeepGram\n.\n\n\nChallenges we ran into\n\n\nCustomizing Elsa's dress, which doesn't follow standard princess dress configuration. We coded a gradient into the dress, by taking the initial RGB value and ending RGB value and having each R, G, and B increase proportionally. We discovered, however, that they can't linearly change or the colors in the middle of the dress will be on a completely different spectrum since the ratios are off. \n\n\nAccomplishments that we're proud of\n\n\nWe SANG in front of the entire closing ceremonies at MHacks: Refactor. Don't believe me? (\"Ask the dishes!\" Kidding.) Check it out \nhere\n. If that isn't every singing averse engineer's nightmare, I don't know what is.\n\n\nAlso, we built a dress that parsed out lyrics and displayed a princess dress in real time. None of us had experience with LEDs or lyric parsing before this project.\n\n\nWhat we learned\n\n\n40A of power wrapped around your body gets a little toasty. We also learned a lot about each other, our love for Disney tunes, and code! We worked heavily with the Edison trying to integrate it with the Arduino IDE, and although it didn't end up working, guess what group of girls knows about back end Linux now? We also learned about waveforms, phonetic parsing, and how much of a headache you can get from staring at LEDs too long.\n\n\n\n\n\n\nBuilt With\n\n\narduino\ndeepgram\nintel-edison\nios\nneopixels"
    },
    {
        "url": "https://devpost.com/software/f-low",
        "content": "Meet f.low, the intelligent audio control system that knows your surroundings.\n\n\nInspiration\n\n\nHave you ever tried to watch a movie on the bus? Study in public? Listen to music while commuting?\n\n\nWe're guessing you have. And, by extension, we're guessing you've had to deal with the frustrating experience of constantly adjusting the volume to accommodate for your changing environment. Everyday distractions like crying babies and noisy neighbors hinder your productivity, your patience, and the \nsick fiya\n you're dropping in your playlist -- but they no longer need to be sources of stress.\n\n\nWhat it Does\n\n\nUsing the built in microphone on your Mac OS X device, f.low is able to detect how loud your environment is and dynamically adjust your volume on-the-fly, keeping your listening experience consistent.\n\n\nMapping microphone input power to decibel values using our fitting algorithm, as well as letting you set a maximum and minimum volume for f.low to work with, we achieve the sound you want, \nall the time\n.\n\n\nHow We Built It\n\n\nf.low is currently available on Mac OS X, and it's just a quick and easy port away from iOS. We've developed it using Swift and Xcode, making use of the hardware existing on every Mac and iPhone.\n\n\nChallenges\n\n\nAchieving a natural, consistent sound is key to listening experience, so great care was put into analyzing and optimizing the data gathered from the environment and achieving the most natural volume control.\n\n\nWhat's Next for f.low\n\n\nOf course there is much more in store and many ideas that need exploring: further optimizing user experience, improving the validity of our detecting algorithm, and re-vamping the UI are three challenges we'd love to tackle in the future.\n\n\nwww.justgetflow.tech\n\n\n\n\n\n\nBuilt With\n\n\nlove\nswift\nxcode\n\n\n\n\n\n\nTry it out\n\n\n\n\n\n\n\n\nowenkun.github.io"
    },
    {
        "url": "https://devpost.com/software/firenode",
        "content": "Desktop Screenshot\n\n\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n\nMobile Screenshot\n\n\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n\nMath\n\n\n\n\n\n\n \n\n\n\n\nLive Demo! Click here\n\n\nInspiration\n\n\nThe idea for our hack came to us as we decided where we were going to set up for the weekend during the hackathon. As we searched crowded rooms across north campus, we knew there had to be a way to get real time updates about how many people are on campus and where they are studying. From this, FireNode was born. \n\n\nWhat it does\n\n\nFireNode is a real time interior localization platform designed to provide high resolution population density maps of large network infrastructures.\n FireNode is a learning algorithm utilizing big data concepts - as more wifi hotspot data is collected, FireNode's knowledge of the network and hotspot locations expands to reflect more accurate user densities. Our service has three main components; data acquisition, analytics, and presentation. The data acquisition is carried out by a mobile app that reads GPS and wireless network information. This information is sent to a server where data analytics are performed using custom algorithms. Our algorithms determine the location of wifi hotspots through trilateration (similar to triangulation) and determine the number of people connected to a given hotspot. From there, the analytics data is sent to be presented to the user as a map overlaid with relative population density with a resolution down to individual rooms.    \n\n\nHow we built it\n\n\nWe built a mobile Android app to continuously monitor network and GPS information in the background of the Android operating system. The app scans all available wireless hotspots every 5 seconds to retrieve access point IP addresses, MAC addresses and RSSI/frequency. The RSSI (signal strength) and frequency allows us to calculate  distance from the phone to a wireless hotspot. In addition, we monitor GPS location changes to pair network hotspots with a Latitude/Longitude coordinate. \n\n\nWe use Firebase as our data management service, sending the real time data outlined above to the Linode Server. This Linode server hosts our java server, which analyzes and processes over a hundred wifi/gps combinations a second to identify the exact location of scanned wifi hotspots. \nWe accomplish this by using a process called trilateration.\n Each non-linear trilateration calculation use all of the previous data points for a given hotspot, and custom calculations to compensate for the spherical coordinates of the earth. \n\n\nFinally, we have a node server running on our Linode host to gather IP connection information from the network. This node server contains an ARP table which allows us to look at all connected devices on the University campus and aggregate IP addresses. Once we have all available IP addresses, we can figure out how many connections each wireless hotspot has and link it to the location data collected before.\n\n\nChallenges we ran into\n\n\nThere were a couple difficult algorithms that we had to write, involving some difficult mathematics to figure out.  Trilateration functionality on the java server was a difficult algorithm to implement. To get an accurate location out of the trilateration algorithm, we had to figure out how to convert latitude and longitude GPS coordinates to useable distances in a pseudo-cartesian coordinate system. For the mobile app, getting the distance between the user and the wifi router proved to be a challenge. We had to manually calculate the distance using the strength and frequency of the connected wifi signal. In terms of networking, matching BSSIDs to IP addresses in order to find unique connections over the entire network was a big task, given the massive size of the network. It required a lot parsing ARP information and finding suitable _ insights _  from that data.\n\n\nAccomplishments that we're proud of\n\n\nWe are proud of our ability to overcome all of the challenges that we ran into while developing FireNode, ranging from the difficult mathematics of trilateration to parsing the massive amount of data about the network. We are also proud of the low latency time between collecting our data and displaying it to the user, i.e. making our application truly real-time.\n\n\nWhat we learned\n\n\nDominik learned Python.\nSean learned about the Android framework.\nMichael learned about networking via the Android OS.\nDominik taught Antonio that Python has a continue keyword...\nWe still haven't learned what the TBD prize was...?\n\n\nWhat's next for FireNode\n\n\nFireNode can now be expanded in countless ways. As our database grows, we can incorporate machine learning for predictive technologies. FireNode aggregates a lot of data about population density and traffic on a \nroom by room resolution\n, which could be useful to both students and the school. Students who are looking for a place to study or an uncrowded room to work in would easily be able to see in real time which rooms would be the best to work at any given time. Schools on the other hand could use the data over a period of time to gauge and better understand what areas of buildings are used the most, have the highest traffic, and what areas of the building may need more Wifi access points. What is really exciting is that FireNode can be applied anywhere with a large network infrastructure, like corporate campuses, malls, or high traffic areas. \n\n\n\n\n\n\nBuilt With\n\n\na-winning-attitude\nandroid\nbig-data\nfirebase\nindoor-localization\ninsights\ntravel\ntrilateration\nwardriving\nwilddog\n\n\n\n\n\n\nTry it out\n\n\n\n\n\n\n\n\nfirenode.hmika.io\n\n\n\n\n\n\n\n\ngithub.com"
    },
    {
        "url": "https://devpost.com/software/rando-shot-first",
        "content": "The app giving a 64-bit integer generated from the results.\n\n\n\n\n\n\n \n\n\n\n\nInspiration\n\n\nAs the saying goes, a fence is only as strong as its weakest link. This holds true not just for physical security, but for cyber security as well. All of your emails and text messages rely on strong encryption to keep your information safe, and that's why we still possess at least some notion of privacy.\n\n\nAt the heart of strong encryption is the ability to generate random numbers.  Unfortunately, the random number generators software developers typically use are purely deterministic -- once the generator has been seeded, all future values are fixed until the next seed. On a desktop, high-entropy data typically comes from a combination of time, mouse movement, and network data. That type of high-entropy seeding is strikingly lacking when it comes to mobile devices. If a malicious entity is able to collect a sufficient amount of generated numbers, they can predict all of the future random numbers generated by your phone. (The crux of it's encryption technology!)  A successful attacker would be able to predict your keys, and read all of your messages.  \n\n\nThe best way around these potential security vulnerabilities is to use a True Random Number Generator (TRNG), which uses hardware to generate numbers, as opposed to a Pseudo Random Number Generator (PRNG), which uses software.  By relying on a high-entropy physical process instead of deterministic software, a TRNG's results are completely unpredictable.  While some specialized devices are equipped with a TRNG, the vast majority of our devices -- for instance, our cell phones -- rely on inferior PRNGs.\n\n\nWhat it does\n\n\nRando Shot First is able to quickly generate large amounts of truly random data by extracting the noise out of several photographs taken on an iPhone.  We employ an algorithm that computes a one-way transform over the the RGB values of a number of photos' pixels and other methods for \"harvesting\" the entropy from several pixels. The original picture is completely destroyed, leaving only random noise due to both sensor imprecision and physical randomness.  Each set of pictures are able to generate approximately 10,000 truly random bits.  We ran several randomness benchmarks over the data such as the Wald–Wolfowitz runs test, which confirmed that our bitmaps have high degrees of randomness.\n\n\nAfter completing its scan, the app itself displays a bitmap representation of the random bits generated as well as a sample 64 bit random integer.\n\n\nHow we built it\n\n\nWe used Swift and Xcode to create the iOS app, grabbing raw, uncompressed images from the camera sensors.\n\n\nChallenges we ran into\n\n\nWhen we initially ran the app, we only used one picture to generate our random numbers. However, regions of the image with low entropy, such places as by light sources, created large sections of white patterns in the resulting bitmap, not the random behavior we anticipated.  In order to solve this, we have the user take several photos in different positions.  All regions of high entropy are superposed and combine to form a highly random bitmap.\n\n\nAdditionally, it took a while to figure out how to have the camera interact with our app and read pixels from the camera since both Swift and the APIs we needed to use have changed quite a bit over the past couple years. It was difficult finding current example code to reference.\n\n\nAccomplishments that I'm proud of\n\n\nThe end result is smooth and streamlined.  It works consistently with a fluid, easy-to-use interface, and it provides an interesting solution to a problem many people don't usually think about.\n\n\nWhat I learned\n\n\nOver the course of this project, we learned a good bit about random number generation, Swift, and Xcode. I feel a lot more comfortable working with iOS design as well.\n\n\nWhat's next for Rando Shot First\n\n\nImplementation of a messaging client that uses the random numbers for public key encryption.  Additionally now that we've published our open source code, any number of other applications could implement this random number generation algorithm.\n\n\n\n\n\n\nBuilt With\n\n\niphone-sdk\nswift\n\n\n\n\n\n\nTry it out\n\n\n\n\n\n\n\n\ngithub.com"
    },
    {
        "url": "https://devpost.com/software/flow-wjcalo",
        "content": "Document Focus Visualization\n\n\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n\nFlow Report\n\n\n\n\n\n\n \n\n\n\n\ndocuflow\n\n\nEver wondered what goes on in people's minds when they read your writing? \n\n\ndocuflow\n is a document viewer that reads your mind, providing insights on what their thinking. How does it do that?\n\n\n\n\nEyeball tracking\n\n\n\n\nDetects which parts of the text the user is actually reading.\n\n\n\n\nMind reading\n\n\n\n\nLogs focus with the Muse headband.\n\n\nDetects which parts of the text require more effort to read.\n\n\n\n\nReport building\n\n\n\n\nView your document as a heatmap of thought intensity.\n\n\nDiscover new interesting things about your writing.\n\n\nActionable insights into how to improve your work!\n\n\n\n\n\n\nWe used the \nMuse headband\n to measure focus when reading the documents and \nxlabs gaze\n to detect what parts of the document the user is reading. Frontend in React and backend in Flask.\n\n\n\n\n\n\nBuilt With\n\n\ncss\nflask\nflux\nhtml\njavascript\njquery\nmuse\npython\nreact\nsocket.io\nxlab\n\n\n\n\n\n\nTry it out\n\n\n\n\n\n\n\n\ngithub.com"
    },
    {
        "url": "https://devpost.com/software/timecheck",
        "content": "Main View\n\n\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n\nBus Driver View\n\n\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n\nPassenger View\n\n\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n\nDownload Portal\n\n\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n\nDescription\n\n\n\n\n\n\n \n\n\n\n\n\n\nTimeCheck\n\n\nA bus-tracking app designed for HoyaHacks 2016\n\n\nCredits:\nAkshay Goel (\nakshay15@vt.edu\n)\nLucas Conti (\nlconti97@vt.edu\n)\nDagmawi Yeshiwas (\ndagmawi@vt.edu\n)\n\n\nNever be too late again.\n\nA positive change in public service.\n\n\nOver the years society has had no option but to deal with the strict bus schedules presented to them. Sometimes though, it doesn't always work out with your schedule, not exactly at least. We have now implemented a way to find a middle ground where both passengers and drivers can communicate to deliver a better system.\n\n\nWith TimeCheck, passengers have the opportunity to ping bus drivers if they are one minute away from the bus stop as a bus approaches. This notifies the driver that there is someone close by and gives passengers the opportunity to board before departure, taking advantage of a small window to make a big difference.\n\n\nDownload Now!  Available for Android from our website at \nlink\n\n\n\n\n\n\nBuilt With\n\n\nandroid\ncss3\nfabric\nhtml5\njava\njavascript\ntwitter\n\n\n\n\n\n\nTry it out\n\n\n\n\n\n\n\n\ntimecheck.tk\n\n\n\n\n\n\n\n\ngithub.com"
    },
    {
        "url": "https://devpost.com/software/trumpscript",
        "content": "TrumpScript \n\n\nMaking Python great again\n\n\nMission\n\n\nTrumpScript is language based upon the illustrious Donald Trump. As the undeniably best presidential candidate in the 2016 language, we found that the current field of programming languages does not include any that Trump's glorious golden combover would approve of.\n\n\nTrumpScript is our solution to this. It's the programming language Trump would approve of. Just like he is making America great again, we hope our efforts will make programming great again.\n\n\nFeatures\n\n\nOur language includes several convenient features, perfect for any aspiring Presidential candidate including:\n\n\n\n\nNo floating point numbers, only integers. America never does anything halfway.\n\n\nAll numbers must be strictly greater than 1 million. The small stuff is inconsequential to us.\n\n\nThere are no import statements allowed. All code has to be home-grown and Amerian made.\n\n\nInstead of \"True\" and \"False,\" we have \"fact\" and \"lie\"\n\n\nOnly the most popular English words, Trump's favorite words, and current politician names can be used as variable names.\n\n\nError messages are mostly quotes directly taken from Trump himself.\n\n\nAll programs must end with \"America is great.\"\n\n\nOur language will automatically correct Forbes' $4.5B to $10B\n\n\nIn it's raw form, TrumpScript is not compatible with Windows, because Trump isn't the type of guy to believe in PC\n\n\nThe language is completely case insensitive\n\n\n\n\nGrammar\n\n\nThe grammar of the language is fairly convoluted, but here's a taste of the enlightened decisions we've made.\n\n\nArithmetic operators:\n\n\n\n\n'+' and 'plus' do addition\n\n\n'-' and 'minus' do subtraction\n\n\n'*' and 'times' do multiplication\n\n\n'/' and 'over' do division\n\n\n'<', 'less', 'fewer', and 'smaller' all evaluate to 'less than'\n\n\n'>', 'more', 'greater', and 'larger' all evaluate to 'greater than'\n\n\n\n\nControl flow:\n\n\n\n\nUse ',' and ';' to treat compound statements as a single evaluation, similar to how '()' are used in other languages\n\n\nUse ':' and '!' to define the scope of loops and if statements, similar to how '{}' is used in Java\n\n\n'if', 'else if', and 'else' do what you think they do\n\n\n'not', 'and', and 'or' do what you expect\n\n\n\n\nThe exciting parts:\n\n\n\n\nStrings are denoted by double quotes (\"I love Trump\")\n\n\nThe 'is' and 'are' keywords are used both to check for equality, and for assignment. To use for assignment, say something like 'Trump is great' or 'Democrats are dumb'. To use to check for equality, do the same but append a '?'. For example, you may need to ask yourself 'Trump is \"the best\"?' (although we all know that would evaluate to 'fact' anyway)\n\n\nAssignment can also be done via the 'make' keyword. E.g. 'Make America great' assigns the value of the variable 'great' to 'America'\n\n\nPrinting to stdout can be done via 'tell' or 'say'\n\n\nWhile loops are denoted via 'as long as'. And that's the only type of loop you need anyway.\n\n\nIf a 'word' (so anything that could be a variable name) is deemed unnecessary by the compiler, it's simply thrown away. So you can make truly self documenting code, or code that appear to read very very similarly to real speeches by the big man himself. You can find some interesting examples in our 'test/test_files' directory\n\n\n\n\nBut most importantly, Trump doesn't like to talk about his failures. So a lot of the time your code will fail, and it will do so silently. Just think of debugging as a fun little game.\n\n\nHistory\n\n\nCreated for HackRice (hack.rice.edu) by:\n\n\nSam Shadwell, @samshadwell\n\n\nDan Korn, @DnlRKorn\n\n\nChris Brown, @CryoBrown\n\n\nCannon Lewis, @cannon10100\n\n\n\n\n\n\nBuilt With\n\n\npython\n\n\n\n\n\n\nTry it out\n\n\n\n\n\n\n\n\nsamshadwell.github.io\n\n\n\n\n\n\n\n\nmakepythongreatagain.org"
    },
    {
        "url": "https://devpost.com/software/ssh-over-sms",
        "content": "Inspiration\n\n\nInspired by our teammate Ian's previous experience with sending text messages over an email server, we decided that it would be extremely useful to be able to do more with text messages.  In addition, it's often necessary to manage a server on the go, and given that text messages don't require a Wi-Fi connection, they're the perfect way to make sure that we could have access to our server 24/7.\n\n\nWhat it does\n\n\nOur python project listens for a text message (through a dedicated SMTP email server) then reads and parses the email to get a bash command.  Then it creates a subprocess to run the bash command on the server's shell.  It also includes two factor authentication, with a config file and a required usage password.\n\n\nHow I built it\n\n\nWe wrote 3 python scripts and used an IMAP library to create a running Python daemon to listen for bash commands.\n\n\nChallenges I ran into\n\n\nIt was difficult to change directory, since processes terminated after a single command, but we decided to store the directory and track changes.  Error handling with incorrect commands was also difficult, since error messages have different formatting.\n\n\nAccomplishments that I'm proud of\n\n\nThe python script is extremely fast, and took relatively little code to write, given the fact that it can run almost any one line bash command.\n\n\nWhat I learned\n\n\nWe learned how to spawn subprocesses, route text messages through email, and in general, how to use python's massive standard library to accomplish tasks far more difficult with other languages.\n\n\nWhat's next for SSH over SMS\n\n\nThe next step is support two line commands and commands that require interaction, such as \"sudo\".  In the future, we'd like to make it possible to conduct even more complex operations over text.\n\n\n\n\n\n\nBuilt With\n\n\npython\n\n\n\n\n\n\nTry it out\n\n\n\n\n\n\n\n\ngithub.com"
    },
    {
        "url": "https://devpost.com/software/class-v6gjmq",
        "content": "Rules the program follows\n\n\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n\nExample slide from powerpoint\n\n\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n\nWork in progress GUI\n\n\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n\nAn output file from testing the Java class. Names would normally display, not <name redacted>.\n\n\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n\nWorking on the GUI in MATLAB\n\n\n\n\n\n\n \n\n\n\n\nVideo: \nhttps://vimeo.com/152185264\n Password: HackHW\n\n\nInspiration\n\n\nAt the start of HackHW, half of our team was away at a another competition. That left the two remaining members to brainstorm our idea. Our initial idea was to make a large program with many different features, but many of them required more knowledge of connectivity between languages than we knew. After throwing around a few school-related ideas, we settled on our current idea that assists with notes based classes. \n\n\nWhat it does\n\n\nOur program takes facts in a plain text document and turns them into questions to study. Many keywords in the sentence are replaced with the appropriate question words to create as many questions from the fact as there are keywords. These questions can be retrieved from the output file or can be studied using a simple GUI.\n\n\nHow we built it\n\n\nThe GUI is written in MATLAB, and calls API from Mathematica and our Java classes in order to run. The personalNotes class reads in a .txt file with a list of facts, each on a new line, and generates an ArrayList for each fact. These questions are, when not using the GUI, written to an output file in the format of the question text immediately followed by the answer on a new line.\n\n\nTo generate the list of questions, the static method toQuestion is called with a fact as a parameter. This fact is parsed into an ArrayList of individual words in the fact. The method then goes through each word and, if it is a question word as per the rules (see gallery), will create a Question object with the question and answer and add it to the ArrayList that will be returned. This repeats until the entire fact has been parsed.\n\n\nFinally, MATLAB calls the personalNotes main method after setting an input and output file. The Java code runs as described above while Mathematica processes answers typed into the GUI and evaluates their natural-language interpretation (so that, for example, NYC can be interpreted as New York City). Then MATLAB takes the output file from the Java code and uses it to display questions in a short-answer quiz format.\n\n\nChallenges we ran into\n\n\nThere were a number of challenges we encountered along the way. When initially working on this project, we had very large ambitions. For example, we wanted to be able to store and organize facts as well as classify the facts and questions into themes. Upon suggestion that the scope was too large, we narrowed it down to its current state.\n\n\nWe also encountered unsolved issues with turning facts into questions. Initially, the grammar in the outputted questions would be bad enough to render the question incomprehensible. Even after we fixed the problem, often, a question will be understandable, but not quite proper (for example, \"in when the Civil War started?\", is a possible question). Other times it will not be possible for us to know whether a question requires either a who/what or a when/how many. In this case, we have to provide both, leading to more strange phrasings. \n\n\nAnother problem was that during the majority of the time, we were only working on one Java class. This made it difficult for all four members to work on different sections. We were able to solve this using Saros, which allowed multiple people to alter the code at once. However, this was only a partial solution for Saros was unreliable when connecting more than 2 laptops.\n\n\nAccomplishments that we're proud of\n\n\nWe are proud that we were able to work together and have the code work well within 2 days. Our teamwork contributed to our success and it allowed us to have better communication. This lead to a decrease in stress and increase in work productivity. \n\n\nWe also managed to create a fairly consistent set of rules which are able to output useful and understandable study questions. These rules, while not complete and sometimes with exceptions, effectively allow us to find simple facts in statements without teaching our program english grammar.\n\n\nIn addition, we learned how to make a GUI in MATLAB that was effective enough to fluently display many questions. Before this weekend, we had almost no knowledge of GUIs (besides very basic Java GUI understanding). Our proficiency of GUIs will be helpful in the future as well.\n\n\nWhat's next for Class\n\n\nIn the future, we would like to both improve on current features and implement additional ones. First, we would like to improve on our rules for finding key words in the question so we would not need to use terms such as \"who/what\". In addition, we could improve the rules so that the questions are grammatically correct.\n\n\nIn terms of extra features, our highest priority goals are to implement a multiple choice mode with additional choices taken from other proper nouns in the notes and an ability to star questions to study again later. Another goal is to increase the usability of the GUI as well as making one through Java and MATLAB. \n\n\nThe Name \"Class\"\n\n\nWe choose the name Class because it sits in the intersection between school (class rooms) and computer science (classes in programs). \n\n\n\n\n\n\nBuilt With\n\n\napi\ngoogle\njava\nmathematica\nmatlab\nnatural-language-processing\nopen-gui-layout-editor\nsaros\nsimulink\n\n\n\n\n\n\nTry it out\n\n\n\n\n\n\n\n\ngithub.com"
    },
    {
        "url": "https://devpost.com/software/connect-0jifq5",
        "content": "Connect, as the name implies, allows you to quickly connect with new acquaintances. Sharing contact information has never been easier. Connect is useful because it allows you to not only share phone numbers and email addresses but also to easily connect with people on social networks such as Facebook, Twitter, and Instagram from one app, all at once.\n\n\nPeople can download the app and make a profile. When they meet someone new, they can open up the app and search for their friends if they’re nearby. If they are and they have the app, the user can tap on their friend’s bubble and quickly access their contact information, and vice versa. You can add all numbers, emails, usernames, and handles to your contacts with a click of a button. Additionally, the app creates a custom note detailing where you two met, just in case you ever need a reminder. And if you want to friend, follow, or tweet at someone, the app takes you directly to the person’s respective profile so you can easily do so, eliminating the difficulty of searching for them yourself.\n\n\nWe used the trnql API to make all of this possible. Smart People was used to find nearby friends, so you can conveniently add the people you meet. Smart Location was used to display the location at which the user searches for people, keeping the user well informed and improving user experience. Smart Places was used to add a cute touch to our app with the “Where You First Met” note. If you ever need a reminder (which you sometimes do), the app will let you know that you met at your favorite coffee shop at 12 PM in January. Using the trnql API for location based services allows us to keep the app platform neutral. Instead of using Bluetooth or NFC, this way allows us to make an Android version in the future, allowing Android and iPhone users to Connect.\n\n\nIn the future, we plan on making a few changes to the app. We want to automate and streamline the friending/following/adding process. Instead of having the user open the app and physically go through the motions, we hope to eliminate that step by incorporating the APIs of other social media apps. So far we’ve begun with the incorporation of Twitter, and with more time we’ll be able to incorporate the rest. There are some security risks in the current design so we plan to add a feature where users have to request access to contact information. And, as mentioned before, we hope to expand to the Android platform as well. We might also add the ability to include more details, such as birthdays, nicknames, and addresses. Another feature we plan to add is the to pick and choose what details you share with what people.\n\n\nMeeting people can be hard. Connecting with them shouldn’t be. Get the app and start Connecting today.\n\n\n\n\n\n\nBuilt With\n\n\nparse\nswift\ntrnql\nxcode\n\n\n\n\n\n\nTry it out\n\n\n\n\n\n\n\n\nitunes.apple.com"
    },
    {
        "url": "https://devpost.com/software/get-by-sms",
        "content": "Inspiration\n\n\nFind an easy, full protected way to share my private files with other persons.\n\n\nWhat it does\n\n\nUsing this tool you can \nshare\n files without effort. \nHow\n ?\n\nUploading the file and sharing to several phones with a few clicks. \nAnd then\n ?\n\nUnlocking and \ndownloading\n the file sending an SMS.\n\n\nHow I built it\n\n\nI used several tools to build this project:\n\n\n\n\nBase code in \nNodeJS\n\n\nRealtime with \nSocket.IO\n\n\nProject hosting on \nHeroku\n\n\nDatabase using \nMongoLab\n\n\nFile storage \nAmazon\n\n\nUrl Shortener \nGoogle\n\n\nSMS support \nNexmo\n\n\nEmail support \nMailgun\n\n\n\n\nChallenges I ran into\n\n\nI studied several APIs to integrate all of them in this project.\n\n\nAccomplishments that I'm proud of\n\n\nThe most important accomplishment is the result, a small, clean, and  secure tool to share files.\n\n\n\n\n\n\nBuilt With\n\n\namazon-web-services\nangular.js\nexpress.js\ngoogle\ngoogle-url-shortener\nheroku\nmailgun\nmaterial\nmongodb\nmongolab\nnexmo\nnode.js\nsocket.io\n\n\n\n\n\n\nTry it out\n\n\n\n\n\n\n\n\ngetbysms.kpots.com"
    },
    {
        "url": "https://devpost.com/software/zircon",
        "content": "Inspiration\n\n\nIRC is cool, and loads of cool people live on IRC! However, sometime the technical barrier can be somewhat daunting, often off-putting people. We decided to create a hack that aims to bridge the gap between normal people and the very much abnormal subset of people that use IRC!\n\n\nWhat it does\n\n\nYou can send messages to IRC channels via SMS. You can even change your nick and switch server + chan!\n\n\nHow we built it\n\n\nNode js backend that forms a link between the world of IRC and the MessageBird API. Also uses MongoDB somewhere in there for persistence.\n\n\nChallenges we ran into\n\n\nIRC is really awkward about... Pretty much everything. Also sending SMS is expensive, and £5 free credit barely cuts it!!\n\n\nAccomplishments that we're proud of\n\n\nWe built a hack that uses IRC and lived to tell the tale!\n\n\nWhat we learned\n\n\nFreenode is very picky about nick conventions.\n\n\nWhat's next for Zircon\n\n\nPretty much\n. But really, who knows. We'll complete the feature set and maybe add a plugins system.\n\n\n\n\n\n\nBuilt With\n\n\nmessagebird\nmongodb\nnode.js\n\n\n\n\n\n\nTry it out\n\n\n\n\n\n\n\n\ngithub.com"
    },
    {
        "url": "https://devpost.com/software/music-tv-music-videos-for-the-masses-finally",
        "content": "The main display with one song in the queue.\n\n\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n\nThe Web Client connected to a screen. Tracks can be upvoted and downvoted to change their overall places in the queue.\n\n\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n\nThe splash screen of our Android Client\n\n\n\n\n\n\n \n\n\n\n\nInspiration\n\n\nMusic in the UCLU / ULU bars suck, as does music in most of the bars and pubs. They're either designed to send you into a state of perpetual self-loathing, round the bend or make you feel about seventy-three years old.\n\n\nBUT THIS MUST BE FOR NO LONGER!\n\nWe need a musical democracy. The (few and far between) current systems available, such as subTV music, suffer from genuinely awful apps, Windows XP nodes that constantly go down and music videos riddled with adverts that apply to nobody. And what about when somebody does manage to get their say? They're probably going to play the £1 Fish Man. \nOver and over again.\n Shouldn't an app be designed to protect against issues from duplicates from the ground up?\n\n\nWhat it does\n\n\nOur new service brings music to the modern cloud. With only a six character pin on each TV you can control your music with any Android smartphone. Not an Android fan? We also implemented SMS support, just for you special dinosaurs out there. Additionally, all users can vote on the choices of other users, allowing only the most wanted tracks to be played straight away.\n\n\nHow we built it\n\n\nBeing first years/Freshers, we can't claim to have the collective programmatical prowess of the node.js gurus of Silicon Valley. What we do have, however, are a very particular set of skills, skills we have acquired over our very long careers as nerds. Skills that make us a daydream for people like you. We will look for you. We will pursue you. We will find you. And we will bring music back home.\n\n\nOn a more serious note, the backend is mainly Hackathon-esque PHP scripts that can be controlled via super-simple HTTP GET commands. These interface with a MySQL database to provide a complete music API. Atop this, we have an Android app and full web interface to pick songs and vote for them. Further still, our SMS integration can allow even those with the most dated of devices to contribute to the eventual parties that this technology can instigate. The power is in your hand......\nThrow in a good measure of Nginx for low RAM overhead and CloudFlare for DDOS protection, et voilá. Your wish is once again your command.\n\n\nChallenges we ran into\n\n\n1) We initially started off using Dreamfactory for a RESTful API, but could not make it fit our requirements past connecting to the MySQL backend, so we had to implement the API alone.\n2) The server kept going down. 768MB of RAM \nshould\n be fine for Nginx, but apparently not.\n3) It turns out Google's YouTube API behaves differently depending on whether you load a video via its ID or URL, even though they are completely intrinsically linked (the latter is derived from the former!)\n\n\nAccomplishments that we're proud of\n\n\nWe made a fully functioning quasi-real-time system in 24 hours without a single second of sleep (except for one member, poor thing!). We truly believe we have a system that's a greater pleasure to use (despite its gaping security holes, etc., that we \nreally\n ought to deal with) than what exists today, and for that we can be proud.\n\n\nWhat we learned\n\n\n1) Be prepared to ditch a new technology and move on if it isn't working for you in the time provided. Time is precious!\n2) \nSometimes you really do just need to switch it off and on again.\n\n\nWhat's next for Music TV?\n\n\nIt needs some serious revamping in the security department. We did not even attempt to implement rate limiting. It also needs a far beefier server with some optimisation of the code. Perhaps a daemon would be far more efficient than a series of PHP scripts!\n\n\n\n\n\n\nBuilt With\n\n\nandroid-studio\nbootstrap\ncloudflare\ncss\nhtml\nhttp\njava\njavascript\njquery\nmessagebird\nmysql\nnginx\nphp\nvolley\nyoutube\n\n\n\n\n\n\nTry it out\n\n\n\n\n\n\n\n\nprojectnadia.windowshelpdesk.co.uk\n\n\n\n\n\n\n\n\nprojectnadia.windowshelpdesk.co.uk\n\n\n\n\n\n\n\n\ngithub.com"
    },
    {
        "url": "https://devpost.com/software/okane",
        "content": "Dashboard\n\n\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n\nTransaction Success\n\n\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n\nStories List\n\n\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n\nSearch Stories\n\n\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n\nSearch\n\n\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n\nProfile\n\n\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n\nLogin\n\n\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n\nLoaner Profile\n\n\n\n\n\n\n \n\n\n\n\nInspiration\n\n\nHaving to deal with banks when you need money is annoying. Finding investors for micro-investments is annoying. We felt like there should be a platform that connects people who have money to invest and those who need some.\n\n\nWhat it does\n\n\nSay Andrew needs £2000 pounds to kickstart his pizza restaurant, but the bank will not provide him money as he's young and does not have a lot of experience. He uses Okano to publish his story and convinces people with passion to make his dream come true invest into his idea. Friends, nearby locals, and people from all around the world, who have spare cash at the end of the month, feel like helping him. After Andrew returns the money, they get an interest that he has set when creating his Story.\n\n\nYou can also press a magic button that will automatically invest your money in the highest-rated loaners, so all you need to do is see your bank account grow.\n\n\nHow we built it\n\n\niOS app written in Swift 2, with a MEAN backend, the CapitolOne API, Revolut API, the DreamFactory API, JustGiving, and the BrainTree API.\n\n\nChallenges I ran into\n\n\nCreating an intuitive and simple user experience for iOS, and use of APIs.\n\n\nAccomplishments that we're also proud of\n\n\nWe created a web application that predicts how much you could possible earn using our platform.\n\n\nWhat's next for Okane\n\n\nGoing live!\n\n\n\n\n\n\nBuilt With\n\n\niphone-sdk\nmean\n\n\n\n\n\n\nTry it out\n\n\n\n\n\n\n\n\ngithub.com"
    },
    {
        "url": "https://devpost.com/software/medicsms",
        "content": "We provide an interface for healthcare providers to allow additional asisstance where necessary\n\n\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n\nWe detect the user's language, and reply accordingly\n\n\n\n\n\n\n \n\n\n\n\nInspiration\n\n\nOur aim is to allow those in developing countries, with limited or no access to the internet, the same first aid information which is readily available to the world that has easy access to the internet.\n\n\nWhat it does\n\n\nMedicSMS\n empowers those in developing countries that have access to ordinary cell phones with the ability to obtain a first aid diagnosis and advice via SMS. Users simply text in their symptoms, and, using a combination of the Twilio and IBM Watson APIs, we translate the natural language SMS into a likely diagnosis. After a quick dialog of decisions, the user is presented with a suggested course of action as a series of steps for their specific condition. We request location information from the user so that the proper local authorities can both contact the aid the patient.  This, location and symptom data can be provided to local charities to help track the spread of illness and disease in these locations.\n\n\nFurthermore, we take into account the language of the user, using IBM Watson's translation API. We detect the user's natural language and respond to them accordingly.\n\n\nHow we built it\n\n\nThe core of MedicSMS is based on the Twilio and IBM Watson APIs. We use Twilio to receive and send SMS messages to our end users, while we use Watson's natural language classifier to classify the symptoms of an individual and then recommend the best course of action. Our two different backend services are written in NodeJS and Python, while our frontend is using AngularJS, Google Maps API and CSS3. Our services are hosted on Heroku and Google App Engine.\n\n\n\n\n\n\nBuilt With\n\n\nangular.js\ncss3\ngoogle-app-engine\ngoogle-maps\nheroku\nibm-watson\njavascript\nnode.js\npython\ntwilio\n\n\n\n\n\n\nTry it out\n\n\n\n\n\n\n\n\nwww.medicsms.org\n\n\n\n\n\n\n\n\ngithub.com\n\n\n\n\n\n\n\n\ngithub.com"
    },
    {
        "url": "https://devpost.com/software/storytunes",
        "content": "landing page\n\n\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n\nsongs returned for article about christmas\n\n\n\n\n\n\n \n\n\n\n\nInspiration\n\n\nWe wanted to enrich the reading experience and provide new means for music discovery by identifying suitable songs for news articles or stories.\n\n\nWhat it does\n\n\nBased on an article URL provided by the user we get the text, title, keywords and entities from the Alchemy API. We use the musiXmatch lyrics dataset of the million song dataset (\nhttp://labrosa.ee.columbia.edu/millionsong/challenge\n) to generate a list of topic cluster of the available songs. Using the Alchemy information we then classify the article to one of this topics and retrieve the nearest songs. In addition we use the article entities to retrieve top songs directly from music APIs. Those combined results are then surfaced on the front end. \n\n\nChallenges we ran into\n\n\nCombining the various modules to create the end-to-end flow proved to be more time consuming then we thought initially. \n\n\nAccomplishments that we are proud of\n\n\nIntegration of complex machine learning techniques and usage of various APIs and getting the whole thing together in the given timeframe.\n\n\nWhat I learned\n\n\nImportance of clear focus on minimum viable product and early integration testing.\n\n\nWhat's next for storytunes\n\n\nImproving the topic classification of songs and articles by finetuning the LDA algorithm.\nAllowing the user to login and import songs directly to spotify, myhumm or other playlists.\nAllowing the user to rate the retrieved songs.\nImproving efficiency of backend processes.\n\n\n\n\n\n\nBuilt With\n\n\nalchemyapi\ndjango\nhtml\nhumm\njavascript\nmachine-learning\nphp\npython\n\n\n\n\n\n\nTry it out\n\n\n\n\n\n\n\n\nstorytunes.azurewebsites.net"
    },
    {
        "url": "https://devpost.com/software/dynamoui",
        "content": "Dashboard\n\n\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n\nAndroid Library\n\n\n\n\n\n\n \n\n\n\n\nInspiration\n\n\nWe were frustrated with downloading the Hack Western Android app every time it updates. We figured it would be nice if there has an open-source library so that developer can change content in real-time; therefore, users don't have to re-download the app everytime it updates. \n\n\nWhat it does\n\n\nDynamoUI is an Open-Source Android developer library for changing a published app at Real-time. After logging in and authenticating, the client can use our simple UI to make real time changes to various app components such as the text, images, buttons, and theme at real-time. This app has immense potential for extensibility and uses such as a/b testing, data conglomeration and visualization.\n\n\nHow we built it\n\n\nWe use Firebase for synchronizing data between Android and the Web platform, and AngularJs to make use of 3 way binding between the markup, js, and database. The mobile client constantly listens for changes on the database and makes changes accordingly through the use of our extended UI Classes.\n\n\nChallenges we ran into\n\n\nSynchronizing data between AngularJS and Firebase was not always straightforward and well documented for special cases. \n\n\nAccomplishments that we are proud of\n\n\nPublished an Open-Source library for the use of other Android apps in real-time.\n\n\nWhat I learned\n\n\nMaking Android library, AngularJS and Firebase\n\n\nWhat's next for DynamoUI\n\n\nImplement A/B testing so marketers can determine which versions perform better in real time. \n\n\n\n\n\n\nBuilt With\n\n\nandroid-studio\nangular.js\nbootstrap\n\n\n\n\n\n\nTry it out\n\n\n\n\n\n\n\n\ngithub.com\n\n\n\n\n\n\n\n\ngithub.com"
    },
    {
        "url": "https://devpost.com/software/homeline-bling",
        "content": "GIF\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n\n\n\n\n\n\n\n \n\n\n\n\nInspiration/What it Does\n\n\nMany people on the streets have no access to mobile devices or Wifi, and no way of finding a temporary home for the night. Homeline Bling is a hotline homeless people can call using a street payphone, and the hotline will relay the name and location of the closest shelter near the caller. So for those late, cold nights, Homeline Bling will provide you with a home.\n\n\nHow We Built It\n\n\nWhen a caller calls the hotline, which is set up using Twillio, and the location of the payphone is extracted using Google geocaching API. The location of the closest shelter is then calculated through our web application built in Django. Then using Twillio's text-to-speech API, the name and location of the closest shelter is relayed back to the user. The web application also allows representatives from shelters to update or add their information on our database. The website is hosted on the cloud using Microsoft Azure. \n\n\nWhat We Learned\n\n\nWe learned how to use the Django framework, and linking Twillio with a web application. We ran into many problems with the implementation, but we're very proud of what we accomplished in the end.\n\n\n\n\n\n\nBuilt With\n\n\nazure\ncss\ndjango\ngit\ngoogle-geocoding\ngoogle-maps\nhtml5\njavascript\njquery\npython\ntwilio\n\n\n\n\n\n\nTry it out\n\n\n\n\n\n\n\n\nhomelinebling.azurewebsites.net"
    },
    {
        "url": "https://devpost.com/software/nerflock",
        "content": "How I built it\n\n\nKinect based image recognition system. Uses Microsoft's open source Project Oxford to verify facial features in comparison to test subject. Uploads match boolean and comparison value to Firebase server. Based on positive or negative image comparison, will either lock a door or shoot a Nerf Gun Blaster at the approaching subject (hardware implemented by Intel Edison). \n\n\nWhat's next for NerfLock\n\n\nFully integrate Facebook API to populate \"verified\" individuals set through JSON queries.\n\n\n\n\n\n\nBuilt With\n\n\nazure\nc#\nfacebook-login-api\nfirebase\nintel-edison\njavascript\nkinect\nmicrosoft-project-oxford\npython\nvisual-studio\n\n\n\n\n\n\nTry it out\n\n\n\n\n\n\n\n\ngithub.com"
    },
    {
        "url": "https://devpost.com/software/codestellation2015",
        "content": "Recall3d\n\n\nMemory can be a fickle thing, especially when we want to remember a great experience. At the same time, small things can often trigger huge flashbacks. That's where \nRecall3d\n comes in:\n\n\nRecall3d\n is a service to which a user can upload a set of images from an experience they want to remember. We generate a physical, spatial representation of this experience, along with an embedded QR code linking to a hosted version of their images. \n\n\nOur Site\n\n\nImplementation\n\n\nDuring this hackathon, we managed to generate all of our component pieces, and string some of the stack together, but did not have enough time to fully automate the process. At this point, we have workable demos, but it is not ready for full scale deployment. These demos include:\n\n\nStripping of GPS coordinates from EXIF data,\nSizing area around all uploaded pictures,\nExtracting STL Map from USGS API,\nGenerating 3D QR codes from arbitrary string input,\nConcatenating STL, QR code and Marker Pins,\n and Hosting our demo data on a website reachable by QR.\n\n\n\n\n\n\nBuilt With\n\n\ncss\nhtml\njavascript\nopenscad\nphp\npython\n\n\n\n\n\n\nTry it out\n\n\n\n\n\n\n\n\ngithub.com"
    },
    {
        "url": "https://devpost.com/software/3degrees",
        "content": "Inspiration\n\n\nBurns are bad. Lots of people suffer terribly from them, and we, as a medical community - don't know that much about them. Our project seeks to address two major issues in the field of burn injuries:\n\n\n1) Diagnosis through Quantitative Metrics\n\nStatus quo diagnosis of burn wounds is left to the eye of the burn surgeon/dermatologist, as per the following 4 characteristics; Depth, Cause, Appearance, and Level of Pain. (\nDiagnostic Parameters\n) Of these metrics, the distinction of depth is often unclear externally, the latter two are arguably immeasurable objectively, and the last often has no standard of comparison (which complicates the entire diagnostic system). This uncertainty manifests itself in the relatively high variance of burn diagnoses for identical images in double blind studies, even amongst leading dermatologists in the field.\n\n\nBy measuring burn depth by proxy with a newly substantiated biometric (heat, as a function of collagen denaturation), one can objectively determine the damage and full topography of the wound. 2D thermal images can be processed, and using the linear nature of heat conduction, can use 3D temperature levels to indirectly derive wound topography. \n\n\nMuch like an iceberg, a burn only demonstrates a small amount of the wound externally, where there is extensive tissue death beneath the surface both immediately, and uniquely - experiences continued apoptosis after removal of heat stimulus. \n\n\n2) Noninvasive Injury Progression Tracking\n\nCurrent widespread method of depth and wound analysis is done through invasive biopsies. Not only do these inherently disrupt the wound healing process, but they only give one unique data point per biopsy. Due to the organic/nebulous shapes of burn wounds, however, these 'guessing points' from biopsies do not nearly paint an accurate picture of the state of the entire burn. In fact, depending on where they are taken, they can improperly generalize the state of the entire wound and lead to incorrect, or at best, quasi-arbitrary dosage information. \n\n\nWhat it does\n\n\nTakes a 2D IR image as an input, creates a dynamic 3D model, computes pertinent wound properties, diagnoses burn degree, calculates post-trauma fluid requirement.\n\n\nHow I built it\n\n\nPython serves as a middleman by taking a 2D orthogonal thermal image from our iOS/Android frontend input to our Mathematica script on the backend, called via the Wolfram Cloud Platform RESTful API. Image is decomposed pixel by pixel, assigning a given Z (depth) value to each coordinate in the XY plane based on the RGB color value from the original image. Points are then restructured and plotted using Wolfram 3DMap and Plot functions, smoothed to a surface of Isotherms, exported, and handed off to our front end again using Python for dynamic 3D Visualization. Further analytics are processed using Wolfram to attain values for \nburn degree\n, various other burn characteristics (\nsurface area, volume\n), and estimated recovery time (note: varies significantly per person, so it's fairly inaccurate, but we wanted to implement the capability as the body of research attains a more accurate rate function).\n\n\nMinimum fluid requirement in 24 hours post-trauma\n \n(as per the Parkland Equation)\n, is calculated, an essential treatment parameter for burn victims. \"Burned Body Area %\" is a fairly difficult value to attain for most people, so we offer a much more intuitive system that includes either estimating the wound size relative to the effected limb(s) (see: [Wallace Rule of Nines])(\nhttp://www.remm.nlm.gov/burns.htm\n), or even easier, a 'Draw' option onto a vectorized body, which then automatically computes the % wound coverage by comparing altered pixels to original pixels.\n\n\nChallenges I ran into\n\n\nWe don't have any thermal camera hardware here, but there are multitude that exist in industry (Nick worked with a couple in the past when he conducted Dermatology research at Stony Brook Medical Center). If possible, we would want to work with the new \nFLIR One\n. They are small enough to easily be transported for medics or emergency personnel, and can seamlessly be integrated with either iPhone or Android phones into any medical professional's repertoire - whether it be a large hospital or a small private practice. \n\n\nAccomplishments that I'm proud of\n\n\nWe were pretty stumped and demoralized, not coming up with a project idea until Saturday morning.\n\n\nNone of us have any mobile dev experience, and we were still able to push functioning (and arguably, good looking) iOS AND Android apps. We literally coded our first lines in Android Development Studio and Swift 24 hours before this was submitted. \n\n\nWhat I learned\n\n\nWhat's next for 3Degrees\n\n\n\n\nImproving relative size accuracy by implementing more precise depth calibration. \n\n\nIntegrating some sort of infrared camera to directly streamline the entire diagnostic process from image capture to analytics and injury progression tracking.\n\n\nMake things prettier. As this was our first time making mobile apps and using the Wolfram Alpha/Mathematica cloud platform, we were heavily prioritizing getting all core functionality working before all else.\n\n\nIntegrating Wolfram Anatomy Libraries\n\n\nBuilding out a proprietary dataset from user uploaded images (heat/depth over time) to begin training a Machine Learning model for burn injury progression.\n\n\n\n\n\n\n\n\nBuilt With\n\n\nandroid\nflir\nmathematica\npython\nswift\nwolfram-technologies"
    },
    {
        "url": "https://devpost.com/software/powergrab",
        "content": "Inspiration\n\n\nWe wanted to find some way, some how to reverse wireless charging. There are energy waves surrounding us at all times, so how cool would it be to leverage that!? PowerGrab uses one form of energy available - vibrations - and converts it in to a DC power supply. \n\n\nWhat it does\n\n\nA piezoelectric transducer captures the vibrational energy from a Pebble watch. The Pebble and transducer are tuned so that their natural frequencies are as close as possible and a rectification circuit converts the output of the transducer in to usable DC power.\n\n\nHow I built it\n\n\nWe spent many careful hours characterizing the piezo and finding its natural frequency, and then doing the same for the Pebble. Next we attached the piezo to the Pebble and began tuning the Pebble's frequency to achieve resonance (a very tricky process). As we approached resonance, the piezo's output voltage increased, so we continued until we got to a usable amount of voltage output with the Pebble vibrating. Next up was the rectification circuit. We needed to convert the AC signal to a DC supply so we implemented a wheatstone bridge with four diodes for full-wave rectification. Next, we tested varying smoothing capacitor values and picked the best of what we had. Finally, we were able to power an LED! \n(We would have loved to boost our DC voltage up to 4 V and feed it back in to the Pebble, but alas, we were missing a few critical hardware components for that.)\n\n\nChallenges I ran into\n\n\nFrequency tuning is a difficult process with the right equipment and piezo's do not give out too much voltage without resonance. Therefore, we had to spend hours upon hours trying to bring down the frequency of the Pebble to get closer to the natural frequency of the piezo. \n\n\nWhen we finally got that working, the struggle continued in to the rectification circuit: we lost any voltage that we thought we had (we were getting less than 0.2 VDC, yikes). We had to take a step back and debug the circuit step by step, try out different hardware components, re-tune the frequency, change components, re-tune, and on and on. \nWe weren't able to get the measly 0.2 V up to a usable 1.85 V until late in to the night. \n\n\nAccomplishments that I'm proud of\n\n\nWE POWERED AN LED WITH A PIEZO! (and we're excited about it =) ) This project idea was ... a stretch, to say the least. We couldn't find any examples of this being done online and thought we might be crazy to be embarking on a project that might be truly impossible scientifically. BUT, we saw a small opportunity for crazy cool power harvesting and the math said it was \npossible\n (though highly implausible), so we dove in. It took days of frustration and seemingly thankless hardware tuning, but in the final hour... WE POWERED AN LED WITH A PIEZO!\n\n\nWhat I learned\n\n\nTuning natural frequencies is quite challenging and piezos are incredibly sensitive. A lot of variables need to be in line perfectly for this to work. Also...you can indeed get free power from the environment if you have a lot of patience and hot glue. \n\n\nWhat's next for PowerGrab\n\n\nWe need to get our hands on a charge pump and boost this sucker up to 4.0 V (the minimum for charging a Pebble battery), just to see once and for all if we can get that beautiful charging icon to light up. Then we need to manufacture this piezo setup in large quantities, place them around noisy cities and construction sites, and grab some power. =)\n\n\n\n\n\n\nBuilt With\n\n\nhardware\npebble"
    },
    {
        "url": "https://devpost.com/software/dragon-story",
        "content": "Welcome to Dragon Story ! This is the app where you can draw a custom character and watch him come to life ! \n\n\nAs story unfolds, children need to draw other objects as well. Their drawings are interacting with the game world and can trigger certain events. Like here, where user needs to draw a rain cloud to fill a large hole with water and then draw a boat for the boy to sail on the other bank.\n\n\nYou draw your way through different obstacles to finally meet a dragon. Grab a pencil and see if you can defeat him! Unique drawing mechanic makes it a great fun ! \n\n\n\n\n\n\nBuilt With\n\n\nwill-sdk\n\n\n\n\n\n\nTry it out\n\n\n\n\n\n\n\n\nintelimind.pl"
    },
    {
        "url": "https://devpost.com/software/obsidio",
        "content": "Inspiration\n\n\nOur team had previously built a game, Tank Anarchy (tankanarchy.herokuapp.com), using the same engine of node.js and socket.io in a top-down arena game. While brainstorming, we realized the potential awesomeness in combining tower defense with top-down shooter, and thus Obsidio was born.\n\n\nWhat it does\n\n\nObsidio is a top-down shooter in its core, with a WASD-controlled player who faces in the direction of the mouse cursor and fires a bullet when the left mouse button is clicked. Players, in the interest of killing other players, collect resources known as Praesidia and use these resources for constructs such as turrets and walls. These constructs can be placed anywhere within the world, and it's up to the player to discover the most effective combination of constructs to create an impenetrable base or to wreak havoc on other bases!\n\n\nHow we built it\n\n\nWe built this game using the fundamental pieces of our previous game, Tank Anarchy. The game utilizes node.js and socket.io for the physics engine and the multiplayer framework, HTML5 Canvases and CSS for the graphics, and NPM, Gulp, and Bower for project management.\n\n\nChallenges we ran into\n\n\nWe generally did not encounter large hurdles aside from the initial debate about whether the players should be people or floating vehicles (???). However, as in any coding project, we encountered bugs by the truckload, requiring an inordinate amount of console.logs and error messages. We also encountered sleepiness and failing cognition as it got later (or earlier), taking a large chunk out of potential coding time.\n\n\nAccomplishments that we're proud of\n\n\nWe are proud of everything we've accomplished! The project looks good, the game works well, and we aren't in too bad of a shape. Our code is also clean and modular enough to support the addition of new features easily, and isn't (too) full of hacks and workarounds (even though it's a hackathon!).\n\n\nWhat we learned\n\n\nWe learned to work together, to streamline the development process, and of course to always break a switch case!\n\n\nWhat's next for Obsidio\n\n\nWe will continue development for sure - there are many ideas just waiting to be implemented, and maybe we can even pull in a graphic designer to design some better-looking sprites. Keep an eye out for our progress!\n\n\n\n\n\n\nBuilt With\n\n\nnode.js\nsocket.io\n\n\n\n\n\n\nTry it out\n\n\n\n\n\n\n\n\ngithub.com\n\n\n\n\n\n\n\n\nobsidio.herokuapp.com"
    },
    {
        "url": "https://devpost.com/software/acunsubscribr",
        "content": "ac-unsubscribr\n\n\nThis little pile of hacks makes up a chrome extension for office 365 outlook users who are tired of unwanted newsletter emails.\n\n\nThe extension scans your inbox for emails with an 'unsubscribe' link and makes it easy to unsubscribe without even opening the newsletter email or navigating to your inbox.\n\n\nThe code was put together as a hackathon submission at the 2015 angular connect conference in London.\n\n\nComplications\n\n\nIt turned out that the office application we registered to allow outlook api calls wouldn't accept redirecting back after authentication to a chrome extension url. It considered the url to be invalid. To work around this issue, we put together a small node.js server to handle the authentication flow itself inside an iframe. It delegates to a browser tab when authentication is required (the authentication process doesn't allow calls from an iframe).\n\n\nThe node server could be hosted in the cloud, so its presence in the solution isn't a complete deal breaker.\n\n\n\n\n\n\nBuilt With\n\n\nangular.js\nchrome\nnganimate\nnode.js\noffice-365\n\n\n\n\n\n\nTry it out\n\n\n\n\n\n\n\n\ngithub.com"
    },
    {
        "url": "https://devpost.com/software/distractedness",
        "content": "Inspiration\n\n\nThe automotive industry has spent years trying to improve driver safety by enabling features that simplify operation and take responsibility from the driver. This, combined with the growing amount of distractions due to mobile tech is arguably creating inferior drivers and a hazardous environment for others.\n\n\nWe think that by providing metrics for driver awareness and mood, we can inspire positive changes in driving habits and an added appreciation for driving smart and safe.\n\n\nWhat it does\n\n\nUtilizes vision enabled by the Kinect to track face movement (particularly eyes). By following eye and mouth movements, we can record metrics to track distraction and mood. In particular, we analyze smiling, head angle and eye focus. In addition we synchronize this data with map and speed information to develop driving trends and point out hazardous patterns or reward improvements.\n\n\nIn particular, the tracked metrics are distractedness, happiness and fidgetiness, but more could be added such as hand position and center of gravity.\n\n\nThe metrics are logged on web app accessible from browsers and mobile devices. It is designed to be simple and easy to understand. An overall score is determined every drive, as well as specific categories  for more detail. A user can look at a timeline map that displays markers on the coordinates of significant distraction events. Ideally patterns may emerge and habits may change.\n\n\nHow we built it\n\n\nThe Kinect is processed with the supplied api. We process various face information and print it on the stream and send it out to our flask server. We used python for post processing, analysis and integrating the server. Node and React were used on the mobile phone side to pull Google maps data and draw event markers on a map. The front end is a combination React, jQuery and Google material design.\n\n\nChallenges we ran into\n\n\nMany of our early struggles were on the vision side. It was tough to find example of advanced facial recognition on the Kinect, and working with Visual studio. The python processing was also difficult since we wanted to simplify all the data in to something any user could understand. We struggled with React and cloud web services and spent much of our time bouncing around between services. In general our team put emphasis on learning new things (React, Python Flask, Visual Studio, ect) rather than staying in our comfort zone. This inherently made things difficult.\n\n\nAccomplishments that we proud of\n\n\n\n\nAccurate, reliable face tracking\n\n\nVisual C++\n\n\nCreative metrics\n\n\nGoogle maps\n\n\nReact\n\n\nHardware\n\n\nSimple design\n\n\nMaking something that has potential to improve safety\n\n\n\n\nWhat I learned\n\n\nExpect anything. This project spawned after a team member was hit by a car driven by a distracted driver. Many roadblocks emerged along the project's development, but we we continued to move forward!\n\n\nWhat's next for distractedness\n\n\nImplement it in an actual vehicle, add a social media aspect\n\n\n\n\n\n\nBuilt With\n\n\nc++\njquery\nkinect\npython\nreact\nsurface\nvisual-studio\n\n\n\n\n\n\nTry it out\n\n\n\n\n\n\n\n\ngithub.com\n\n\n\n\n\n\n\n\n159.203.247.122"
    },
    {
        "url": "https://devpost.com/software/wyshlist-oqj0ps",
        "content": "Inspiration\n\n\nMy Amazon wishlist is unfortunately effective at emptying my bank account on the internet, and being able to do that in real life seemed like a pretty solid business plan\n\n\nWhat it does\n\n\nwyshlist lets users pick categories of items they're interested in buying (\"A new jacket,\" \"A pair of boots\") and then uses iBeacons to connect them to deals at their favorite stores\n\n\nHow we built it\n\n\nBootstrap, Angular, Haskell, Bluetooth LE. Runs on an AWS instance\n\n\nChallenges we ran into\n\n\nWe couldn't find a real iBeacon, so we had to get a Mac running Mavericks to emulate one. Our team is very new to the hackathon scene, so understanding exactly what goes on at a hackathon and best practices for sleeping, eating, etc. was a bit rough at times\n\n\nWhat's next for wyshlist\n\n\nA hardware management system - a handheld watcher that an employee can walk around a store with, to check to see if any beacons have died.\n\n\nYou can try our app in person at the expo booth!\n\n\n\n\n\n\nBuilt With\n\n\nandroid\nangular.js\nbootstrap\nflask\nhaskell\nionic\npython\ntodo-backend\nwow.js\n\n\n\n\n\n\nTry it out\n\n\n\n\n\n\n\n\nklatz.co"
    },
    {
        "url": "https://devpost.com/software/sns",
        "content": "Inspiration\n\n\nWe love Apple Pay but can't get everything we need from Walgreen's alone. We wanted to build a wireless payments system that works anywhere we do!\n\n\nWhat it does\n\n\nSNS wirelessly simulates a card-swipe on any credit card terminal right from your phone. SNS works at any credit card terminal that allows you to swipe.\n\n\nHow We built it\n\n\nWe bottle up a card number and expiration date into a WAV container, play that audio track out of the phone's headphone jack, amplify it just a little, and then blast it through our small induction loop which simulates a credit card swipe by emulating the magnetic field created by a real card stripe on a reader head.\n\n\nChallenges I ran into\n\n\nBuilding the induction loop.\nConverting the number and expiration date to a WAV file.\nDecoding the WAV file from audio input\nPointers in C...\n\n\nAccomplishments that I'm proud of\n\n\nSomehow making this whole thing work.\n\n\nWhat I learned\n\n\nPhysics\nHow WAV files work\nHow credit card payments are handled\n\n\nWhat's next for SNS\n\n\nSleeping.\n\n\n\n\n\n\nBuilt With\n\n\nanalog\nandroid\nios\nmagnets\nsquare-reader\ntape\nwire"
    },
    {
        "url": "https://devpost.com/software/informant",
        "content": "Informant -- How it works & What it does\n\n\n\n\nInformant is the most powerful way to enhance your YouTube viewing experience.\n\n\nSee someone you don't recognize? Press on the face of the celebrity that you don't quite recognize to identify him for her with a short blurb from Wikipedia all right within your browser. \n\n\nWe additionally monitor the actual content of the YouTube video to provide a more in-depth look at how participants within the video interact with each other. By providing sentimental analysis and utilizing NLP technologies we summarize who and what actions are being taken place live on the screen and at the end of the video provide a conclusive view on the relationship between participants within a conversation through color representation.\n\n\nWe were able to come up with who says what by utilizing sound processing techniques -- distinguishing between frequencies for male and female differentiation and analyzing who is speaking by what they say.\n\n\nTech Stack\n\n\nChrome Extension (JS, HTML, CSS)\nNode.JS & ExpressJS Server\n\n\nWit.AI (NLP)\nProject Oxford (Microsoft Computer Vision)\nIndico.io (Sentiment analysis)\n\n\nDownload YT files as MP4.\nConvert MP4 to MP3.\nSlice MP3 files into 10 second chunks.\n\n\nSignal Processing\n\n\nFFFMPEG \nLAME MP3 Encoder - Libmp3lame\nSoX (Sound Exchange) - Audio splitter\n\n\nTechnical Difficulties and Challenges\n\n\nThis challenge was full of technical challenges in regards to barriers set by the Chrome Extension to the local file system and audio manipulation to identify speakers. In order to use the Wit.AI API we had to somehow get the AUDIO clip of a YouTube video, then break it down into chunks of less than 10 seconds, then identify who is speaking during each 10 second clip duration.\n\n\nWe ended up using a sort of pipeline to process the audio input that went sort of like this:\n\n\n\n\nDownload video as MP4 from YouTube (Download)\n\n\nUse FFMPEG to convert from MP4 to MP3 (Convert)\n\n\nUse MPSPLT and SoX to split up audio into appropriate sized chunks of < 10 seconds (Slice)\n\n\nSend each chunk up into Wit.AI to be processed into a Queue Data Structure (Analyze and Identify Speaker)\n\n\nLong-poll from the Chrome extension to the Node server to get new data from the Queue. (Get Data)\n\n\nDisplay results\n\n\n\n\n\n\n\n\nBuilt With\n\n\naudio\ncomputer-vision\njavascript\nmongodb\nnatural-language-processing\nnode.js\nwit.ai"
    },
    {
        "url": "https://devpost.com/software/propagate",
        "content": "Propagate\n\n\nis a beautiful underwater seascape that can be both a calming solitary experience, or an engaging social one.\n\n\nWatch as diverse species of virtual flora flourish and fade in turn, in a persistent environment, synchronized to users anywhere in the world.\n\n\nEach species is defined by yourself or others, using the Lua scripting language and our simple API + integrated editor and testing stage. When you've designed a species you're proud of, push it live and see it interact with the ecosystem. \n\n\nA trivial example:\n\n\nwhile 1 do\n     grow(\"up\")\nend\n\n\n\nHow it runs\n\n\nHTML5 Canvas running on the client, communicating via web socket with the server.\n\n\nServer is running Go and spinning off 100's of Lua VM's to sandbox user-generated code and control plants.\n\n\nThe server and VM's communicate concurrently using channels and go routines.\n\n\nChallenges\n\n\nPropagate has a complex (probably over-engineered) architecture, consisting of three major parts.\nThe Client, the Server, and the Lua VM's running sand-boxed flora code. These components all needed to communicate in a thread-safe concurrent manner, and these channels+websockets were the primary challenge and presented lots of iterations and interesting debugging. We had some bizarre stuff happening in the terrarium!  \n\n\nBuilding a concurrent system meant that we needed to clean up all the concurrent processes our program created. This was challenging because it is hard to design a system that cleans up after itself when moving so quickly to improve features. \n\n\nGo is not an ecosystem that our entire team had used prior to this event, but we decided to use it as our server side for it's excellent concurrency story. This meant that half our team was getting mentored on Go best practices as we laid down the framework for Propagate, but by Saturday night we were all able to be productive and confident in the code-base.  \n\n\nFinally, tweaking mechanics to achieve a satisfactory ecological balance was an important challenge as Propagate came together. We wanted the ecosystem to be alive and dynamic, meaning that old plants gradually withered away to make room for young. An interesting mechanic that emerged is the way that plant's tendencies to drop spores in close proximity to themselves lead to \"forests\" of one species\n\n\nGoing forward\n\n\nWe would like to explore algorithmic iteration of Propagate programs, including generational mechanics and fitness values. We also want to expand the ecosystem simulation to include an entire food chain of procedural critters \n\n\n\n\n\n\nBuilt With\n\n\ngo\ngulp\njavascript\nlinode\nlua\nwebsockets\n\n\n\n\n\n\nTry it out\n\n\n\n\n\n\n\n\n45.79.135.161\n\n\n\n\n\n\n\n\ngithub.com"
    },
    {
        "url": "https://devpost.com/software/happyhour",
        "content": "Inspiration\n\n\nFor students away at universities, it has become more important than ever to know how to take care of oneself. As college students, we are exposed to a variety of things, including alcohol. Even though we've all had educational classes or seminars in school about drinking safely, sometimes a gentle reminder can make a huge difference. That's where we came up with the idea of \nHappyHour\n, a pebble app that can keep you safe. \n\n\nWhat it does\n\n\nHappyHour initially asks users for a couple of inputs: height, weight, and gender. These parameters will be used to calculate the blood-alcohol-content (BAC). BAC is a universal measurement of the alcohol in an individual's system. It is a complex equation that our application simplified for each individual personally. \n\n\nFrom then onwards, users can specify a '+' or a '-' button, allowing them to keep track of how many drinks they've had with a click of a button. The BAC accordingly updates and creates a vibration, reminding them about their drinks. When the BAC is above 0.08 (the legal threshold for drunk driving), HappyHour creates a longer vibration, indicating that the user should take an Uber home. A click on the \nselect\n button will trigger an http request that the pebble sends to our node.js server, where we use Uber and Twilio API to text the user information on the nearest Uber, time required to take them home, and estimated price. \n\n\nChallenges We ran into\n\n\nDeveloping on Pebble Time was a learning curve at some points, especially when we were having trouble uploading color resources and performing GET http requests to send longitude and latitude data. \n\n\nAccomplishments that We are proud of\n\n\nWe are definitely proud of parsing through the Uber JSON API data, as well as setting up Pebble Time to perform GET XMLHttp requests and extract information from that to obtain the geographic coordinates of the Pebble. The entire process of getting user-selected data and displaying the BAC in real time was also difficult, but we've learned a lot in the process!\n\n\nWhat's next for HappyHour\n\n\nWe hope to clean up the user interface code a bit (it's difficult to have good coding habits at hackathons!). We also hope to be releasing the app for othesrs to see and use. \n\n\n\n\n\n\nBuilt With\n\n\nnode.js\npebble\nsquarespace\ntwilio\nuber\n\n\n\n\n\n\nTry it out\n\n\n\n\n\n\n\n\ngithub.com"
    },
    {
        "url": "https://devpost.com/software/quickactions",
        "content": "Inspiration\n\n\n\n\n3D touch in iPhone 6S.\n\n\nRight click menu in the desktop world\n\n\n\n\nWhat it does\n\n\n\n\nProvides a 3D touch like experience for Android phones without any integration with App Developers.\n\n\nHovering over Maps will show you actions like - \"Drive to Home, MyLocation\". If the user chooses \"My Location\", maps is opened showing user's location.\n\n\nHovering over LinkedIN, you get actions - \"Your Profile, Updates, Compose\". Choosing \"Your Profile\" opens your profile directly which was otherwise 3-4 taps away.\n\n\nWe have shortlisted popular apps like (fb, linkedin, twitter, maps, phone) as of now.\n\n\nHovering over a link in twitter, gives a peek into the content in a window which destroys automatically in 5 sec if the user doesn't want to read post the summary. It is super fast for the user to look at multiple links without jumping out of twitter, once the user starts reading a link the auto-destroy isn't effective and control goes back to user.\n\n\n\n\nReaching the Team\n\n\n@SerialHackers - Follow us for more nerdy and cool stuff : \nhttps://twitter.com/SerialHackers\n\n\nIs lt live?\n\n\nYes you can download it here - \nhttps://play.google.com/store/apps/details?id=io.quicly.android\n\n\nHow I built it\n\n\n\n\nWe use Android Overlays to provide a small icon, hovering over HomeScreen and inside any app.\n\n\nOnce the user drags the QuickActions icon on top of an app-icon/link, we use Android Accessibility Service to scrape the screen context (Icon Name/Link Text)\n\n\nWe reverse engineered popular android apps to discover their deep links.\n\n\nClicking on an action we open the app with the deep link.\n\n\n\n\nChallenges I ran into\n\n\n\n\nScreen scraping using Accessibility Service to get the right app name.\n\n\n Discover deep links of apps. Tried using SDK's from URX, deeplink.me but we had to fallback to reverse engineering the apps to discover deep links.\n\n\n\n\nAccomplishments that I'm proud of\n\n\n\n\nSuper simple 3D touch like experience for end-users without hardware costs/developer integration.\n\n\n\n\nWhat I learned\n\n\n\n\nDeep concepts of Android\n\n\n\n\nWhat's next for QuickActions\n\n\n\n\nLaunch in play-store, get users, get feedback and iterate.\n\n\n\n\n\n\n\n\nBuilt With\n\n\nandroid\nkotlin"
    },
    {
        "url": "https://devpost.com/software/ioume-a-peer-to-peer-small-loan-lending-platform",
        "content": "Inspiration\n\n\nWe were inspired by our lack of money and our frequent need for emergency cash to get things done like pay a bill, buy a cheap car, go to a family reunion.\n\n\nWhat it does\n\n\nIOUme allows CapitalOne customers to transfer money from their accounts to other accounts of friends that they know.  When someone needs a loan, they now have the option of sending a request for a specific amount to friends and contacts of theirs that are part of the platform.  The lender can lend securely and safe to their friend and track the repayment of the loan on the IOUme platform.  The borrower defines the amount and the length of the loan.  The lender defines the interest rate and if the borrower accepts, money is automatically transferred.  CapitalOne has the opportunity to earn 1% from each loan.\n\n\nHow I built it\n\n\nWe used iOS and the CapitalOne API.  We used software such as XCode and Photoshop.\n\n\nChallenges I ran into\n\n\nWe are not particularly strong in coding since we are newbies but we were able to consult with the CapitalOne mentors to apply the API.\n\n\nAccomplishments that I'm proud of\n\n\nWe are proud of the user interface that we constructed since we kept in mind the ease of use and perspective of the user.\n\n\nWhat I learned\n\n\nWe need to map out deliverables and keep track of progress throughout the process\n\n\nWhat's next for IOUme: A peer-to-peer small loan lending platform.\n\n\nWe would like to talk with CapitalOne about the potential for this product especially since no big banks have yet jumped into this market.\n\n\n\n\n\n\nBuilt With\n\n\ncapitalone-api\nios\n\n\n\n\n\n\nTry it out\n\n\n\n\n\n\n\n\nioume.folau.us"
    },
    {
        "url": "https://devpost.com/software/tvtalk",
        "content": "Inspiration\n\n\nPeople like talking about TV shows on social media as much as they do actually watching them. It engages us in discussion and has even been proven to \nboost a show's ratings\n. But for us, it's somewhat of an annoyance to shift our glance back and forth from our TV to our smartphones or laptops.\n\n\nWhat it does\n\n\nTVTalk is a DirecTV app that displays an IRC-esque chat room for the TV channel you're currently watching right on your screen and updates in real time. The app itself takes up a narrow amount of space on the left side of the TV, and its transparent background doesn't block the picture.  Using a web browser on your computer or smartphone, you can instantly join the discussion. Even if you don't want to engage in it, you can still just watch the feed.\n\n\nHow we built it\n\n\nOur app was built with HTML5 and Javascript, using several extensions provided by the DirecTV platform. Node.js served as the back end. All testing was done on the set top box.\n\n\nChallenges we ran into\n\n\nThe set top box was unresponsive at times,  forcing us to reset it which took up to 5 minutes.  We estimate that we've lost at least 50 minutes of hacking time just by waiting for it to reboot. \n\n\nA lot of web development features were disabled on the set top box. It couldn't display certain images and colors, and there was a very small collection of supported fonts; as a result, our app lacks polish, but it allowed us to focus more on functionality. The box also doesn't support https connections, which proved to be problematic when we attempted to deploy our app to Firebase.\n\n\nAccomplishments that we're proud of\n\n\nWe were able to build our own chat system from the ground up as well as understand most of DirecTV's API. We believe that what we've made can become part of the future of television.\n\n\nWhat we learned\n\n\nWe learned a lot about The DirecTV platform and that it has a lot of potential to become something big. The DirecTV team was always willing to help us out with any troubles that we had. They gave great answers to our questions, offered useful advice, and came over to us to check on our progress. We'd like to give a big thanks to them for all their support this weekend.\n\n\nWhat's next for TVTalk\n\n\nIf we were to turn this into a real product, ideally we would want the TV networks to moderate their chat rooms. Not only would they be able to filter out inappropriate content, but they can also produce content unique to the program currently on the channel.\n\n\n\n\n\n\nBuilt With\n\n\ncss3\ndirecttv-developer-platform\nhtml5\njavascript\n\n\n\n\n\n\nTry it out\n\n\n\n\n\n\n\n\ngithub.com"
    },
    {
        "url": "https://devpost.com/software/wearshare",
        "content": "WearShare\n\n\nWearShare is an app for Android Wear smartwatches which allows users to send and recieve files without having to use their mobile phones by leveraging the Send Anywhere API. The 6 digit pin obtained using Send Anywhere's API allows for the app to greatly simplify sharing multiple files of varying sizes.\n\n\nFeatures\n\n\n\n\nWirelessly clicks photo using the camera on the user's mobile device and allows the user to preview and share it with others quickly using their smartwatch.\n\n\nInbuilt file explorer for the contents on the user's mobile device.\n\n\nAllows users to share multiple files and folders which are located on their phone from their watch.\n\n\nShows the progress of upload and download in the notification center.\n\n\n\n\nWorking\n\n\n\n\nThe interaction of the app with Send Anywhere's API comletely takes place on the user's mobile device and hence saves battery on their smartwatch.\n\n\nPhone and the smartwatch exchange information using Android's \nWear API\n.\n\n\n\n\nFuture Improvements\n\n\n\n\nSupport for iOS and Apple Watch.\n\n\nCompanion app for the mobile device.\n\n\nQR Code on the smartwatch along with the 6 digit key.\n\n\nAdd more options to the send menu on the smartwatch such as music or pictures and allow users to preview them before sharing.\n\n\n\n\n\n\n\n\nBuilt With\n\n\nandroid\nandroid-studio\nandroid-wear\ngit\nsend-anywhere\n\n\n\n\n\n\nTry it out\n\n\n\n\n\n\n\n\ndrive.google.com"
    },
    {
        "url": "https://devpost.com/software/purchasemate",
        "content": "PurchaseMate is a barcode scanning app which brings transparency to your shopping cart. By using your smartphone to scan the barcode of a product in a store, this app will give you easy access to the details of the corporation that produces it, including their donations to political and charitable organisations and their UN Human Rights rating. Our aim is to make socially conscious shopping easy and accessible. \n\n\n\n\n\n\nTry it out\n\n\n\n\n\n\n\n\npurchasemate.co\n\n\n\n\n\n\n\n\nonedrive.live.com\n\n\n\n\n\n\n\n\ntwitter.com"
    },
    {
        "url": "https://devpost.com/software/resistora",
        "content": "Inspiration\n\n\nFor Sean and Neerajen's final project for their AP Physics C course, they had to build a digital seismograph. This project involved many Integrated Circuit chips, PCBs, and resistors. \n\n\nLots of resistors.\n\n\nThere were so many resistors in so many different values such that the progress on the seismoeter grinded to a halt. They had to consult a table every time to solder a resistor in. And they didn't finish the seismograph.\n\n\nBut with an app that could tell them the value of the resistor, they would have been able to finish their project. So to prevent any future encounters with resistors, we as a team decided to make Resistora: the image recognition resistance finder.\n\n\nHow it works\n\n\nThe app's UI is quite simple. Open resistora, and align resistor with the two lines in the center of the app screen. Our manually overrided camera control algorithm will zoom in and take a picture for image processing and calculation, which will then be displayed at the bottom. \n\n\nChallenges I ran into\n\n\nWorking with RGB and hue was very difficult. It is difficult for a computer to differentiate between colors such as Red, Orange, Brown, and Gold. As such, we tried multiple methods to identify the location of the colored bands on each resistor - such as Floodfill, normalizations, and transformations. Ultimately, we were able to find a way to identify the colors and its location using a graphing technique, and local maxima.\n\n\nAccomplishments that I'm proud of\n\n\nWe used our knowledge from AP Physics to find an efficient solution to image processing. Firstly, each column was averaged. Then, the lightness and saturation were calculated from the image. One interesting thing we noted is that at each band on the resistor, the saturation value would spike. This would mean that all we would have to do is find the local maximas and then find the rgb values at those locations. However, this would occasionally produce false positives which were eliminated by using the lightness data. Whevener the lightness dropped, creating a local minima, this would indicate a band. This actually worked!\n\n\nWhat I learned\n\n\nWhat's next for Resistora\n\n\n\n\n\n\nBuilt With\n\n\nandroid\ncamera\njava\nmathematica\nresistors\n\n\n\n\n\n\nTry it out\n\n\n\n\n\n\n\n\ngithub.com"
    },
    {
        "url": "https://devpost.com/software/dchacks2015",
        "content": "Metryx\n\n\nMetryx is an innovative tool for DC Metro users. It provides essential information for both new and experienced riders alike – including Metrorail and Metrobus accessibility heatmaps, station predictions, and real-time train and bus location information.\n\n\nWe built Metryx out of a need we had for better transit data tools. Even the official WMATA-provided data display webpage is mediocre at best (and completely broken at its worst). Most applications provide a limited set of data pulled straight from WMATA's API – like train predictions (which Metryx provides). Metryx builds on this and synthesizes a novel visualization of transit availability.\n\n\nTechnology\n\n\nWe used the Flask framework to provide a JSON API frontend. Based on Google Maps, that frontend provides a fluid and easy-to-use data display mechanism. \n\n\nWe used Redis to cache relatively static Metrorail data, and used PostgreSQL as our database backend for storing historical and current train timing data.\n\n\nNotes\n\n\nWMATA is the Washington Metropolitan Area Transit Authority. They run Metrobus and Metrorail, the DC area's primary mass transit systems. Metrorail is the second most-used commuter rail system in the US (right behind NYC's).\n\n\nWe are not affiliated in any way with WMATA.\n\n\n\n\n\n\nBuilt With\n\n\nazure\ncss\nflask\ngoogle-maps\nheatmap\nhtml\njavascript\njquery\nmetrorail\npeewee\npostgresql\npython\nredis\nwmata\n\n\n\n\n\n\nTry it out\n\n\n\n\n\n\n\n\ngithub.com\n\n\n\n\n\n\n\n\ndchacks.cloudapp.net"
    },
    {
        "url": "https://devpost.com/software/nexcall",
        "content": "Inspiration\n\n\nI've always been annoyed after talking to my friends for an hour that they were on Verizon and I was on AT&T. That means I just used up 60 anytime minutes! As soon as I saw this API by Nexmo, I realized its potential to solve this problem. With nexcall, always be in the know in real-time and save your minutes!\n\n\nHow it works\n\n\nJust install the app and it runs in the background! Every time you get a call, it will let you know right away if the caller is on the same mobile network as you, helping you save your precious out-of-network mobile minutes and charges. It will also tell you which country the call is from so you can help avoid international call charges and learn something new!\n\n\nChallenges I ran into\n\n\nGetting Android call recognition to work smoothly.\n\n\nNexmo: by default unless a \"1\" is prepended, the API doesn't recognize bay area numbers as US numbers - mine got recognized as Peru by default. Also, US numbers are recognized as \"North American Number Authority\". Would be nice if it said \"USA\".\n\n\nAccomplishments that I'm proud of\n\n\nMaking a nicely designed call notification :-)\n\n\nWhat I learned\n\n\nThe complexity of phone numbers.\n\n\nWhat's next for nexcall\n\n\nUse it everyday myself! Get it working on phone boot-up and not having to open app to set it up.\n\n\n\n\n\n\nBuilt With\n\n\nandroid\nnexmo\n\n\n\n\n\n\nTry it out\n\n\n\n\n\n\n\n\npivotle.com"
    },
    {
        "url": "https://devpost.com/software/underground-map-pxy6qd",
        "content": "We highly recommend chrome for viewing the map since other browsers such as firefox can not handle the massive amount of data sometimes, we tried to fix this, and improved performance significantly, but it still wasn't enough.\n\n\nAbout the project\n\n\nWe are 4 students from The Netherlands and the United Kingdom We created this project because of the \"Maps of art\" challenge. Since we all love open source and open data we decided to combine so many data \n\n\nInspiration\n\n\nThe assignment for this Challenge was \" Create a ‘wall worthy’ interactive map that illustrates one way in which you see the world.\". We did it a bit different, we created a map of the world as we do not see it. There are thousands of cables, pipelines and tunnels, and of course even more natural resources, such as gas, oil, ore veins, and tectonical plates. Our goal was to make it work just fine.\n\n\nHow it works\n\n\nWe use the powerful openlayers v3 to render the data on our map, which is openstreetmap. On the back-end we created a way to load geojson from a clusterpoint nosql database, unfortunately due to the size of the files that had to be loaded, and the lack of time to improve the render engine we weren't able to use clusterpoint for all datasets, however the json generator is there.\n\n\nChallenges we ran into\n\n\nOne of the biggest problems we ran into was performance, after 3 rewrites of the engine that handles rendering of data layers and different libraries we have found something that fitted our needs, yet it can get a bit slow sometimes.\n\n\nMeet the team\n\n\nNick Vernij\n - Nick is a 16 year old developer and student from Rotterdam, the Netherlands, creating websites since he was 12. He mostly codes in PhP, Javascript and java\n\n\nLem Severein\n - Lem is a 16 year old developer and student from Rotterdam, the Netherlands, he can do anything with his skills in css, html and javascript, and PhP. \n\n\nSufi gaffar\n - Sufi is a 17 year old webdeveloper and student from London, England. He is constantly learning new things, and has a fair knowledge of some languages.\n\n\nDaniel Mizrachi\n - Daniel is a 17 year old webdeveloper and student from London, England. And is definitely in love with open data and APIs.\n\n\nContact:\n\n\nhello@nickforall.nl\n\n\n\n\n\n\nBuilt With\n\n\nclusterpoint\ncss3\njavascript\nopenlayers\nopenstreetmap\nphp\n\n\n\n\n\n\nTry it out\n\n\n\n\n\n\n\n\nunderground.nickforall.nl"
    },
    {
        "url": "https://devpost.com/software/passionfruit-wqurvk",
        "content": "Inspiration\n\n\nOur inspiration grew from our personal experiences from middle school, being able to relate to how careers seemed out of our reach and a little abstract. We wanted to find a way to bridge the gap between what kids are learning and what they want to do when they grow up.\n\n\nHow it works\n\n\nPomelo is a platform that connects students, teachers, and parents with the career pursuits and interests of the students, by integrating with existing educational management systems, such as Blackboard. By leading the students through an update-able, interactive and quirky game, their personalities and strong suits can be gauged, allowing Pomelo to suggest potential career paths tailored to each student. Teachers and parents can then track their interests and recommend subsequent curricula to direct students in their preferred direction area of study. \n\n\nChallenges I ran into\n\n\nChallenges we faced include creating images and manifesting ideas from scratch without prior experience, and being able to write code that represents our vastly complex and intricate ideas. Many of our initial ideas were difficult to do with such a time constraint, and subsequently many facets of the original idea were cut.\n\n\nAccomplishments that I'm proud of\n\n\nWe are proud of the ideation process resulting in our carefully deliberated design, which appeals to all populations of interest as well as being able to integrate into current academic management systems. We did not jump straight into coding, and took on the roles of devil's advocate to assure the balance of ideas and execution. We are also proud of our integration of multiple technological frameworks, such as Ionic Framework and Typeform. \n\n\nWhat I learned\n\n\nWe learned the importance of front end development as it relates to molding a pleasant user experience for our target populations. We learned the importance of articulation in communication to work more effectively under tight constraints. We also became more familiar with different languages and frameworks that helped develop our technical experiences and problem solving abilities. \n\n\nWhat's next for Pomelo?\n\n\nWith more time and thorough thought, Pomelo can be applied as a better supplement to learning. For example, using a database that stores different school curriculum information, Teachers will then be able to recommend online ans see it visually which courses students are advised to take in upcoming years, mapping out a path or plan for their career of interest. Pomelo can also transform into an interactive resource for students interested in extra-ordinary work, such as engaging in learning code and exploring in-depth into various fields. Pomelo can also aid in students' experiences with the Thinkabit Lab, as they are creating and engaging in hardware and software alike. \n\n\n\n\n\n\nBuilt With\n\n\nangular.js\napache\ncss3\ngimp\nhtml5\nionic\njavascript\nphotoshop\ntypeform\n\n\n\n\n\n\nTry it out\n\n\n\n\n\n\n\n\ngithub.com"
    },
    {
        "url": "https://devpost.com/software/reaxn",
        "content": "Inspiration\n\n\nOur team identified a common pattern with existing women’s safety apps: excellent features exist for alerts, messages, location tracking and 911 calls - but they all lack quick one-step access to preconfigured actions so that women can react immediately when in danger. That’s where ReaXn comes in.\n\n\nHow it works\n\n\nReaXn’s iOS prototype allows a user to knock on their phone, without taking it out of their pocket, and send a message alert to anyone. The receiver, content, and other information are pre-configured so reacting is truly a one-step action.\n\n\nChallenges we ran into\n\n\nProduct: Providing tangible value without reinventing the wheel. \nDesign: Creating a minimal, user-friendly yet eye-catching design.\nEngineering: Making texts/calls through Twilio, by-passing the iOS lock screen to enable direct actions when device is knocked on, creating an API for our features.\n\n\nWhat we learned how to:\n\n\nConnect complex global issues with technology and design.\nWork in a team, split up tasks, and communicate with people we just met.\nUse existing iOS knowledge creatively to provide easy usage.\n\n\nWhat's next for ReaXn\n\n\nProvide more safety features, and more quick-access means\nCreate an API for iOS and Android for any app to extend our quick-access features.\n\n\n\n\n\n\nTry it out\n\n\n\n\n\n\n\n\ngithub.com"
    },
    {
        "url": "https://devpost.com/software/tweetee",
        "content": "Tweetee was created by Devin Mancuso and Justin Frost for the ChallengePost Summer Jam 'Connect with your team' Online Hackathon Series.\n\n\nTweetee is a fun little project to turn sporting highlights into merchandise as they happen, allowing fans to create awesome one-of-a-kind merch to support their team and express their creativity and fandom.\n\n\nHow it works\n\n\n\n\nUsers search for a keyword or hashtag. \n\n\nWe return all the images from relevant tweets using the Twitter API\n\n\nUsers can then edit an image by resizing or adding text\n\n\nWe then post the image data to the Rapanuistore API and present the user with a storefront where they can buy their custom one-of-a-kind shirt.\n\n\n\n\nWhat's next for Tweetee\n\n\n\n\nImprove state handling for loading screens and error states.\n\n\nImprove response handling for API errors\n\n\nAbility for users to share their product page url via other social networks\n\n\nImprovements to the image search function to ensure more images are returned for each search, currently limited to searching the first 100 tweets.\n\n\nAutocomplete search results for popular sporting team official hashtags\n\n\n\n\n\n\n\n\nBuilt With\n\n\nbootstrap\ncss\nhtml\njavascript\npackery\nphp\nrapanuistore\nscss\ntwitter\nvoxmedia\n\n\n\n\n\n\nTry it out\n\n\n\n\n\n\n\n\nwww.happypandas.com"
    },
    {
        "url": "https://devpost.com/software/bulldozair-flir-k6h8v",
        "content": "Inspiration\n\n\nAli is a construction engineer, spent over 6 years managing construction sites for a TOP 3 General Contractor in Europe.\nMaxence is a software engineer, spent over 5 years developing mobile apps for retail companies, universities, ...\nThey met during a hackathon and developed BulldozAIR, a mobile and web app for the construction industry that helps architects, contractors, engineers collaborate better on the field and in the office.\n\n\nHow it works\n\n\nIt's a note taking app that syncs plans, technical documents, tasks and collaborative notes between the 50 companies on a construction site.\n\n\nOur users do visit of construction sites.\nThey need to inspect thermal leaks for example.\nThey use the tablet with FLIR to take thermal pictures within collaborative notes.\nThey can sketch what needs to be done on the pictures.\nThey locate issues and pictures on blueprints.\nThey set a priority level.\nThey assign a task to another collaborator who will receive this note.\nThe collaborator uses the same app, and replies in the same note.\nFinally a Word report is automatically generated.\n\n\nBulldozAIR stores all your blueprints. It works offline if you're at a sub-level with no connection. It syncs when you get a connection back. Our backend provides the same sync API to mobile (Android, iOS, Windows 8) and web apps.\n\n\nChallenges I ran into\n\n\nSDK integration: switching between simulated device and real device, debugging with no connection to XCode or Android Studio (USB port used by the device).\n\n\nAccomplishments that I'm proud of\n\n\nSo glad we met the FLIR team, we missed them in December, it's great they are back.\n\n\nWhat I learned\n\n\nThe hardware in the FLIR One for smartphone is as good as standalone and expensive devices on the market!\n\n\nWhat's next for BulldozAIR - FLIR\n\n\nWe're strongly committed to publishing this new version of BulldozAIR with the FLIR module. There is a real market here, many of our current customers are interested in the BulldozAIR + FLIR combo. Our landing page is up, ready to get orders... all it needs is the FLIR team approval and their logo!\n\n\n\n\n\n\nBuilt With\n\n\nandroid\nflir\nios\njavascript\n\n\n\n\n\n\nTry it out\n\n\n\n\n\n\n\n\nbulldozairflir.strikingly.com"
    },
    {
        "url": "https://devpost.com/software/time-me",
        "content": "To read about the process of how I created this project, please visit my 15 projects in 30 days challenge: \nhttp://jeancarlbisson.com/2015/06/30/time-me-with-nexmo/\n\n\nInspiration\n\n\nI needed to keep track of how long I took to complete tasks. \n\n\nHow it works\n\n\nSend an SMS to Time Me using the command begin  or finish . Time Me will start timing the task named . If you want the time elapsed (if task is still active), or total time the task took, you can text the name of the task to Time Me and receive an SMS response with the start time, end time (if completed), and the elapsed time of the task.\n\n\nWhat I learned\n\n\nYou don't need passwords when you use the Nexmo Verify API.\n\n\n\n\n\n\nBuilt With\n\n\nangular.js\nnexmo\nnode.js"
    },
    {
        "url": "https://devpost.com/software/sharon",
        "content": "SHARON\n\n\nSince our very first, hands-on experience to interact photos from the very first iPhone back in 9 Jan 2007, pinching photos using multi touch interactivity has literally taken the world by storm. Today, almost all smartphones - tablets - and even some laptop models, use this as their key features for their input functions. \nWhile we often marvel at this, we refuse to accept that such interactivity and to an extent, manipulation - can’t be extended to videos, or moving imagery. With Youtube and other online video offerings expanding and growing year on year, to only passively watch content seems and feels very yester-year. What’s worse, unless you’re in the same space as your social group - sharing actual video experience by highlighting specific moments in parts of the video, would require that you to either verbalise them vocally or via text, or - actually re-edit that same video within an editing software, and export it out again. It was unbelievable old school, frustrating and above-all - technologically backwards… \n\n\nThat is - until we cohesively decided to do something about it. \n\n\n\n\n\n\nBuilt With\n\n\nandroid-studio\njava\nphp\nwebsockets\n\n\n\n\n\n\nTry it out\n\n\n\n\n\n\n\n\ngithub.com\n\n\n\n\n\n\n\n\ngithub.com\n\n\n\n\n\n\n\n\nwww.dropbox.com\n\n\n\n\n\n\n\n\ndocs.google.com\n\n\n\n\n\n\n\n\netiene.net\n\n\n\n\n\n\n\n\netiene.net"
    },
    {
        "url": "https://devpost.com/software/infrapic",
        "content": "Inspiration\n\n\nWhen brainstorming potential project ideas, we wanted to take into consideration accessibility. We wanted to create a game that one could play on the go, like during 20-minute restaurant wait. The FLIR One Camera allows for the game to be played on top of any smooth, nonreflecting surface and because the players draw with his/her fingers, you don’t even need a pen.\n\n\nHow it works\n\n\nInfrapic utilizes the FLIR ONE to play a drawing game similar to Pictionary. At the start of each player’s turn, the phone randomly generates a word and the player sketches the key word using their fingertips. Then, the other player watches a video recording of the drawing and guesses the word. If the player is able to correctly guess the word, then he/she receives 1 point. Otherwise, no point is awarded. The two players switch roles and play again until a player reaches 5 points.\n\n\nChallenges I ran into\n\n\nImplementing the abstract idea to code was a challenge. I have done simple iOS development before but have not made a full app. It was another challenge to learn how the language works.\n\n\nAccomplishments that I'm proud of\n\n\nWe accomplished to make a usable game app that actually works and can have fun with it over the weekend!\n\n\nWhat I learned\n\n\nObjective-c programming language, general idea of how to make iOS app, Flir one SDK\n\n\nWhat's next for InfraPic\n\n\nIncorporate online gaming server that will connect gamers throughout the world.\nImage enhancer for the virtual chalk board\n\n\n\n\n\n\nBuilt With\n\n\nflir\ngreatidea\nios\nobjective-c"
    },
    {
        "url": "https://devpost.com/software/eventual-ly",
        "content": "Inspiration\n\n\nReduce The Clutter Of To-Dos In Your Camera Roll\n\n\nHow it works\n\n\nEventual.ly is an automated solution for users, extracting information from images.\nTake a picture of a poster and the event will be created on your calendar with the correct date and time.\n\n\nChallenges I ran into\n\n\nIntegrating API for OCR was challenging along with comparing two results from two OCRs to improve accuracy. and using stanford webprotege online service for concept mapping using ontology network.\n\n\nWhat's next for Eventual.ly\n\n\n\n\nSeamless sharing with friends\n\n\nEvent managers can monitor poster exposures\n\n\nDevelopers can create powerful connections with users via Eventual.ly \ne.g. link to ticket vendors on Songkick via Eventual.ly\n\n\n\n\n\n\n\n\nBuilt With\n\n\ndropbox\nidol-ondemand\nprotege\ntesseract\n\n\n\n\n\n\nTry it out\n\n\n\n\n\n\n\n\nwww.slideshare.net"
    },
    {
        "url": "https://devpost.com/software/stereogram-5fxl2",
        "content": "http://stereogram.indios.rip/\n\n\nInspiration\n\n\n4-lenses Lomos.\nAnalog film stereoscopic photos.\nOld-school 3d glasses. \n\n\nHow it works\n\n\nA group of people (up to 7) open the Stereogram app on their iPhones (Android app pending) and point the camera at the same subject, from multiple perspectives. If one person takes a picture, all the phones take a picture at the same time. All the images are uploaded to the server and stereoscopic GIFs (and optionally a 3D anaglyph) are generated, tagged using the Imagga API, so users can search and filter them.\n\n\nThe server also provides a JSON API and a basic web client so the users can list, search and see the generated images.\n\n\nChallenges we ran into\n\n\nSyncing the phones. We thought of and tried a bunch of different approaches to trigger all the shutters: \n\n\n\n\nproducing a sound with a specific frequency and listening on all the devices (didn't work well on noisy environments);\n\n\nsyncing the clock using NTP and scheduling a time to take the picture (laggy)\n\n\nsending a callback from the server (even more laggy)\n\n\nfinally settled on using the iOS multipeer connectivity framework that uses BT 4.0 or WiFi to connect devices and has an \"unreliable\" method, akin to UDP, which fires and forgets.\n\n\n\n\nGenerating the images:\n\n\n\n\nThe GIF generation was pretty simple, but the anaglyphs took a bit more time to nail.\n\n\n\n\nAccomplishments that I'm proud of\n\n\nWe're proud of actually managing to build the system and getting some great results with it:\n\n\nSome of our favorite stereoscopic images:\n\n\n\n\nhttp://stereogram.indios.rip/stereos/65\n\n\nhttp://stereogram.indios.rip/stereos/85\n\n\nhttp://stereogram.indios.rip/stereos/46\n\n\nhttp://stereogram.indios.rip/stereos/215\n\n\nhttp://stereogram.indios.rip/stereos/177\n\n\nhttp://stereogram.indios.rip/stereos/113\n\n\nhttp://stereogram.indios.rip/stereos/100\n\n\nhttp://stereogram.indios.rip/stereos/83\n\n\n\n\nSome of our favorite 3D anaglyphs (you need red-green 3D glasses to check the effect):\n\n\n\n\nhttp://stereogram.indios.rip/stereos/125D6099-0B0C-40F0-9AE9-689E684C5EFC.jpg\n\n\nhttp://stereogram.indios.rip/stereos/D011E39F-B7F2-4D5C-8F5A-7FC0AE1E8A23.jpg\n\n\nhttp://stereogram.indios.rip/stereos/B67542B3-AE8C-4D25-99EC-4CC339DC90B7.jpg\n\n\n\n\nWhat I learned\n\n\nHow to sync multiple iphones and make them talk to each other instantaneously.\nHow 3D anaglyphs work. \nPeople love moving pictures more than stills!\n\n\nWhat's next for Stereogram\n\n\nImprove iOS app usability. Tweak the web client version. Build an android app. Get people using it!\n\n\n\n\n\n\nBuilt With\n\n\n3d-glasses\nafnetworking\nbluetooth\nimagemagick\nimagga\nios\nruby\nruby-on-rails\n\n\n\n\n\n\nTry it out\n\n\n\n\n\n\n\n\nbitbucket.org\n\n\n\n\n\n\n\n\nbitbucket.org\n\n\n\n\n\n\n\n\nstereogram.indios.rip"
    },
    {
        "url": "https://devpost.com/software/emotcha",
        "content": "Inspiration\n\n\nI recently had to use a captcha and was frustrated by how many tries it took me to prove that I wasn't a robot. Then it hit me, why are we looking at squigly text, what's more human than emotions? I had just looked through indico.io's list of apis and saw that they had a facial emotion detection api and it looked pretty doable to implement, so I thought why not create a proof of concept for the first emotional captcha.\n\n\nHow it works\n\n\nEmotcha shows a user an image of a human's face and the emotional profile of the face. Next to that is a live webcam view of the user and a stream of their emotional profile. Once the profiles match, the user is able to login to the mock website.\n\n\nChallenges I ran into\n\n\nThe indico api expects just faces, so I had to implement live facial localization into the algorithm. Doing this while also fighting to keep the webcam view as smooth as possible was quite challenging\n\n\nAccomplishments that I'm proud of\n\n\nI'm proud of the general user experience of the proof of concept, that it just works. \n\n\nWhat I learned\n\n\nI learned how to implement a variety of powerful tools, indico.io's apis, materialize's beautiful css framework, and facial detection\n\n\nWhat's next for Emotcha\n\n\nNext is to make it responsive (cause it really isn't, curse of being a backend guy) and perhaps package it up on npm so that other developers could actually use it in the future\n\n\n\n\n\n\nBuilt With\n\n\ncss\nhandlebars.js\nindico.io\njavascript\nmaterialize\n\n\n\n\n\n\nTry it out\n\n\n\n\n\n\n\n\ngithub.com\n\n\n\n\n\n\n\n\nemotcha.herokuapp.com"
    },
    {
        "url": "https://devpost.com/software/boalert-avoid-presidential-traffic-it-stinks",
        "content": "Inspiration\n\n\nI travel to Chicago a lot for work.  The President spends his fair share of time here as well.  When he comes to town, flights are delayed, the roads are closed, and it generally can stink if you get caught up in the areas the President is visiting.  \n\n\nBased on these experiences, I was inspired to write an app based on open data that would allow me to see where and when the President (or Vice President) is traveling, and if the  alert me as soon as possible if he is traveling to my town.  That way, I can plan my week to avoid getting caught up in the mess.\n\n\nHow it works\n\n\nLocation Based Alerts.\n When you run the app, it asks for permission to access your coarse city-level location.  If you agree to share your general location with BOAlert, when the BOAlert Back-End Database gets information that indicates the President is going to travel to where you are, you are sent a push notification with the details.\n\n\nBOAlert Back-End Database.\n  The BOAlert application draws its data from the BOAlert back-end server.  The server, obtains its data from three sources:  \n\n\n\n\nA presidential schedule is posted to whitehouse.gov on most days.  See   \nhttps://www.whitehouse.gov/schedule/complete\n.  Unfortunately, but it is typically posted half way through the day.  Simply put, it is not sufficient to know ahead of time where the President or Vice President will be, but it can be helpful.\n\n\nThe FAA VIP Temporary Flight Restriction Database.  See \nhttp://tfr.faa.gov/tfr2/list.jsp\n.\n\n\nPoliticalPartyTime.org's Open Database of Political Fundraising Events.  See, e.g., \nhttp://politicalpartytime.org/calendar/\n.  The President loves to raise money for his party, so the thought was to mash this open data up with the government-provided open data to get even further (but less certain) information about where the President may travel, and also to see the correlation between Democratic fundraising events and the President's travel choices.\n\n\n\n\nAll three of the above data-sets are monitored and refreshed by the BOAlert Back-End.\n\n\nBOAlert App Data Browser.\n  The BOAlert application provides access to the relevant upcoming events that have been processed by the BOAlert Back-End Database.  \n\n\n\n\nTable View.\n\nThe table view shows a chronological listing information collected from official government data sources (whitehouse.gov and faa.gov).  If any of the events are nearby, the cell color will change from blue to red to let you know that you need to pay attention on that day at that time.\n\n\nMap View.\n\nThe map view provides a geographic display of the official government data sources (whitehouse.gov and faa.gov) on a map (using red pins), and the political fund-raising event information (politicalpartytime.org) (using purple pins).\nThe map view also attempts to guess the general areas and roads that may be closed down for the President's motorcade.  For example, when the President travels, the government shuts down airspace in the shapes of inverted cones in the general areas the President will be.  By analyzing where the bottom of the cone is, BOAlert determines the parts of cities the President will likely be visiting.  By ordering these geographic points chronologically, we can see what the typical driving route would be and assume that the president will take the typical driving route to get from point to point.\n\n\n\n\nChallenges I ran into\n\n\n\n\nDebugging JavaScript code that is running in the cloud on another computer is always fun.\n\n\nApple push notification setup could be more streamlined.\n\n\n\n\nWhat I learned\n\n\nI became more familiar with Parse CloudCode, how to scrape webpages using JavaScript, and various APIs including Google geocoding and reverse geocoding APIs.\n\n\nChanges from Prior Version\n\n\n\n\nMore Data!  The prior version of the application only showed one data set (derived from data provided by the FAA) in both the table view and map view.  For this jam I added an additional dataset from PoliticalPartyTime.org that is shown along side the data derived from the FAA.\n\n\nImproved Map View.  The map view has much better annotations, and shows data from multiple data sets.\n\n\n\n\n\n\n\n\nBuilt With\n\n\njavascript\nobjective-c\nparse\n\n\n\n\n\n\nTry it out\n\n\n\n\n\n\n\n\nwww.boalert.com"
    },
    {
        "url": "https://devpost.com/software/jerri-can",
        "content": "Inspiration\n\n\nI want to live in a world where I never have to go the gas station again.  Humans should not have to look at a fuel guage to know that the car is empty, the car should predict when it will be empty and ask for gas.\n\n\nHow it works\n\n\nAfter learning your driving pattern, Jerri learns where home is and where where work is.  Imagine a Tuesday morning when you leave home with only a quarter tank of gas and get to work with an almost empty tank.  A local service attendant will get a notification that your car is empty.  He'll have secured access to your car and be able to fill up your tank.\n\n\nWhen you get back in the car, the gas tank is full - and you never have to think about going to the gas station again.\n\n\nChallenges I ran into\n\n\nThe Mercedes Benz data API had a few kinks that we had to work through. The touchpad for a digital signature was a tough nut to crack, but it worked!\n\n\nAccomplishments that I'm proud of\n\n\nWe used a lot of the Mercedes technologies.  We used the car data api, nokia's mapping technology, the car's camera and the command center controls and screen. We're also proud that what we built will change the world - it's not an incremental improvement.  We are pushing the envelope so that a car company can become a lifestyle company.\n\n\n\n\n\n\nBuilt With\n\n\nandroid\npubnub"
    },
    {
        "url": "https://devpost.com/software/moisture-gardening-invention-social-movement",
        "content": "DewGood\n\n\nThe Challenge:\n\n\nLawns suck… a lot of water. More than half of all water usage in residential homes is outdoors. How can we get more Californians excited about replacing their lawns with a smarter alternative?\n\n\nThe Solution:\n\n\nWe’ve invented a product and a brand to help kick-start a new tech-based social movement that inspires people to move away from water-sucking ornamental lawns. \n\n\nThe Invention:\n\n\nDewGood is an atmospheric water condenser and irrigation device that’s disguised as an elegant solar yard lamp. Combined with DewGood’s Moisture Gardening  movement, which replaces grass with drought-tolerant native plants, this product will be the first step in changing the way people look at their yards.\n\n\nDewGood Prototype:\n\n\nYeah, it really works.\n\n\nTo prove that the DewGood idea is viable, we built a functioning prototype with a 3D printer. This prototype is the simplest possible rendition of the proposed product. \n\n\nOur vision is to build and market a gorgeously designed version of this technology that is incredibly efficient, compact and solar-powered.\n\n\nThe DewGood Smart Phone App\n\n\nBy connecting our products with our users on a digital level, we’ll create more opportunity to remind the user about the true purpose hidden in DewGood’s beautiful ornaments: water conservation and awareness. We’ll also make our devices even more efficient using the phone to track weather patterns and adjust settings automatically. \n\n\nThe DewGood App connects you to your eco-friendly yard – and allows you to post sharable content on social networks.\n\n\nDewGood Moisture Gardening Approach:\n\n\nDewGood, in all its communications, will promote a new tech-based alternative to ornamental grass lawn. \n\n\n1) Your lawn sucks… so get rid of it. DewGood will connect you to resources to assist you in removing the turf from your yard. DewGood will promote the LA DWP California Friendly Landscape Incentive to help you get involved.\n\n\n2) Add drought-tolerant California native plants. DewGood will provide fun, easy-to-implement advice on how to reinvent your garden with unthirsty succulents, flowers and plants.\n\n\n3) Keep your drought-tolerant plants healthy with “sustainably generated water” that you produce “off-the-grid” (and off your water bill) with your DewGood Moisture Gardening technology.\n\n\nDewGood is more than just a cool product, it’s a movement.\n\n\nDewGood tech will help create a new hobby (and obsession) that people can get into  – and connect with a new community that exists on social networks and their neighborhood. \n\n\nSimilar to the DYI movements like craft beer-making, hot rod building, Etsy craft-making and even artisian pickle making, the appeal of Moisture Gardening is that people can feel like they’re doing something special and find joy in the creative process of helping to build a bold new eco-friendly tech-forward movement.\n\n\nThey no longer have a boring lawn. They can now be Moisture Gardeners and DewGood Pioneers.\n\n\nPartnership Ideas:\n\n\nHope Gardens Landscaping: Their clients have already made their lawns drought resistant. Let’s make them influencers and offer our product to some of their clients.\n\n\nHome Depot: Not only can we offer workshops on how to DIY, but HD is both a resource for parts and a potential partner to produce kits.\n\n\nTreePeople: DewGood will be a perfect fit for their Green City Workshop Series.\n\n\n\n\n\n\nBuilt With\n\n\n3d-printer\ndivision-of-agricultural-resources\nlapalitas-plant-database"
    },
    {
        "url": "https://devpost.com/software/can-i-has-symphony",
        "content": "Inspiration\n\n\nThe four of us all are excited (or at least feel guilty enough) to want to go to the symphony. We don't believe you need to water down the symphony's music to make it exciting though; we believe the music stands on its own, and we only need to remove barriers to entry. These barriers include comfort in the space, not having a pre-existing emotional connection with the music, understanding the rituals and typical misconceptions of the symphony experience, and navigating the jargon of the symphony (what is a POP series? What is an A-series?). \n\n\nHow it works\n\n\nOur app is light and humorous, which engages those users who are interested in giving the symphony a try. It draws the user along a path into a gamified learning experience, learning from the successes of Codeacademy and Duolingo. As the user answers questions and navigates the app, content is curated and personalized to their demographics and level of expertise, making this app an invaluable sidekick whether it's their first concert or their fiftieth.\n\n\nChallenges we ran into\n\n\nThe two challenges we had this weekend were lack of time and in-depth user data. We were able to gather fantastic preliminary insights and spent the first 50% of our time on sense-making and problem definition. The prelude on Friday night and each of the concert experiences this weekend built our own connection to the symphony and helped us identify barriers to entry that hold novice symphony goers back. These preliminary insights were invaluable in driving the content design of our app. This design can continue to evolve as we track the effectiveness of each component.\n\n\nWhat we learned\n\n\nBehaviour change requires the user to have the motivation, ability and a trigger. Our app focuses on motivating new symphony goers to take the leap by giving them a personal connection to the concerts, improving their ability to change by lowering the barriers to entry, and  triggering the behaviour change with a fantastic ticketing experience. We found this comprehensive approach does not exist in any of the symphony’s current marketing approachers (or if it does, it isn’t obvious to the user). We learned that each one of these three components has unique challenges to overcome. We also personally grew, sharing our strengths with fellow team members, and learning from the hackathon community at large.\n\n\nWhat's next for How do I Symphony?\n\n\nThrough in-depth user and expert research, How do I Symphony will expand to include the experience of attending the symphony in its entirety. Phase 2 will include revamping the ticketing experience, leveraging interactive, connected tools to turn the ticket into a learning portal. Phase 3 will enhance the user experience up until the house lights are dimmed, including further learning opportunities, providing opportunities to prepay for parking and drinks, and letting them know when and when not to clap. Phase 4 will capture the after concert glow of participants to increase customer retention.\n\n\n\n\n\n\nBuilt With\n\n\nbackbone.js\ndjango\nsticky-notes\nuser-mapping\n\n\n\n\n\n\nTry it out\n\n\n\n\n\n\n\n\ngithub.com"
    },
    {
        "url": "https://devpost.com/software/rollout-instant-trash-pickup-schedule-recycling-app",
        "content": "Inspiration\n\n\nKenton was sick and tired of being in the dark about trash pickup and recycling days. When he checked the City of Houston website, he had to go through 8 steps to get the schedule. There had to be a better way. \n\n\nRollout!\n\n\nWe wanted to make an app that had all the information you need (geolocated) the moment the app launched. Once you know the next pickups (the essential part), you can enjoy additional features that create added convenience.\n\n\nFeatures\n\n\n\n\nInstantly see the next trash pickup date, next recycling pickup date,  next heavy trash date \n\n\nSchedule reminders\n\n\nFind recycling centers based on material\n\n\nLearn about what's recyclable\n\n\n\n\nWhere we are\n\n\nSo in less than 24 hours, we have a pretty darned functional prototype. We're pulling ESRI / ArcGIS map data with lots of attributes we've uploaded for recycling locations. We are close to mastering the My City Map ESRI data for pickup  schedule data.\n\n\nWe want to launch\n\n\nWe want to complete this project quickly and offer it at no charge to the City of Houston as an app for their website to give Houstonians power over their waste.  \n\n\nThank you. \nKenton, Joel and Jason\n\n\n\n\n\n\nBuilt With\n\n\nandroid\nangular.js\napache\nes6\nesri\nios\nmy-city-map\n\n\n\n\n\n\nTry it out\n\n\n\n\n\n\n\n\nslides.com"
    },
    {
        "url": "https://devpost.com/software/rally-u34ya",
        "content": "On the Saturday after hearing about the Uber Hackathon, we got a text from our friend Jon inviting us to dinner at an awesome southern bar and restaurant in New York City called Jacob’s Pickles. He had made a reservation for eight that we would lose if we didn’t completely fill, so he asked us to invite a few more people. Over the next few hours all of us invited friends, some of whom later backed out, and Jon eventually realized that he had originally given us the wrong time for the reservation. By the time we thought we had to go, we were unsure if the dinner was even happening or if we were going at the right time, and we had sent more texts than either of us care to remember in an attempt to get everyone on the same page. \n\n\nTo cap things off, we were the only two people to make it to the restaurant on time and had to start texting again to confirm that our friends were even coming. Even though the night ended in great food and drinks, it was representative of the frustrations of  planning spur of the moment outings. We decided to build Rally to solve that problem.\n\n\nRally lets you plan impromptu events with friends, making sure everyone gets to the right place at the right time without the confusion and hassles that life creates. The tools we had before Rally (calendar invites, group texts, and outdated links to Yelp) are not well suited to the needs of planning a same-day event.\n\n\nOur feature set was driven by the desire to make it so convenient to arrive to an event on time and let people know your status, that it would be harder not to show up:\n\n\n\n\nSuccinct Event Details\n: One glance at the event page gives you the who, what, when, and where in a graphical and easy to digest format.\n\n\nEffortless Sharing\n: With just a click, you can invite any of your facebook friends, send an email, or shoot a text with a link to the event. Rally provides all the necessary details on Android or mobile web so there’s no confusion.\n\n\nAutomatic Reminders\n: When you join an event on Rally, you automatically get a notification reminding you when it’s time to get going and preventing you from losing track of time.\n\n\nSeamless Uber Integration\n: From the details page of the event or the reminder notification, you’re two clicks away from having an Uber pick you up at your current location and take you to the event. We’ve made it dead simple to be on time.\n\n\nAutomatic Status Updates\n: Rally starts reporting location data to your friends as soon as you indicate that you’re on the way, so you’ll always have a realistic expectation of when people will arrive. Once you get to your destination, you’re automatically marked as “Arrived” so your friends will know to look for you\n\n\n\n\nWe had a great time developing Rally, and we think it has a lot of potential. We’re really excited to be able to share it with you and others. As evidence of that excitement we leave you with this selfie we took with, Mohamed, the driver of the first real Uber scheduled through Rally’s integration with the Uber Request Endpoint. Cheers!\n\n\n\n\n\n\n\n\nBuilt With\n\n\namazon-rds-relational-database-service\nandroid\nfacebook-graph\ngoogle-maps\nheroku\npostgresql\nredis\nruby\nsidekiq\nsinatra\nuber"
    },
    {
        "url": "https://devpost.com/software/clutch-ratings",
        "content": "Inspiration\n\n\nHaven't you ever been in a debate about who was a better, more clutch player? Was it Michael Jordan, or Lebron James? Was it Steve Young, or Joe Montana? Who threw more 4th quarter touchdowns in big games?\n\n\nNo more arguing. Let's look at what the data tells us. \n\n\nHow it works\n\n\nAt the biggest moments in the game, some players come up big, and others fall short. Those that come up big will have a strong clutch rating, and those that fall short will have their overall clutch rating reduced. The dataset we've used contains play and game-time data for NFL players from 2009-present. \n\n\nAn overall clutch rating is computed through the aggregate of all plays by that player, over the entire dataset. If you want to see what plays went into the equation, you can click on a player and get a more detailed view -- which will include things like height and weight data, in addition to a list of all of the plays, along with each individual play's clutch rating (can be positive or negative), which is factored into the player's total score. \n\n\nThe real kicker is that we give users a video link to watch a particular play they are interested in.\n\n\nChallenges we ran into\n\n\nClean data sets with strong historical data were hard to come by.\n\n\nAccomplishments that I'm proud of\n\n\nWith no UI folks on our team, we think we did a pretty good job. The tables also have search functionality, and the additional feature of having a link to play-videos is exciting. \n\n\nWhat we learned\n\n\nLook out for semi-colons, they'll getcha every time.\n\n\nWhat's next for Clutch Ratings\n\n\nWe plan on launching the site.\n\n\n\n\n\n\nBuilt With\n\n\nbrainpower\ndatatables\nflask\njavascript\npostgresql\npython\n\n\n\n\n\n\nTry it out\n\n\n\n\n\n\n\n\nclutchratings.com"
    },
    {
        "url": "https://devpost.com/software/match_it",
        "content": "MatchIT\n\n\nEver wanted to find just the right shade for your shirt or dress? Ever had to wander around a store just to find something that somehow resembles the thing you had in mind?\n\n\nWith MatchIT we make shopping easier, faster and better!\n\n\n\n\n\n\nBuilt With\n\n\nandroid\njava\npaypal\nruby\nsinatra\nsphere.io\nwearable\nwunderbar\n\n\n\n\n\n\nTry it out\n\n\n\n\n\n\n\n\ngithub.com"
    },
    {
        "url": "https://devpost.com/software/vidcomb",
        "content": "Inspiration\n\n\nWe love surprising the people around us! One way we can show our love is through video. However, communicating with a large group of people to participate in the video making project can be really tedious and time consuming. \n\n\nWe created VidComb, an application that makes video compilation convenient, easy and hassle free!\n\n\nHow it works\n\n\n\n\nThe admin user will create a project on VidComb.\n\n\n\n\nThe admin user will decide on the\n\n\n\n\nTitle of the project\n\n\nDue date of the project \n\n\nMaximum duration of each video clip\n\n\nWhich participant to invite\n\n\n\n\n\n\nThe participant will click on the link provided to record a video of themselves. The user can also choose to upload a file from Dropbox if they have already pre-recorded a video. After submitting a video, the participant can invite more people to participate in the project. There will also be a list of people who have completed recording and a list of people who have been invited but yet to submit their video. The purpose of showing the list is to allow all participant to invite those who should be in the video, but not in the list.\n\n\n\n\nThe admin will have an overview of all the people who have participated in the project. (If time permits, we would like the admin to have the ability to re-order the video, add text, audio, transition effects before the download)\n\n\n\n\nOnce all the participants have uploaded the video, the compiled video is ready to be downloaded.\n\n\n\n\nWhat's next for VidComb\n\n\n\n\nAllow users to upload compiled and completed video to YouTube, Dropbox and Vimeo\n\n\nAdministrative dashboard to manage participants\n\n\nRe-order the video after all participants have submitted\n\n\nAbility to add music and introduction text to the compiled video\n\n\nTemplate introduction and transition effects for the compiled video\n\n\n\n\nChallenges we ran into\n\n\n\n\nUnderstanding and using the APIs provided\n\n\nEmbedding a video with a url link\n\n\nAllowing users to upload compiled and completed video to YouTube, Dropbox and Vimeo (Due to time constrain)\n\n\n\n\nAccomplishments that we are proud of/ What we learned\n\n\n\n\nGetting a real actual site up and running! (Try it!)\n\n\nWorking together as a team for the first time & having great communication throughout\n\n\nHaving fun building our product\n\n\nCreating something that will be useful for the community\n\n\n\n\nSponsored APIs Used\n\n\n\n\nThank you to the great team from Ziggeo, Google, FireBase, Stupeflix, Vimeo, Youtube and Dropbox for mentoring and guiding us during the #VideoHackDay Hackathon!\n\n\nUpdated as of 10 May 2015, 10.10AM \n\n\n\n\n\n\nBuilt With\n\n\ncss\ndropbox\nfirebase\nhtml\njavascript\nphotoshop\npython\nstupeflix\nziggeo\n\n\n\n\n\n\nTry it out\n\n\n\n\n\n\n\n\ngithub.com\n\n\n\n\n\n\n\n\nglaring-inferno-2666.firebaseapp.com"
    },
    {
        "url": "https://devpost.com/software/fetch-cac4s",
        "content": "Maybe you're at a conference, a student at a lecture, or simply taking notes for your self.\n\n\nOpen Fetch, turn it on, and it'll start translating the speech to text, choose the main topics, and will return research and information based on those points, sorted – into sections.\n\n\nYou can use Fetch to study, learn new topics, research your next presentation, or use it as your presentation!\n\n\nHow it works\n\n\nThe team began thinking about pitch decks - we all make them for speaking engagements, some people excel at them and others have a hard time. How could the process of creating them be automated?\n\n\nWe started by creating a tool that could pull content from a pdf, and using the Alchemy API, produce an analysis that showed its main concepts, keywords, and entities. With these filtered concepts, we were then able to query different sources for that content, and in turn pull them back into the app.\n\n\nWhile the app was being created, we realized that it might be a great fit for students who need to study after lectures, and so integrated a speech to text feature\n\n\n\n\n\n\nBuilt With\n\n\nalchemyapi\nandroid\nbluemix\nibm-watson\nnode.js\n\n\n\n\n\n\nTry it out\n\n\n\n\n\n\n\n\ngithub.com\n\n\n\n\n\n\n\n\ngithub.com"
    },
    {
        "url": "https://devpost.com/software/cheer-me-up",
        "content": "Inspiration\n\n\nNeeded a quick pick me up and love internet memes.\n\n\nHow it works\n\n\nUser creates a custom page that include a single emoji, animated gif background, song, and quote for a friend.\n\n\nChallenges I ran into\n\n\nDeciding on an idea and lack of sleep.\n\n\nAccomplishments that I'm proud of\n\n\nWe finished our app and it works and it's fun to use.\n\n\nWhat I learned\n\n\nReally dug into node.js and angular.js.\n\n\nWhat's next for Cheer me up\n\n\nPossibly saving past custom pages and a clean up of our URLs.\n\n\n\n\n\n\nBuilt With\n\n\nangular.js\ncss\nemoji\ngiphy\ngulp.js\nhtml5\njavascript\nnode.js\nresponsive\n\n\n\n\n\n\nTry it out\n\n\n\n\n\n\n\n\ncheermeup.website"
    },
    {
        "url": "https://devpost.com/software/the-streets",
        "content": "Inspiration\n\n\nWhat makes New York City New York City is, street music/dance performances. There are numerous talented and free-spirited artists here and they don't have much communication means with their audience. We wanted to provide a platform for people who enjoy street arts and artists. \n\n\nHow it works\n\n\nWhen a user of the app sees a nice street performance, they can upload a real-time video that automatically includes the location and time. The videos can be shared with other people including tourists who want authentic street art experiences in New York City. If the dancer has a profile on the app, then you can directly make a payment to the performer. \n\n\nAlso, you can donate to an artist after watching a video without having to be on-site of the performance. In addition, you can get future event schedules of the street dancer. If you're within 100 meters of the performance of interest, you will get a notification on your phone that you are nearby a cool performance. If you're close enough to the dancer, the app will show you how close you are to the performers with the bluetooth integration with Gimbal. \n\n\nChallenges I ran into\n\n\nThe hardware limit was the biggest challenge in the process of development of this app. We wanted to give the user an exact number for how far is the performance, but the bluetooth integration had a limitation of detecting the distance. \n\n\nAccomplishments that I'm proud of\n\n\nFirst of all, we incorporated 3 different apis: MasterCard api for the payment, Esri for map view and Gimbal for geo-location and geo-fencing. Out of all, our biggest accomplishment is the video view laid on top of the Esri map api in a bitmap drawable with real-time information.\n\n\nWhat I learned\n\n\nThose who came here to make money will not get much out of the hackathon as opposed to those who came to make an app out of passion. \n\n\nWhat's next for The Streets\n\n\nWe'd like to provide access to street performances with tourists as well as resident new yorkers. We hope to help underground artists have an opportunity to shine and make their name. \n\n\n\n\n\n\nBuilt With\n\n\nandroid-studio\nesri\ngimbal\nmastercard"
    },
    {
        "url": "https://devpost.com/software/whatsnex",
        "content": "An emoji gaming platform that is hosted through text serving company Nexmo. It is a new, fun and creative way to game with emojis and text. Users can simply text a dedicated mobile number to play old school arcade style games. \n\n\nWon 1st place for the best use of Nexmo Sponsor Challenge at TechCrunch Disrupt New York 2015.\n\n\nVideo Pitch Demo: \nhttps://goo.gl/H9iTuR\n.\n\n\nFeatured on Enterprise SPARKS Issue 6: Acing the Disrupt Hackathon.\n\n\n========================================================================================\n\n\nBored and sick of the video games you play ? Wanna find out \"What's Next\"? \n\n\nYou're in luck ! We are bringing you a new, creative and fun way to game with emojis and text services!  \n\n\nPlay old school arcade style games like Space Invaders, Bingo and Word Scramble all within 70 text characters.\n\n\nSo.. Are you game? \nText \"What's Next?\" to 1 239 900 0619 and play!\n\n\n\n\n\n\nBuilt With\n\n\napache\nlinux\nnexmo\nphp\nsms\n\n\n\n\n\n\nTry it out\n\n\n\n\n\n\n\n\ngithub.com"
    },
    {
        "url": "https://devpost.com/software/chilltwo",
        "content": "Home Screen\n\n\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n\nBreathing Exercises\n\n\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n\nMedications\n\n\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n\nLog\n\n\n\n\n\n\n \n\n\n\n\nAnnouncing chillTwo--the ultimate anxiety cooler, and the first ever anxiety-relief application to grace the Pebble App Store--as brought to you by the creator of \nchill blue\n.\n\n\nThe Problem\n\n\nIn the midst of a panic attack, it can often be difficult to discern exactly what actions to take to solve the problem. As your heart rate skyrockets and your breathing becomes jagged, your thoughts become broken and disjointed. In the back of your mind, you know that you should try to take deep breaths and possibly take a medication, but it seems physically impossible to do so without any aid.\n\n\nThe Solution\n\n\nFrom the comfort of your wrist, chillTwo boasts a simple, yet effective, breathing exercise guaranteed to calm your heart rate, an easily-customized health center to monitor your medications, and makes it easy for you to log your attacks for future reference.  chillTwo will revolutionize your recovery from, and experience with, anxiety.\n\n\nMarketability\n\n\nAccording to the Anxiety and Depression Association of America, anxiety disorders are the most common mental illness in the United States, affecting more than forty million adults in the U.S. alone. While anxiety disorders are highly treatable, only about one-third of those suffering receive treatment. People with an anxiety disorder are five times more likely to go to the doctor and six times more likely to be hospitalized for psychiatric disorders than those who do not suffer from anxiety disorders. Our application would be widely marketable to those forty million adults, as well as to minors not included in such statistics.\n\n\nProgress\n\n\nHaving had too-much experience with panic attacks myself, I know how frustrating and useless currently available anxiety-relief options can seem.  Thus, I went to TeenHacks several weeks ago with the aim of creating the most user-friendly, valuable product possible.  This weekend, I attempted to extend that platform to Pebble users by creating an application characterized by its clarity, elegance, and security.  With no prior knowledge of Objective C or Javascript, developing an application including frustrating animations, perplexing graphics, and strangely complex buttons within 36 hours was quite difficult.  While I am quite proud of what I have achieved here at CarlHacks, in the future I hope to improve upon chillTwo with further breathing exercises, more detailed support in the health center, and a location-based help service.\n\n\n\n\n\n\nBuilt With\n\n\nobjective-c\npebble"
    },
    {
        "url": "https://devpost.com/software/chipebble",
        "content": "Chipebble = Chipotle + Pebble! To make this work, we had to reverse engineer the Chipotle API by man-in-the-middling their iOS app and decompiling the Android app. Using a combination of those, we were able to reproduce the app's functionality and build an app for Pebble.\n\n\n\n\n\n\nBuilt With\n\n\nandroid\nchipotle\ndecompile\nflask\nios\nman-in-the-middle\npebble\npython"
    },
    {
        "url": "https://devpost.com/software/sizeof",
        "content": "Inspiration\n\n\nThe idea for sizeOf came about when Davis saw how much data his DSLR camera gave for his pictures. Using some simple trig we are able to deduce the size of real life object using nothing but the data from the camera and computation. \n\n\nHow it works\n\n\nWith only a picture and some information about the camera and lens, we are able to calculate the approximate real height and width of an object in focus.  Simply point the camera at an object, tap to focus, and drag a crop box around the object of interest to get the real size of that object, in meters.\n\n\nThe math is derived from lens formulae and some simple trigonometry.  \n\n\nChallenges I ran into\n\n\nEvery camera is different, and almost every variable is non-linear.  This makes creating solutions difficult.  \n\n\nAccomplishments that I'm proud of\n\n\nWe were able to get sub-centimeter accuracy in the .1m to .5m focal range, and +/- 10cm accuracy at medium to long ranges.  On an SLR, +/- 1cm accuracy was possible on .5m to 3m range.\n\n\nWhat I learned\n\n\nAdam tried Android development for the first time, and was very successful. \n\n\nWhat's next for sizeOf\n\n\nWe're looking to create calibration curves for more cameras and eventually package the software into an easy to use open-source API for others to implement.  \n\n\nPrizes We're Seeking:\n\n\n-Best Design\n-Best mobile hack\n-EMC Innovation Award\n-Coolest Hack W/ Least Storage\n-Best Android hack\n\n\n\n\n\n\nBuilt With\n\n\nandroid\nc\ngit\nios\njava\nobjective-c\npython\n\n\n\n\n\n\nTry it out\n\n\n\n\n\n\n\n\ngithub.com"
    },
    {
        "url": "https://devpost.com/software/alt-text-bot",
        "content": "Inspiration\n\n\nTwitter is an important part of public discourse. As it becomes more and more image heavy, people who are blind are left out of the conversation. That's where Alt-Bot comes in. Alt-Bot fills the gaps in image content using an image recognition API to add text descriptions.\n\n\nThe inspiration for the format of the message is a \ntweet by @stevefaulkner\n, in which he adds alt text to a retweet.\n\n\nHow it works\n\n\nMention @alt_text_bot in a message or retweet that has an image attached, and you'll get a reply with a text description.\n\n\nAlt-Bot uses APIs from Twitter and \nCloudSight\n to retrieve and transcribe images in Tweets.\n\n\nChallenges I ran into\n\n\nSome people asked why anyone cares about meme photos or \nwhat I ate for dinner\n. My response was \"We don't get to decide who cares\". We need to make sure \neveryone\n is involved in the \nentire\n conversation, even if it's mostly trivial. We decide for ourselves what's important.\n\n\nAccomplishments that I'm proud of\n\n\nThe application has captured the attention and imagination of people on Twitter. There have been almost 100 transcriptions by 2:30pm on Sunday, and a handful of \nnotable retweets\n. I'm very happy with the quality of the descriptions too.\n\n\nWhat I learned\n\n\nI learned not to expect anyone to work harder for the same content. The people I talked to wanted image descriptions inline instead of posting or retweeting. I've already started work on a Twitter client to make this possible.\n\n\nWhat's next for @alt_text_bot\n\n\nAlt-Bot needs to be \"push\" rather than \"pull\". That means that I'll be creating a Twitter client that adds descriptions inline as part of the feed.\n\n\n\n\n\n\nBuilt With\n\n\ncloudsight\nruby\ntwitter\n\n\n\n\n\n\nTry it out\n\n\n\n\n\n\n\n\ntwitter.com\n\n\n\n\n\n\n\n\ngithub.com"
    },
    {
        "url": "https://devpost.com/software/master-speech",
        "content": "Inspiration\n\n\nWe made MasterSpeech to help anyone who struggles with or wants to practice their speaking skills. We felt that this was something that everyone needs to practice, whether you're a student giving oral presentations, a candidate seeking to improve a speech, or even a salesperson working on a pitch.\n\n\nHow it works\n\n\nMasterSpeech runs entirely client-side in Javascript. It uses Google's online speech recognition software, then parses the results and displays an analysis of the speech. Think of it as an \"enhanced\" version of recording yourself speaking and playing it back -- while we do offer that feature so you can evaluate yourself, we also take non-obvious quantitative data, such as spoken words per minute, to give further insight into where your weaknesses may lie.\n\n\nChallenges I ran into\n\n\nWe spent half of the first day trying to find a suitable speech processing library. Originally we planned on doing it locally on the backend, but the library we tried for that (CMU Sphinx) gave unusably bad results. We quickly tried out several other libraries, but we found that ultimately the library that worked the best was a wrapper around Google's online speech recognition. We realized that we could reimplement this library entirely on the client side in Javascript, eliminating the need for any dynamic content entirely.\n\n\nAdditionally, we had hoped that speech-to-text APIs would transcribe \neverything\n spoken, including filler words like \"um\" and \"uh\". This way, users could also receive a transcript of what they had said and be able to see where they often used these words. Unfortunately, it seems that these words are often filtered out and automatically disregarded by the software.\n\n\nAccomplishments that we're proud of\n\n\n\n\nFinding and using a suitable API for speech recognition\n\n\nFirst hackathon for half of our team\n\n\nMinified, entirely static and cacheable version (/min.html) that is fully functional under 5kb\n\n\nGood looking website for people who have never done any web design before\n\n\n\n\nWhat's next for Master Speech\n\n\n\n\nMore statistics like pause duration, vocabulary variation, tracking along with a pre-written speech, pitch variation, etc.\n\n\nWritten transcript of what you said, filler words and all\n\n\nSaving recorded audio to playback and analyze later - talk to your phone in the car or when you have free time, then read over your stats later\n\n\nTwilio integration - simply call a number to have your speech recorded, transcribed, and saved to be accessed later online through an account\n\n\n\n\n\n\n\n\nBuilt With\n\n\ncss\nfoundationdb\ngit\ngithub\nhtml5\njavascript\nnamecheap\nspeechapi\n\n\n\n\n\n\nTry it out\n\n\n\n\n\n\n\n\nwww.masterspeech.me"
    },
    {
        "url": "https://devpost.com/software/hithive",
        "content": "Inspiration\n\n\nProcessing big data involves many machines working together in a cluster. These can be expensive and hard to set up. HitHive allows website owners to use their site's visitors to complete map-reduce tasks and process large datasets while they're browsing your site. \n\n\nBloggers can make money with our platform without having to host visible advertisements. Academics and data scientists can use our platform to process data at a very low cost and none of the set-up pains.\n\n\nUsers no longer have to view ads to support the creation of the content they consume, and instead help scientists solve the world's most difficult problems. \n\n\nHow it works\n\n\nUsers of our service upload their map and reduce functions along with their dataset.  Once a payment is made, the user is given some generated javascript code to paste into their website. When people visit the website, their device will run this code and process small data chunks in the background using the provided functions.  We use python Flask with MySQL to host a restful api that allows browsers to retrieve and send back this data. Website viewers only get small portions of data at a time, which means they can view site content without any slowdown issues.  \n\n\nWhat's next for HitHive\n\n\nWe intend on expanding HitHive to provide a platform to replace online advertising and affiliate programs with a much more profitable solution for webmasters.\n\n\n\n\n\n\nBuilt With\n\n\nbootstrap\ncss\nflask\nhtml\njavascript\norm\npython\nrest"
    },
    {
        "url": "https://devpost.com/software/mind-vs-might",
        "content": "Inspiration\n\n\nWe wanted to explore the growing functionality of EEG headsets. While most EEG headset applications focus solely on mental well being, we wanted to incorporate EEG into a fun interactive game involving both the mind and body. \n\n\nHow it works\n\n\nThe game consists of three components-- an EEG headset, a robotic wrestling arm and an Android app. The player with the EEG headset attempts to focus as hard as s/he can on a physical action like pushing. We extract the percentage focus from the EEG headset and map it to a torque value. The robotic arm actuates against the force of the opponent's arm. The game ends when either the robotic arm or the opponent's arm touches ground.\n\n\n1) EEG processing\n\n\nWe use the Muse EEG headset to collect brain activity, specifically the relative values of the brainwaves (alpha, beta, gamma and theta). We use LibMuse on Android to connect to the headset and process the data. We then compute the percentage focus using a trained neural net. We collected many EEG samples of a teammate focusing on one object and also meditating/relaxing. Using the Encog Java library, we train a neural net with the goal of classifying an input into either focus or meditation. Once the neural network is trained on a developing machine, we serialize the object. We use this object in the Android app to classify a moving window of EEG signals. By averaging out several samples, we are able to effectively compute the torque value the robotic arm needs to apply force with.\n\n\n2) Robotic Arm\n\n\nThe robotic arm is made with Vex components; a DC motor spins at a rate opposite to the opponent's arm. Using gear-reduction, we increase torque as much as possible to mimic human arm-wrestling. The motor is controlled by a microcontroller (Atmega32u4+nrf8001) through a motor driver; it only activates when a button on its arm is pressed by an opposing hand. The microcontroller in turn receives a control value from an Android app. \n\n\n3) Android App\n\n\nThe Android app simultaneously communicates with the Muse headset and the robotic arm's microcontroller. It uses the neural net we designed to convert the real-time EEG data to a torque value. Using Bluetooth Low Energy, it then sends the value over to the robotic arm. \n\n\nNamecheap domain name: mindvsmightvs.me\n\n\n\n\n\n\nBuilt With\n\n\nandroid\narduino\nble\nencog\nmachine-learning\nmuse\nnrf8001\nvex\n\n\n\n\n\n\nTry it out\n\n\n\n\n\n\n\n\ngithub.com"
    },
    {
        "url": "https://devpost.com/software/kickback",
        "content": "Inspiration\n\n\nAmazon has a policy stating that if a product you buy from them has a lower price within a week of your purchase, you are entitled to a refund of the difference. Unfortunately, many aren't aware of this policy, and even if they are, it goes unused as it's a real hassle to do all the required legwork.\n\n\nKickback is something that people could and should actually use.  A few days before LA Hacks, I bought a scooter. Just yesterday, the price dropped $10. Because the application hadn't yet been built, I had to go through the \"traditional\" process of verifying what I'd paid, checking the new price, finding the dates to see if I was eligible, and finally the tedious task of contacting Amazon. This experience made me realize how needed this product was.\n\n\nKickback has literally no downsides to the consumer, and offers huge potential upsides.\n\n\nHow it Works\n\n\nUsers log in through Google in our mobile application (or soon via a webapp), which also serves as a beautiful list of pending and successful refunds. We check your Gmail Inbox for new Amazon orders, and watch for fluctuations in price. If something drops (monitored via Amazon's Product Advertising API), not only do we recognize that you're owed a refund, but we also send an email on your behalf (from your account using the Gmail REST API) to Amazon, requesting that the difference be refunded to you.\n\n\nYour only interaction with Kickback will be to sign up once, and then receive push notifications whenever we've saved you money.\n\n\nAccomplishments\n\n\nWe're pretty happy to have made a simple way to passively take advantage of a great policy that you may not have known existed. \n\n\nIn terms of technical achievements, we were able to build a relatively scalable MVP in a short amount of time; we organized worker execution with simple in-memory schedulers that could probably handle hundreds or thousands of users as is.\n\n\nWe also made a really pretty app.\n\n\nWhat's next for Kickback\n\n\nThis service could be very easily scaled; as more and more people automatically monitor products and get refunds, request overhead goes down due to overlap. The service could stay operational by charging a small percentage of each refund amount, or through donations. We built this on Azure, so it'd be dead simple to scale by switching to an actual Mongo cluster and a standard microservice architecture. \n\n\nIn the future, we'd also want to use some of Azure's built in caching/queueing features to make this a bit more asynchronous; requests are currently done inline because of time constraints. The fact that Azure did basic DNS for us was \namazing\n, huge props for that feature.\n\n\n\n\n\n\nBuilt With\n\n\namazon\nazure\ngmail\ngoogle\nios\nmongodb\noauth\npython\nxcode"
    },
    {
        "url": "https://devpost.com/software/pothead",
        "content": "So last week I was driving, and was pretty annoyed that I had to swerve around the road to avoid the potholes. Last month I messed up my wheel alignment because of a pothole, and now I'm just tired. We were inspired to create \nPotHead\n, an onboard pothole sensor using the Intel Edison Board and Python+Flask. We want to improve city infrastructure maintenance, and we're starting with something we all know and love, Potholes. \n\n\nHow it works\n\n\nWe start off with the Intel Edison Board. This board runs a python script that read output from an accelerometer. When the z acceleration is beyond a certain threshold, the sensor registers that you hit a pothole, and gets ready to send a request to the database. Our Edison creates a POST request to the digital-ocean database, and registers that you've hit a pothole in the city's database. The city can then see all the potholes encountered throughout the city, and can decide the best plan of action to fix them.\n\n\nChallenges I ran into\n\n\nA lot.\n\n\nThis was our first time working with the Edison board, and hardware hacking in general, so getting used to the basics of electric components was  time-consuming. We tried really hard not to fry any parts. \n\n\nWe also had problems with creating an API, and making sure any faulty requests wouldn't break anything.\n\n\n(We also kept mixing up longitude and latitude, but we don't like to talk about that)\n\n\nAccomplishments that I'm proud of\n\n\nWe got it to work!\n\n\nOf course, 1 hour before project submission, everything decides to work and we're rushed for time, but we managed to implement most of the basic features we wanted to see in Phase 1. It was really cool to see the physical sensor trigger an API call on our server in another state!\n\n\nWhat I learned\n\n\nHardware Hacking Basics, Serial Ports, Analog vs. Digital signals, Intel Edison, Arduino, Creating API Best Practices\n\n\n\n\n\n\nBuilt With\n\n\naccelerometer\narduino\ndigitalocean\nflask\nintel-edison\npython"
    },
    {
        "url": "https://devpost.com/software/sourcefetch",
        "content": "SourceFetch is an online service and Sublime Text plugin that allows you to turn descriptions of problems into actual working source code. SourceFetch works with every language, and gives incredibly accurate results. This tool is perfect for generating solutions to difficult problems without having to leave the text editor, and greatly improves the workflow of a software developer. The user simply enters an English phrase, such as \"implement web server\" or \"radix sort\" followed by a keystroke, and SourceFetch returns a solution in the language of the source file. Computer Science is all about abstractions, and what better abstraction is there than removing the need to code? The future is nigh\n\n\nSourceFetch uses a modified version of the \"howdoi\" command line tool, making it incredibly intuitive for beginners using Sublime. Code is found by searching StackOverflow and returning the most approved response - this means that there is an incredible amount of solutions to an incredible amount of problems accessible by our users. We implemented SourceFetch as an API in Python Flask-restful, which allows us to expand the platform to other popular text editors such as vim, emacs, eclipse and atom. Our UX team has worked really hard to create a fast web interface for queries on-the-go. \n\n\n\n\n\n\nBuilt With\n\n\ncss\nflask\nhtml\njavascript\njquery\npython\nrestful\nstackoverflow\n\n\n\n\n\n\nTry it out\n\n\n\n\n\n\n\n\nsourcefetch.me"
    },
    {
        "url": "https://devpost.com/software/dntbite-me",
        "content": "Estimates vary, but somewhere around 25% of people bite their nails. Just about every single one of them wants to stop and has made attempts to stop, but conventional fixes just don't work.\n\n\nWe augmented the HackIllinois hat to detect nailbiting with very high accuracy using a pressure sensor. We connected this to a Spark Core, so your nailbiting habits are pushed to the cloud. Using the Twilio API you are sent an SMS every time it catches you biting. A web interface lets you visualize your progress over time and see how long you've gone without biting.\n\n\nAll of this tracking is done for you, without any self-reporting required. We feel we have made something that can have a huge impact on a lot of people!\n\n\n\n\n\n\nBuilt With\n\n\nchart.js\nfirebase\nflask\nheroku\nparticle\npython\ntwilio\n\n\n\n\n\n\nTry it out\n\n\n\n\n\n\n\n\ndntbiteme.herokuapp.com"
    },
    {
        "url": "https://devpost.com/software/schoolsource",
        "content": "This theme was presented by Alif Ailaan and it struck a chord in each team member's heart. We are all from different backgrounds and hope to alleviate the aches of parents and students alike, as well as improve our schooling system.  Our project, SchoolSource, will provide a platform which will help citizens select schools for their children as well as review schools and voice their concerns. Stripped down to basics it is \"Yelp!\" for schools. \n\n\nIf you are a parent looking for schools for your child in a given area then SchoolSource is the right app for you. SchoolSource understands that each stakeholder in a school has different priorities and therefore our app provides you with a variety of criteria to find and rate them. These criteria include the availability of facilities (toilets, water, teaching staff, teaching aids), hygiene, school systems etc.\n\n\nWhen you choose to ‘find a school’, you can filter results by location (city) and the aforementioned criteria. Once you select a school in the app, you are directed to the school information page which includes the average user rating and a concise summary about the school. You can choose to view other users' reviews regarding this school or rate the school yourself if you'd like. \n\n\nIf you are interested in rating schools, the ‘rate a school’ option allows you to rate and comment on various criteria mentioned. Additionally you can also upload images for each criteria mentioned. \n\n\nOur project will also aid overseas Pakistanis whose families are still in the country to select appropriate schools for their children. It will also inform them of the on ground situation of schools in Pakistan and empower them to help where possible.\n\n\nCrowd-sourcing the reviews will give policy makers and the media a raw view of what is actually happening in public and private schools, empowering them to pressure associated authorities. We hope that this detailed feedback on schools will be used to help concerned parents and eventually leverage the power of the people to improve our schooling system.\n\n\nGitHub \nhttps://github.com/codeforpakistan/EduRate\n\n\n\n\n\n\nTry it out\n\n\n\n\n\n\n\n\ngithub.com"
    },
    {
        "url": "https://devpost.com/software/cyantranslate",
        "content": "Using CyanogenMod, we modified the Android operating system and created several new built-in features for manipulating text. By highlighting text and hitting a button, the user can take advantage of these features. The new features include:\n\n\n\n\nAutomatic translation between languages\n\n\nOne-tap Google search \n\n\nUrbanDictionary definition lookup\n\n\nBook information lookup\n\n\n\n\n\n\n\n\nBuilt With\n\n\nandroid\nbluemix\ncyanogen\nnode.js"
    },
    {
        "url": "https://devpost.com/software/destination",
        "content": "When walking alone in an unfamiliar neighborhood, you may find yourself feeling uncomfortable and unsafe even though you are following the turn by turn directions of Google Maps. This is a direct result of the oversight and assumption that the shortest path is the best path to follow. Furthermore, once you are in an unsafe scenario, without prior preparation there is not much that can be done to help. \n\n\nWe aim to solve this problem with the development of \nDestination\n, an iPhone application that provides users with the safest routes to their desired locations. We determine the safety of a route by aggregating publicly available crime information and determining the relative risk of following a specific path. \nDestination\n also tracks the users' movement patterns and if it detects any suspicious activity, such as a quick increase in speed or inactivity for extended periods of time, it gives you a popup Pebble smartwatch asking if you are okay. If there is no response or if you respond no, using the Twilio API we are able to immediately and automatically phone for help.\n\n\nBefore we can deal with determining possible paths, we had to collect and understand the crime information. We used Chicago as our case study, since the government publicly releases all crime information. Given some number of crime incidents we calculate the Kernel Density Estimation, which allows us to interpolate the risk of crime and generate a heat map of where crimes are likely to occur. We overlay this information on the user's map to allow them to make educated decisions about their routes. \n\n\nLooking at popular destination routing software, we saw that no API would give us the ability to route around specific edges. Since we felt this ability was necessary, we downloaded the entire map information and store it as a graph, manually implementing our own pathfinding algorithms. We store all of our crime information, nodes, and edges all in MongoDB. We use a modification of the bidirectional Dijkstra pathfinding algorithm to determine the shortest and the safest paths. We then recursively compute other paths in between that provide a trade off between safety and distance. We then allow the user to choose, using a slider, what path they would be most comfortable following at a glance. \n\n\nOnce the user selects the path they would like to follow they are presented with turn by turn directions that guide them to their destination. Currently, we have plans to extend \nDestination\n by taking into account more comprehensive measures of risk as well as trying to find characteristic features of specific regions that are connected with high risk areas and using this to find safe paths in cities that do not make their crime data publicly accessible.  Overall, by providing a much needed service to people walking alone in an intuitive and elegant iPhone application, we believe that \nDestination\n can make an actual difference in reducing harm and making people feel more comfortable traveling alone in the city.  \n\n\n\n\n\n\nBuilt With\n\n\nc\ncrime\ngraph\ngraph-theory\nkde\nmongodb\nobjective-c\npathfinder\npebble\npython\nsketch\nxcode"
    },
    {
        "url": "https://devpost.com/software/project-julius-2br2v",
        "content": "Inspiration\n\n\nIn 1997, an episode of Pokemon aired in 4.6 million homes in Japan. This episode had a scene with many quick flashes that caused 685 children to be hospitalized for photosensitive epileptic seizures.\n\n\nIn 2008 an online community known as 4chan posted seizure-triggering images and videos on epilepsy related forums and websites.\n\n\nIn 2012 a promotional video for the London Summer Olympics was reported to trigger seizures in people with photosensitive epilepsy.\n\n\nThe threat posed to those with epilepsy in each of these incidences could have been significantly mitigated with software that detects when potentially hazardous elements are present on a screen, and blocks them.  This is what Project Julius does.\n\n\nThe name comes from Julius Caesar, the roman emperor. Julius Caesar was an innovative leader who has been post-diagnosed with epilepsy. We aim to innovate the seizure prevention measures in place for video on computers.\n\n\nHow it works\n\n\nProject Julius monitors what is displayed on the computer's monitor and blocks quick image changes in order to prevent seizure triggers.\n\n\nUsing CamTwist and OpenCV to capture the images on screen.  We then process them using a histogram analysis in broken-down regions of the screen to look for quick changes.  If high-frequency flashing is detected, a window is pushed above all other open windows, warning the user and covering the threat.\n\n\nIn order to analyze the display to figure out if there are flashes that we should block, we perform a histogram analysis on the image. First, we take 2 consecutive frames and then divide them into a 10 by 10 grid of pixel regions. We then analyze the colour spectrum and create a histogram representing each region. Then, we evaluate the integral of the Hellinger Distance to find the Bhattacharyya Coefficient. This distance quantifies the similarities between the regions and allows to easily see major changes in the image such as a flash. If there is a 95% change in the image, determined with the Bhattacharyya Coefficient, then we declare it to be a \"flash event\". We then compare this result to the previous 10 results for each regions, if 60% of the frames contain \"flash events\" then the seizing guard is triggered and the window is minimized. We analyze  the display at 30fps, and require of 60% of the last 10 frames to contain dangerous flash events to declare a possible triggering event. This makes Project Julius able to catch possible seizure triggering events after only 0.2 seconds.\n\n\nChallenges we ran into\n\n\nThe first challenge we ran into was being able to complete the capture, analysis, and blocking in real-time with zero lag.  We altered our algorithm and refactored most of our existing python code, ultimately accomplishing precise analysis with minimum resource consumption.\n\n\nThe biggest challenge we ran into was being able to accurately detect when an image was flashing repeatedly, rather than an object moving or a scene changing. A lot of things seem like they could be flashes at first, but are clearly not when watched by a human. By comparing multiple frames one after another we were able to better analyze what is happening on the display.\n\n\nAccomplishments that we are proud of\n\n\nOur proudest accomplishment is learning about epilepsy.  In order to understand the requirements of our project we had to learn a few things about epilepsy including triggers, effects, and number of people affected. However, we wanted to go further. Since one of our team members has had family experience with epilepsy we decided to learn more. We learned about multiple different kinds of epilepsy, triggers for different kinds, statistics, long-term effects, symptoms, and the demographic of affected persons. In our opinion, the coding may have taught us new ways to accomplish our tasks, but we think the knowledge we know have will have a greater impact on our lives. Epilepsy is something we have have to deal with first hand in our life be it friends, family, or anyone in public.\n\n\nWhat we learned\n\n\nDuring the creation of Project Julius we learned a lot about Epilepsy and its causes. Using online resources we discovered how many people it affects, common triggers, long-term effects, and large incidents in public.\n\n\nWe also learned a lot about how to process large images (upwards of 1080p) quickly and effectively using histograms of regions on the screen. The changes in the histogram values indicate a change in content, a large difference indicates a flash or scene change. We then had to further our analysis to compare multiple changes to detect the rate at which the image changes, and thus determine if we should block it.\n\n\nOur initial approach to capturing the screen was to use the low-level frame buffer built into Linux. We spent a few hours learning how to access and process the frame buffer but ended up deciding not to go with it. We felt that using something built into Linux would greatly decrease our potential reach so we changed our screen capturing method to be more portable and cross-platform by using OpenCV and native desktop capturing applications that can pipe into Python. We found multiple programs for the different operating systems such as CamTwist and ManyCam.\n\n\nWhat's next for Project Julius\n\n\nProject Julius is in a state such that it can be applied in many situations, however it isn't ready to be released fully. We hope to continue development and create a fully refined and deployable application. We would like to contact Epilepsy Action in order to verify that our program would be effective and available for as many people as possible.\n\n\nLinks\n\n\nWebsite: \nprojectjuli.us\n\n\nGithub:    \ngithub.com\n\n\n\n\n\n\nBuilt With\n\n\ncamtwist\nimage-processing\nnumpy\nopencv\npython\nseizure-prevention\n\n\n\n\n\n\nTry it out\n\n\n\n\n\n\n\n\nprojectjuli.us"
    },
    {
        "url": "https://devpost.com/software/android-swear",
        "content": "We got tired of hearing people (ourselves) give bad presentations. A really big part of this is the use of crutch words (ums, ahs, literally, basically, etc.) and unless you focus hard on listening for them, it's hard to be aware of how frequently they get used. Enter Android sWear - an app to listen to a set of words that you give it, catch you every time to use them, and keep a count for each word.\nThis has lots of other possible uses to help people change for the better - such as swearing, speaking negatively, or anything a user might want to track.\nIs battery consumption an issue, since it always listens? Amazingly, no. One of the things we're really proud of is being able to continuously listen in a battery-efficient way. Using the Carnegie Mellon pocketsphinx voice recognition project, we were able to define hotphrases that the phone listens for, rather than constantly processing using more straightforward Speech to Text APIs.\n\n\n\n\n\n\nBuilt With\n\n\nandroid\nvoice-recognition"
    },
    {
        "url": "https://devpost.com/software/chameleon-3jegv",
        "content": "Chameleon is an Android application which helps colour-deficient people by adjusting the colour spectrum to accommodate their needs.\n\n\nWe target the most common type of colour deficiency, dichromacy/anomalous trichromacy, in which the user is only able to clearly distinguish two colours. For instance, in red/green colour-blindness, the user sees red and green as various shades of the same colour (typically represented as yellow). \n\n\nThis causes a problem when, for example, viewing a line graph which uses red and green to identify different lines. Chameleon remaps the colour space so that red and green are displayed as blue and yellow, which are distinguishable. \n\n\nThe remapping is done using OpenCV by converting the colour to YCrCb space and rotating the Cr/Cb plane by a user-controlled angle. This way, image brightness is not affected.\n\n\n\n\n\n\nBuilt With\n\n\nandroid-studio\nopencv"
    },
    {
        "url": "https://devpost.com/software/massaware",
        "content": "Summary:\n\n\nWe design the following system as a means to collect accurate data on maternal and neonatal deaths in rural India. We are collecting data from both the ASHAs (Accredited Social Health Activists, typically 1 per community) as well as from directly surveying families in India.\n\n\nThe ASHA Survey:\n\n\nWe used the Google Drive API to gather data from an initial data survey given to the ASHA. We implemented this by creating a Google Forms questionnaire, and as a proof of concept we were able to gather and parse data such as names, phone numbers for the mothers, as well as location based data and expected delivery date. \n\n\nThe Family Survey:\n\n\nAfter extracting out phone numbers for the mothers from the aforementioned survey, we were able to design an SMS based survey targeted for mothers using Twilio. We implemented a skeleton that checks in on the health status of the mother and child on a weekly basis.\n\n\nFuture Work:\n    - Building a GUI for easily editing the questions in the SMS - based surveys (or enable\n\n      read in through a text file)\n    - Using the Twilio-powered SMS not only as a means to gather data, but also as a \n      means to dispense key healthy living tips to mothers and families\n           - This could act as a means to incentivize families to engage with the data \n             collection \n    - Further exploring data collection from the ASHAs\n           - One ideas is to have ASHAs also report whenever there was a death in the\n\n             community, but weren't sure if there would be issues with incentivizing the \n             ASHAs to perform this additional work\n           - Collecting continuous updates from the ASHAS would serve as  a powerful \n             dataset, as we could reconcile with other collected data sets to get a sense of\n\n             whether the self-reported data is accurate\n           - Determine whether ASHAs are typically stationary (if an ASHA is in a particular\n\n             city, will they stay for years?)\n\n\n\n\n\n\nBuilt With\n\n\nexcel\nflask\ngoogle-drive\ngoogle-driver-api\nmongodb\npowermap\npython\ntwilio"
    },
    {
        "url": "https://devpost.com/software/invest1k",
        "content": "Inspired by the power of machine learning algorithms and our unwillingness to read dense quarterly earnings reports, we created a purely quantitative app to tell you what to buy, how long to hold, and when to sell, securities. Our algorithm is general enough to aggressively seek growth in all publicly traded equities, nimble enough to adapt to any fluctuations, and has proven itself to weather \"surprise\" events such as the financial crisis of 2008 and the tech bubble of 2000. \n\n\nWe believe that cleverly designed algorithms and unprecedented access to data will reshape the financial landscape. With Invest1k, just $1000 of seed money can empower the individual like never before. We are thankful for Bloomberg's API, which gave us the tools we needed to implement a real-time high frequency version of our algorithm. \n\n\n\n\n\n\nBuilt With\n\n\namazon-web-services\nappgyver-steroids\nbloomberg\ncss\ndropbox\nhtml5\njavascript\nkimono\nmatlab\npython\n\n\n\n\n\n\nTry it out\n\n\n\n\n\n\n\n\nshare.appgyver.com"
    },
    {
        "url": "https://devpost.com/software/faceboxx",
        "content": "Faceboxx is an open source tool that innovates a new feature for Facebook Chat: Cloud storage. Cloud storage is cheap. Incredibly cheap. But many providers are still charging users by size. We are disrupting the cloud storage space and allowing unlimited uploads using Facebook chat. \n\n\nWe have created a program that breaks any file up into 25mb chunks, encrypts them using 256-bit AES encryption, and uploads them to a users Facebook inbox, where they are stored indefinitely. At a later time, a user can use our program to download, re-combine, and decrypt the file. Also, we have discovered that the Facebook permalinks to these files can actually be shared publicly, which means we could turn our product into the ultimate cloud storage application. \n\n\nFaceboxx is currently a proof of concept, but can easily be expanded and created into a robust app that would solve the the issue of Cloud Storage and file sharing once and for all!\n\n\nFaceboxx: Free, Unlimited, & Secure Cloud Storage for All!\n\n\n\n\n\n\nBuilt With\n\n\nphantomjs\npyminizip\npython\nselenium\nwxpython\n\n\n\n\n\n\nTry it out\n\n\n\n\n\n\n\n\ngithub.com"
    },
    {
        "url": "https://devpost.com/software/arcode",
        "content": "ARcode, Audio Response Code, is an app that lets you send or broadcast data over the most universal medium on Earth, sound. \n\n\nBy a team of hackers from Rice University, UT Austin, and UC Berkeley, the inspiration for ARcode was to create a more convenient and useful upgrade of the standard \"QR code\" as a means for simple, instant data transfer. Integrating Microsoft Azure technology, ARcode has the compatibility to send any kind of data via a speaker and a microphone, no accounts, no pairing, no hassle.\n\n\nOur target users are anyone with a desire to share data to any audience instantly, whether it be a friend to another friend in a coffee shop, or a professor to a classroom full of students. Because of its robust simplicity, ARcode has versatile applications over a widespread area of society.\n\n\n\n\n\n\nBuilt With\n\n\naudacity\nazure\nfft\nios\njava\nnumpy\nobjc\nphp\npython\nwindows-movie-maker"
    },
    {
        "url": "https://devpost.com/software/guess-hue",
        "content": "Guess Hue?\n\n\nGuess Hue, a colorful twist on the classic \"I Spy\" for the Android platform that lets people interact more with the world around them. The game consists of three thirty-second rounds. A color is shown on the screen, and the clock starts! The player has to take a picture of something that includes the shown color before the time runs out. Points (up to 1000 per round!) are awarded based on how close the match is. After three rounds, the scores for the three rounds are added up for a total score, and the user gets to post to the local leaderboard if he or she does well enough.\n\n\n\n\n\n\nBuilt With\n\n\nandroid\n\n\n\n\n\n\nTry it out\n\n\n\n\n\n\n\n\ngithub.com"
    },
    {
        "url": "https://devpost.com/software/when-we-meet-again",
        "content": "FRESHMAN\n\n\nWhen We Meet Again\n is an unconventional reminders application that uses the proximity between two users in order to deliver its reminders. For example, if Nikhil and I were to decide to discuss hackathon ideas the next time we meet, the next time our phones came close together, I would get the reminder. This is extremely useful in our forgetful & rushed world today, where we tend to always postpone (or procrastinate) the things we have to do.\n\n\nWhat inspired us was the endless things we were forgetting to do when we decided to do it the next time we met. Our target user base is bottomless. From students to professionals to housewives, this app can be useful to everyone & anyone. We have an endless choice of location-based reminder applications today but none use relative-location or proximity between two phones as their parameter for reminding, which we thought was an untapped attribute of reminder applications. \n\n\nOur app uses Bluetooth in order to detect the proximity of the phones. We establish peer-to-peer connections and don't store any data, making the process as seamless & private as possible.\n\n\n\n\nExplanation of Screens:\n\n\n\n\nThe main activity showing all the reminders.\n\n\nAdding a new reminder. Note that the \"Person\" field is empty at this point.\n\n\nUsing an in-built Bluetooth Scanner to add a \"Person\" to the list. [List Display]\n\n\nUsing an in-built Bluetooth Scanner to add a \"Person\" to the list. [Toast after Adding]\n\n\nAdding a new reminder. Note that the \"Person\" field now contains the name of the person with whom you want to be reminded.\n\n\nAn example of the reminder notification.\n\n\n\n\n\n\n\n\nBuilt With\n\n\nandroid\nsqlite"
    },
    {
        "url": "https://devpost.com/software/heatkey",
        "content": "Key screens are a common video special effect. However they typically require a specially colored green screen for a background, and frequently the green color bleeds into the image.\n\n\nHeatKey uses the FLIR One's IR camera to detect people standing in front of a cool surface (for example, almost any wall that isn't on fire). It generates a mask that is then used to drop people into an arbitrary scene.\n\n\nHeatKey is based on OpenCV and the CVFunhouse computer vision framework for iOS.\n\n\n\n\n\n\nBuilt With\n\n\ncvfunhouse\nopencv\n\n\n\n\n\n\nTry it out\n\n\n\n\n\n\n\n\ngithub.com"
    },
    {
        "url": "https://devpost.com/software/hang-j74v2",
        "content": "We’re so obsessed with the act of taking photographs that we rarely reflect on the many images we’ve already taken. Compared to the instant gratification of digital photography, the time-consuming steps of editing, printing, and framing your photographs can be daunting. But the results can transform your space and can be so rewarding.\n\n\nHang streamlines this process for you. With this mobile tool, you can easily visualize complicated layouts with your photo library, with real time measurements, before having to commit to printing and framing.\n\n\nKey Features: \n\n\n\n\nStreamlines the multi-step process of editing > printing > framing > hanging\n\n\nHelps you visualize potential layouts without the complicated tools\n\n\nNo measuring tapes required, we do the measuring for you\n\n\nInspires you with new and different designs\n\n\nNo need for expensive interior designers\n\n\nQuick and easy image editing\n\n\nSimplifies the process for non-tech-savvy users as well as the design-naive \n\n\nCan pull images from ShutterStock\n\n\n\n\nAPIs Used:\n\n\n\n\nAstra: Stores the final images\n\n\nAviary: Image editor\n\n\nShutterstock: Image source for those lacking photos\n\n\nTwilio: Lets you know your order progress\n\n\nWalgreens: Send to print\n\n\n\n\n\n\n\n\nBuilt With\n\n\nandroid\nastra\naviary\nmongodb\nshutterstock\ntwilio\n\n\n\n\n\n\nTry it out\n\n\n\n\n\n\n\n\nwww.hellohang.com"
    },
    {
        "url": "https://devpost.com/software/lostoffline",
        "content": "Quick brainstorming session today at noon, about how one of us got lost in Edinburgh one night and ended up on a freeway thanks to having no data connection...\n\n\nNow we have managed to build up a simple, light android app (less than 900K!) that solves this problem by sending an SMS to ask for directions, to a server that gives output based on the esri API. Even better, the  directions are in the phone's native language - so you don't have to worry about being stranded in Beijing and having to use expensive data roaming.\n\n\nWe have coded the app and the companion server - which is also extremely lightweight, running in Javascript and Python, using Twilio for SMS processing.\n\n\nPresentation: \nhttps://docs.google.com/presentation/d/1cAoik-HOEXwsZjp7-iAdk-ZcklfDGFJFEqJsLlcXZpA/edit?usp=sharing\n\n\n\n\n\n\nBuilt With\n\n\nandroid\nesri\ngit\ngithub\njava\njavascript\npython\ntwilio\n\n\n\n\n\n\nTry it out\n\n\n\n\n\n\n\n\ngithub.com"
    },
    {
        "url": "https://devpost.com/software/infobahn",
        "content": "An internet connected chicken hatchery, demonstrating the value of IoT in agriculture."
    },
    {
        "url": "https://devpost.com/software/qlock-pj127",
        "content": "Trying to rid the world of keys! Qlock allows you to replace all of your pad locks and combination locks with a Qlock and your iPhone. With Qlock you can unlock your goods with a simple code on your phone. We would also like to support TouchID in the future.\n\n\n\n\n\n\nTry it out\n\n\n\n\n\n\n\n\ncarlwoodward.github.com"
    },
    {
        "url": "https://devpost.com/software/acc-dont",
        "content": "Acc!dont\n\n\nThe purpose of this project is to predict the future location of traffic accidents, redistribute traffic flow to minimize the likelihood of accidents, and give hospitals and EMS staff advance notice to reduce emergency response times and improve patient outcomes. This will be accomplished by pulling historical traffic, weather, accident and Twitter information and applying machine learning algorithms to generate future accident hotspots and warning signs to a high degree of accuracy. This information will then be used in app format to inform drivers of alternatives routes to avoid predicted future collisions, while also feeding notifications to local hospitals to allocate resources and route EMS staff to respond quickly and efficiently.\n\n\n\n\n\n\nBuilt With\n\n\nangular.js\nazure\nbootstrap\ncsv\nfirebase\nfirebase-datastore\nhere\nhere-batch-geocoder\nhere-geocoder\njavascript\nnode.js\npython\ntwitter\ntwitter-rest-api"
    },
    {
        "url": "https://devpost.com/software/byke",
        "content": "Byke is a system that allows conventional bike to change gears automatically.  The goal is to maintain a constant cadence while biking.  The system works well to assist you while riding on flat land, uphill, or downhill.  \n\n\n\n\n\n\nBuilt With\n\n\narduino\nhall-effect-sensors\nservos\n\n\n\n\n\n\nTry it out\n\n\n\n\n\n\n\n\nwww.princeton.edu"
    },
    {
        "url": "https://devpost.com/software/justread",
        "content": "The process: place your phone onto the 50¢ scanner, speak 'Just Read', and the phone will begin taking a video. Scan the length of the page (which takes about eight seconds), and stop the video. The video will be fragmented into stills and stitched together into one picture. JustRead will extract the text from the picture, print out the text, and speak it out loud.\n\n\nUsers:\nBlind people: the JustRead scanner, carved out of Google Cardboard, is designed to fit up with the edges of a page such that a person doesn't even need to see the product to use it--perfect for the six million blind Americans who cannot read everyday documents that are not encoded in Braille or recorded as audio.\nFirst-generation immigrants: your daughter brings her 2nd grade homework to you, but you cannot understand the language enough to help her. Scan the page and read a translation without having to type a single word.\nYoung readers: does your 2nd grader not know what a word sounds like? Does she need someone to read a passage to her? JustRead has it covered.\n\n\nWe are proud of the whole thing! Opening up the world to people with disabilities is, clichés be damned, our passion. When we realized that JustRead could be used for much more than just the blind community, we were even more thrilled.\n\n\n\n\n\n\nBuilt With\n\n\nandroid\nangular.js\napache\ngoogle-cardboard\nhtml\nionic\njavascript\nknife\npoetry\npython"
    },
    {
        "url": "https://devpost.com/software/dreadmill",
        "content": "Exercise equipment has tried for years to incorporate games and entertainment, but they've never hit the mark. That's why we made Dreadmill. Using an Oculus Rift, Dreadmill allows you to race through a Unity-built game world full of the undead while in real life beating your love handles on a modified, Arduino-controlled treadmill. A Myo armband adds even more interactivity, letting you fire on baddies during your aerobic workout. Dreadmill intends to prove that exercise and gaming aren't mutually exclusive, and to showcase the great leaps forward that have been made in virtual reality. So, strap in, load up, and get ready to run for your life!\n\n\n\n\n\n\nBuilt With\n\n\narduino\ncsharp\nhardware\nmac\nmyo\noculus\nunity\n\n\n\n\n\n\nTry it out\n\n\n\n\n\n\n\n\nwww.facebook.com"
    },
    {
        "url": "https://devpost.com/software/sheet-reader",
        "content": "We wanted to know what it would feel like if we had an artificial musician in our pockets powered by the cloud, so that the music of the greatest composers could be played in real time, without the need of a person nearby to play them! \n\n\nSheet Reader is helpful for any musical enthusiast who enjoys classical music or even music of all genres, such as Rock.  Enthusiasts would be able to use the technology to play any kind of music using the standard Western music notation. It would also be helpful for musicians who want to hear the notes on their music sheets as they are practicing or composing on their instruments.\n\n\n\n\n\n\nBuilt With\n\n\naudiveris\nffmpeg\nflask\njava\nmusescore\nmusicxml\npython\nswift"
    },
    {
        "url": "https://devpost.com/software/soundscribe-txtcp",
        "content": "Finding sheet music is hard, and writing it is even harder. Despite the countless number of websites proclaiming uninhibited access to written music, most are blocked by pay-walls or held back by limited selections. SoundScribe offers an alternative, by instantly transcribing any music you play into a clean PDF. SoundScribe allows anyone to either transcribe a song they memorized and have long-since forgotten, or one they just created.\n\n\nWith SoundScribe, you merely record and upload your music to our application. Given the audio file, SoundScribe uses signal processing and machine learning to extract different notes, based on the particular timbre of your instrument. It then generates a LilyPond file which then gets exported to a PDF -- completely hassle free!\n\n\nThis kind of software does not yet exist, especially in an open source format. Even with the assistance of Fourier Transforms, the problem of recognizing notes and beats from a set of frequencies has been typically regarded as a very challenging problem. However, we managed to circumvent these issues by utilizing machine learning to allow our program to learn to identify patterns in recorded music. We even allow for basic music notation to more fully communicate your music.\n\n\nSoundScribe makes the preservation of musical culture easier and much more accessible.\n\n\n\n\n\n\nBuilt With\n\n\nc\ncanva\nfftw\njava\nlibsndfile\nlilypond\nmac\nmachine-hearing\npython\nsignal-processing\nwindows"
    },
    {
        "url": "https://devpost.com/software/headlights",
        "content": "All of us being avid bikers, we all agreed that the current situation for bikers in busy streets like New York or Boston dealt with way too many accidents and unfortunate circumstances.  One of the reasons this is the case is because there is only an archaic set of gestures that indicate to drivers and other cyclists behind them whether they were turning or stopping.  This needs to be known by both the biker and the observers in order for them to be fully functional.  Enter our product.  This allows for people to use simple gestures to avoid collisions and in the chance that there is an accident, contact preset people or the police to let people know that there was an accident.  \n\n\nWe are most proud of the way that the myo works with the turn signals.  We have built it in a way such that it will be extremely convenient for bikers to be able to use our product. The Myo itself talks to our Android app through bluetooth while the app connects to the Edison through the Wifi without needing internet access from either device. \n\n\nFeatures:\n\n\n\n\nmyo controlled lights to indicate turning and stopping\n\n\nCrash detection via myo's accelerometer\n\n\nTexts emergency contacts after an accident using Twilio\n\n\nData visualization for Bike crashes using fusion tables\n\n\n\n\nTable 6\n\n\n\n\n\n\nBuilt With\n\n\nandroid\ngoogle-fusion-tables\nhardware\nintel-edison\nnode.js\ntwilio"
    },
    {
        "url": "https://devpost.com/software/color-reno",
        "content": "Color Reno\n\n\nVisualizes color pallettes from images in the Reno area.\n\n\nThe project used Apache spark to comb through 100M creative commons images and find images that were geotagged in and around Reno. These images were then processed with three different color palette finding tools. Imagemagick, colorific, and a custom implementation were used to find color palettes. They produce surprisingly different results.\n\n\nThe primary data visualization was implemented as a canvas layer on top of leaflet.js. The image explorer was implemented with cartoDB.\n\n\n\n\n\n\nBuilt With\n\n\ncss\njavascript\npython\nruby\nshell\nweb\n\n\n\n\n\n\nTry it out\n\n\n\n\n\n\n\n\ncolorreno.com"
    },
    {
        "url": "https://devpost.com/software/new-york-times-wedding-annoucements-data-visualizations",
        "content": "For my hack I wanted to explore the New York Times (NYT) Wedding Announcements (\nhttp://www.nytimes.com/pages/fashion/weddings/\n) and do some corresponding data visualizations. The NYT's wedding announcements are a strange cultural phenomena and are often very elitist and downright strange (seriously, go read one of them). I used modern data mining/AI/NLP techniques to extract a lot of interesting information from this data source and constructed some (awesome) visualizations using D3. Go check out (\nhttp://nytimes-thesis.herokuapp.com/\n) for more information and please ignore any typos in the content. The only sleep i got was when i was waiting for NLP jobs to finish running :'(\n\n\n\n\n\n\nBuilt With\n\n\nd3.js\ndreams\nflask\nhopes\njavascript\nkanye\nmongodb\npython\nreact\nt.pain\ntears\nweb\n\n\n\n\n\n\nTry it out\n\n\n\n\n\n\n\n\nnytimes-thesis.herokuapp.com\n\n\n\n\n\n\n\n\nnytweddings.com"
    },
    {
        "url": "https://devpost.com/software/seeusoon",
        "content": "Are you engaged in a long distance relationship? Do you often spend long hours planning your trips to meet your beloved one?\n\n\nWe have the solution for you!\n\n\nOur webapp helps you monitor the flights on the route you and your partner do in order to find the best way to get together, week after week.\n\n\nWe offer you a notification service to warn you when a significant change in price is seen in your itinerary, or when your partner have found an interesting flight and want to share it with you.\n\n\nThanks to our payment service, we plan on enabling you to share the purchase of a flight with your partner, so each of you can contribute to the payment in case one of you wants to help out the other.\n\n\nFinally, because you are not the only couple travelling across your route, we bring to you some useful tips from a couple in the same situation as yours.\n\n\nSeeUsoon is a custom made travel agency answering the needs of millions of couples sharing their love across two different cities\n\n\n\n\n\n\nBuilt With\n\n\nangular.js\njavascript\nmongodb\nnode.js\nweb\nwindows\n\n\n\n\n\n\nTry it out\n\n\n\n\n\n\n\n\nwww.seeusoon.io"
    },
    {
        "url": "https://devpost.com/software/duck-hunt-imax-with-real-robotic-duck-and-mobile-gaming-app",
        "content": "We want to make mobile gaming more outdoorsy and engage everybody to be more active with their mobile device! Introduce the most revolutionary robot-human interactive gaming system ever, the Duck Hunt Robotics.  For people in our 20s, we are often very nostalgic about the classic Nintendo Duck Hunt Game. Even though it is a great adoption and interesting simulation of the real world hunting activity, the unique excitation of the physical activity cannot be captured. With the advance of new technologies in robotics, mobile, cloud and computer vision, here we took the classic duck hunt game idea even further and pushed the boundary of traditional mobile gaming. With this software and hardware hack, we aim to promote the future of gaming where cyber-physical interaction plays a prominent role, enabled by the synergy of the computing technology, the immerging robotic technology, virtual reality and artificial intelligence.\nHere for our system, the ‘duck’ is a robotic flapping ornithopter (the duck robot) which is radio-controlled locally and cloud-controlled through cloud service (i.e., a device of internet of things). The prototyped gaming app adopts onboard computer vision algorithms to track the flying duck robot. Once the ‘duck’ is aimed, the user ‘shoots’ the target with a touch on the screen. If the shot is on target, the app will send a HTTP request to the cloud server and then web server agent will message the Electric-imp end hardware to command the robot wirelessly to drop and fall. This control of ‘kill’ is through cloud service, thus not limited by range. \nSpecifically for the hardware hack, to incorporate the remote control flapping wing robot with the game, we modified the radio remote of a commercial available flapping wing toy. A Wi-Fi module is connected to the remote, and linked to the cloud server of Electric Imp. This enables the data transfer between the app and the remote. As an add-on to the gaming system, a confetti-popping machine is built to add a more dramatic effect. The machine also incorporates a Wi-Fi module to communicate with the game; a microcontroller (Arduino Leonardo) is used to drive actuators. We utilize a DC motor as actuator to trigger the confetti popper, however, different objects can be adapted to the system.\nSpecifically for the software hack, the GUI is concise yet elegant. It features a single and multiplayer mode, which is perfect for partying or family reunion. Each player, the duck hunter in the game, can compete with others, sync score in the cloud and share achievements in the social network (future work). When the player is in the sniper mode, DuckHunterIMAX uses computer vision and mobile computing techniques, such as CamShift and Dominant Color Domain (DCD) feature to track interesting object, the duck robot in this case. If the duck is within a certain range of the target in the shooting view, it will be detected. Each player has 3 chances (bullets) to hunt the duck. If the hunter shots the duck, a trigger signal will be sent to Electrical Imp cloud, which later will be used to communicate with the Arduino based Hackathon badge to hack the robot controller. Then, Duck robot remote controller will be commanded to cut off. Consequently and sadly, the duck is falling from the sky.  Code and firmware available at \nhttps://github.com/air23zj/DuckHuntRobotics\n\n\n\n\n\n\nBuilt With\n\n\nandroid\narduino\ncpp\nelectric-imp\nhardware\nhttp\nios\nmotor-control\nopencv\nradio-control\nsquirrel\nweb\nwindows-phone"
    },
    {
        "url": "https://devpost.com/software/tank-robot",
        "content": "VIDEO: \nhttps://www.dropbox.com/sh/3r1oy48q8lt9xwv/AACcR3JKoDo3P1rBVOask5xRa/2014-10-18%2018.23.26.mp4?dl=0\n\n\nThe Tank Robot is a robot that can be controlled from any location in the world. I can be in a remote village in China while this robot roams the HUB. How? We send simple data packets over the internet in order to achieve seamless communication over the cloud with nearly INSTANT response, even with the crowded Wi-Fi.\n\n\nHow do we control the robot over the internet? We use Oculus Rift, which gets a direct video feed from the phone camera on the robot. The user is fully immersed in a 3D projection of the skype call, and can look around in the robot's using the Oculus Rift.\n\n\nWhat do we use to control the robot? We can control it using Android, Android Wear, Myo, and any other device that has internet connectivity. How did we achieve this? We used firebase to update a child node on the cloud every 50ms with the left and right values for the tank drive. We only send a value if it is different from the previous value sent, to save on bandwidth. Simple, fast, & efficient.\n\n\nIn addition to all this, the robot sends its location to the controller, and the controller app plots the trail the robot took on a Google Maps window. Furthermore, perhaps the coolest feature of all is that the Android app has a button to POP OUT into a floating window, so that the user can still control the app while Skyping via the phone. The controller app on Android and Android Wear feature arrows for direction, as well as a joystick with smooth animations!\n\n\n\n\n\n\nBuilt With\n\n\nandroid\nandroid-wear\narduino\nc\nfirebase\ngoogle-maps\nhardware\njava\nmac\nmyo\noculus\nrobotics\nweb\nwindows\n\n\n\n\n\n\nTry it out\n\n\n\n\n\n\n\n\nwww.dropbox.com"
    },
    {
        "url": "https://devpost.com/software/green-hackers",
        "content": "The app is named \"Chaatall\"\n\n\nHere is the story: We have people from different places working in our company (as obvious) and we had a group created in WhatsApp for internal chatting purposes. Cracking jokes and passing thoughts etc. But we had people chatting in their regional language sometimes which actually made some kind of invisible sub-groups in that group. This made us feel bad. We thought what if we can build one application which allows the user to text in their choice of language and the other person can understand the same in their language.\n\n\nOur target users are people like this. People who love their language while texting. Language should not be a reason for people not to communicate each other.\n\n\n\n\n\n\nBuilt With\n\n\nandroid\ngoogle-translate\ngson\nruby-on-rails\n\n\n\n\n\n\nTry it out\n\n\n\n\n\n\n\n\ngithub.com"
    },
    {
        "url": "https://devpost.com/software/splash",
        "content": "The internet may be ubiquitous, but networking is not a solved problem.  With NSA spying, celeb nude leaks, and growing awareness of the limitations of ipv6, the last few years have brought much public attention to over-the-top layered protocols and even alternative nets.\n\n\nAny mass gathering is a threat to internet stability.  The Arab Spring brought to light the incredible power of Twitter during protests, and more recently FireChat's intranet helped fuel student involvement in Hong Kong.  During protests, a government's ability to disable cell service and thus internet data can be debilitating and deadly.\n\n\nPrivacy also seems like a bigger concern now than ever.  The Tor protocol has taken off as a way to get near-anonymity online, and despite its reputation for drugs has played an important part in securing civil liberties.\n\n\nIt also really sucks when wifi is down from mass usage at hackathons, and barely anyone can tether because the building is so packed.\n\n\nSo we created a one-to-one messaging platform based on Bluetooth Low Energy technology in our iPhones.  It is fast, requires no wi-fi or cellular connection, and completely local.  There is no NSA threat, because the messages never hit the internet.  In large gathering, the intranet can be as big as the space the people occupy.\n\n\nEach iPhone acts as a Bluetooth Low Energy node, and finds nearby nodes.  While one phone may have a BLE range of 30 - 100 feet, our application lets them chain together to form a mesh.  By creating a mesh network of phones, messages can bounce through multiple nodes to get to their recipient. \n\n\n\n\n\n\nBuilt With\n\n\nbluetooth\nios\nmultipeer-connectivity-framework\nobjective-c\nprediction-io\nswift"
    },
    {
        "url": "https://devpost.com/software/bartndr-a-location-aware-end-to-end-retail-solution",
        "content": "Everyone has had the experience of having trouble getting the bartender's attention at a bar before. What if you never had to go through the bartender to get your drink?\n\n\nBartndr is an \nautomated, location-aware end-to-end\n retail solution. It includes the Bartndr iOS app that serves as beautiful location-aware point of sale system through the use of \niBeacon\n technology and \nBraintree\n. It also includes the Bartndr barbot, which serves as a simulation of automated retail production.\n\n\nPlease come by our table and see what \nBartndr\n can do!\n\n\nUser Flow:\n\n\n\n\nUser A brings phone in radius of Beacon B (Beacon B -> Store B)\n\n\nUser A receives a push notification to open Bartndr\n\n\nUser A presented with list of Items from Store B\n\n\nUser A selects Item X and checks out\n\n\nBarbot C receives Task X for Item X and begins production\n\n\nBarbot C notifies API when production of Task X complete\n\n\nUser A receives push notification to collect Item X \n\n\n\n\nTechnologies Used:\n\n\n\n\nArduino\n\n\nBJT\n\n\nAdafruit Neopixels\n\n\nAdafruit CC3000 WiFi Shield\n\n\niOS\n\n\nNodeJS\n\n\nAngularJS\n\n\nExpress\n\n\nGrunt\n\n\nParse\n\n\nBraintree\n\n\n\n\n\n\n\n\nBuilt With\n\n\nadafruit\narduino\nbraintree\nexpress.js\ngrunt.js\nhardware\nios\nnode.js\nobjective-c\nparse\nweb\n\n\n\n\n\n\nTry it out\n\n\n\n\n\n\n\n\nwww.bartndr.me"
    },
    {
        "url": "https://devpost.com/software/popsicle",
        "content": "Browse the Internet without The Internet\n\n\nWhat if you could browse the Internet in Airplane mode? Through the power of sharing, caring, and iOS 8 — you can. Open any website in Safari, tap the \"Save with Popsicle\" button, and the \nentire website\n is saved for offline browsing — including links, images, and styles. You can even click links on the page and browse them offline as well, using our intelligent spidering algorithm.\n\n\nEven if you never downloaded it\n\n\nStuck on a plane without a recent copy of the New York Times? Someone on board probably has your back. Popsicle can connect via Bluebooth or peer-to-peer WiFi to transfer caches between users. Just tap on the website you want to download from another popsicle user, and in a matter of seconds you'll be reading the latest edition.\n\n\n\n\n\n\nBuilt With\n\n\nios"
    },
    {
        "url": "https://devpost.com/software/fyrefly",
        "content": "Fyrefly is an iOS app that lets users share a canvas in real time.  Your drawings disappear as you create them so draw fast and be spontaneous!\n\n\n\n\n\n\nBuilt With\n\n\nfirebase\nios\nswift\n\n\n\n\n\n\nTry it out\n\n\n\n\n\n\n\n\nwww.fyreflyapp.com"
    },
    {
        "url": "https://devpost.com/software/contrabox",
        "content": "We originally centered our idea around the theme of the competition, \"The Past,\" by planning a two-player game set at the Boston Tea Party. That idea is now in \"the past,\" as we removed the historical element from our game to focus on creating an interesting, two-player game with a clean and simple design. Our game is called \"ContraBox\", and is played by two players using the same keyboard and mouse in a two-player Rush Hour-inspired interface. Players alternate moving either their main box or one of their obstacle boxes on each turn, and win by navigating their main box to the exit on the far right of the game grid. Because players can have control of either their main box or one of their obstacle boxes, one cannot simply box in their opponent with obstacle boxes, and their opponent could then do the same to the original player. As a result, one must carefully consider strategy in moving their obstacles throughout the game. \nWe are most proud of our collision feature, which prevents any two boxes in the grid from overlapping. This allows each box to act as an obstacle for the others. \n\n\n\n\n\n\nBuilt With\n\n\ncss\nhtml\njavascript\njquery\nweb\n\n\n\n\n\n\nTry it out\n\n\n\n\n\n\n\n\nshare.pencilcode.net"
    },
    {
        "url": "https://devpost.com/software/pebilepsy",
        "content": "We built our application, Pebilepsy, for members of Eric’s family who both have struggled with epilepsy their entire lives. Eric and his family looked for ways to prevent further incidents. When they sought specialists and used medications, the family members quickly found that they were still having micro-epileptic episodes, informally called \"shakes\". \n\n\nThough epilepsy patients have medication, specialists and other forms of prevention, shakes are still common and are very wide spread. During appointments to the specialists, questions on these shakes often asked impossible to answer questions without biometric tracking or very subjective questions of the severity of these episodes. Additionally, nocturnal seizures could be a risk, as a majority of patients go un- or misdiagnosed due to a lack of active monitoring by other people during sleeping hours. Additionally, the disrupted sleep cycles substantially raises the risks of both day- and night-time seizures in these patients.\n\n\nPebilepsy targets North American sufferers of nocturnal seizures, specifically those with myoclonic and tonic-clonic seizure attributes. We target users that have had a history or signs of nightly seizure triggers or excessive sleep deprivation. In North America, 1% of the population has epilepsy, and between 6-10% of those has nocturnal seizures; our initial market is between 213,000-354,000 people. We intended to encompass more nations after the initial trial. With 50 million epileptics globally, developing nations represent a hefty majority, over 80%! This represents a 4.5-7+ million user global market. Due to a lack of medication, health services and applicable medical data, they also represent the global majority of nocturnal seizure suffers. I\nIntroduction of affordable smartphones and wearables make this an attainable market in the coming years.\n\n\nPebilepsy is a Pebble application that syncs with mobile and web platforms. A user activates the application before going to bed. Nocturnal seizures are most prevalent during light sleep, soon after going to bed and waking up. The majority of these seizures are tonic-clonic. After use, data is instantly uploaded to the web and mobile platforms, recording the number of seizure like incidents and the severity of the attack.\n\n\nWe use Pebble's accelerometer specifically to target our audience. Recording during sleep makes the data more accurate, and Pebble's battery life allows us to produce detailed logs of nightly episodes. Information is then sent to stakeholders in two formats. To the user & their physician, a detailed visualization is provided of the number of episodes and levels of severity. Use of Pebble's accelerometer allows us to segment episodes by degree of attack. The second format produced is primarily for third party research and data extraction purposes. Produced in CSV, we provide a platform specifically to enable easy summation of the data as well as its raw form. \n\n\nThree key features set Pebilepsy apart from its competitors. Firstly, Pebilepsy represents an affordable and practical method of data collection. Other solutions rely on expensive and bulky equipment or impractical methods, such as invasive devices or sleeping with a cell phone. Second, this market is grossly under-serviced. Because this condition is difficult to monitor and often misdiagnosed, current available data is incomplete at best. So much so, that our application could immediately produce several new forms of data inputs that could be viable applications to future clinical research. Nocturnal seizures are rarely witnessed by specialists, people living with epilepsy or their family members, thus Pebilepsy would be a welcome addition to the diagnostic armamentarium! Finally, combine the Pebble's accelerometer and data visualization technology to produce new and unique insights. We can track the number and severity of an attack, as well as which parts of the body are convulsing the most. This enables better research capabilities, prevention methods and analysis of an under-diagnosed condition. \n\n\nHaving family who live with epilepsy, as well as working with Epilepsy Ontario on prior initiatives, this is a cause Eric and our team believes in. Until now, the technology, data and necessary infrastructure were unavailable. We are entering a phase of information and technological revolution, where niche health care conditions can get the attention, medication and available data they need to live higher quality lives. Pebilepsy will be the first important step in this journey.\n\n\nDemos here:\n\n\nhttps://www.youtube.com/watch?v=HnQbcM2YGDI&list=PLA6wy84hw2aNGiHELgUsw1khQvXzRUC-q\n\n\nhttps://www.youtube.com/watch?v=FxrDIuKMHEM&index=2&list=PLA6wy84hw2aNGiHELgUsw1khQvXzRUC-q\n\n\n\n\n\n\nBuilt With\n\n\naccelerometer-api\nandroid\nbootstrap\nc\nc++\nd3.js\ngroovy\nhardware\nheroku\njava\njavascript\nnode.js\npebble\npython\nr\nshiny\nshinyapps.io\nweb\n\n\n\n\n\n\nTry it out\n\n\n\n\n\n\n\n\nthawing-garden-9627.herokuapp.com"
    },
    {
        "url": "https://devpost.com/software/box-of-dreams",
        "content": "The volumetric display was created by channeling fog generated by an ultrasonic mister through a pipe, directed by straws downward to form a fog column, into which we projected 3 separate rendered views of the object in question. We used a kinect to acquire facial data in 3 dimensions and processed it from multiple perspectives. By displaying each perspective on its own projector, we managed to reconstruct the 3D object, in this case a face, within the fog column.\nIn addition, we also wrote a program to render any video on this display. \n\n\n\n\n\n\nBuilt With\n\n\nc++\nhardware\njava\nkinect\nmac\npoint-cloud-library\nwindows"
    },
    {
        "url": "https://devpost.com/software/helping-hands-bra",
        "content": "The Helping Hands Bra is a collaboration of experienced nursing and pumping moms, lactation experts and dedicated mechanical engineers.  With the idea that nursing moms deserved more than just suction based lactation tools, we designed a sturdy, easy to clean, minimal parts, hands free compression bra.  Our starting inspiration from real moms, like us (5 out of 8 on our team are experienced nursing mothers) combined with personal stories of moms all over the United States highlighted an underserved market.   Moms looking to increase milk supply pumping, or feeding, and moms with over supply needing plugged duct/mastitis treatment and relief are using the same techniques- breast compression and massage.  \n\n\nUsing a simple motor to inflate and deflate quilted soft bead filled compartments, in a concentric ring pattern over all the milk storage and compression tissue, the milk is gentlyly and comfortably moved forward to be removed by baby or pump. The beaded compartments are inflated and deflated using a simple pump, and the compression is helped by a network of drawstring supported elastic.  Comfortable, warmable, light-weight, easy to clean, makes this an innovative solution to a myriad of common breastfeeding challenges.\n\n\n\n\n\n\nBuilt With\n\n\nwindows"
    },
    {
        "url": "https://devpost.com/software/pain90x",
        "content": "With the growth of health apps, smartphones are becoming an increasingly powerful means of maintaining fitness. However, as most people can attest, staying true to your fitness goals can be challenging, which is why we created Intellifit. Our app monitors your motion and keeps you accountable to your physical goals with machine learning and potentially painful punishments.\n\n\nFirst, Intellifit monitors your motion through the Myo API and presents that data with a graph visualization. Our algorithm can match motion data to predefined exercises, essentially recognizing what you are doing. In addition, you can record your own exercises and set goals for how many of each exercise you want to do. If you happen to fall behind on your goals, Intellifit will lock you out of certain apps until you complete them.\n\n\n\n\n\n\nBuilt With\n\n\nhardware\nios\nmyo"
    },
    {
        "url": "https://devpost.com/software/leapsign-qejau",
        "content": "ASL is the primary means of communication for those with hearing and speech impairments in the United States. While there are resources such as interpreters and translators available to ASL users, these resources are not always present, making it difficult for ASL users to communicate.\n\n\nLeapSign aims to bridge the communication gap between ASL users and non-users by allowing seamless and natural translation from ASL to spoken language.\n\n\n\n\n\n\nBuilt With\n\n\neclipse\ngoogle-translate\njava\nleap-motion\nweb\nwindows"
    },
    {
        "url": "https://devpost.com/software/fuji",
        "content": "Fuji is like Xcode in your browser. It's a fully functional web IDE for developing iOS apps in your browser.You can log in with GitHub, immediately edit and commit to your projects, and view your work instantly in an iPhone simulator directly in your browser.\n\n\n\n\n\n\nBuilt With\n\n\nace\ngo\nguacamole\nhtml5\nios\njquery\nos-x\nvnc\nweb\nxcode"
    },
    {
        "url": "https://devpost.com/software/magic-board",
        "content": "I saw the Boosted Board when a member of the Facebook group \"Hackathon Hackers\" posted a link to the website (boostedboards.com).  I was instantly pumped about the Boosted Board and wanted to get my hands on one.  Simultaneously, I was really pumped about the Myo.  So, I combined my two new favorite things and came up with the \"Magic Board\".\n\n\n\n\n\n\nBuilt With\n\n\nbluetooth\nboosted-board\nhardware\nios\nmyo\nnode.js\nobjective-c\ntessel"
    },
    {
        "url": "https://devpost.com/software/amation",
        "content": "Amation combines the extensive capabilities of SVG with the power of media tools and animation. Users animate vectors in a timeline-based editor suite (like After Effects) and can distribute them as native SVGs, in all their scalable glory.\n\n\nAmation is for the design community. UI design workflows include prototyping interactions to communicate with developers. With Amation, designers can maintain a full vector workflow, ultimately saving time, easing distribution, and improving performance.\n\n\nScalable Vector Graphic, or SVG, is extremely powerful file format. SVG is often used for individual assets, but has media opportunities in motion graphics, animation, and video. While raster media (like GIFs and MP4s) are based on grids of pixels that lose quality when resized, SVGs are infinitely scalable and lite. Since they're XML-based, Amation uses JavaScript to manipulate the code and create packaged, native SVG media files that animate in the browser.\n\n\n\n\n\n\nBuilt With\n\n\nbluemix\njavascript\njquery\nmongodb\nnode.js\nsvg\nweb\n\n\n\n\n\n\nTry it out\n\n\n\n\n\n\n\n\namation.me"
    },
    {
        "url": "https://devpost.com/software/tranquility-5jch4",
        "content": "We were inspired by PennApps Health and the opportunity to build something that sets out to solve a serious problem in healthcare.\n\n\nWith all of today's wellness apps, we saw a major opportunity in changing the paradigm. All apps require users to type in their data and record logs of their activity. This is an awkward process and often discourages potential users from using the app. We wanted to take this step out of the equation and automate it using voice control.\n\n\nThe target user is anyone. Whether you are a fitness nut who likes to keep track of your diet or someone who needs to do better at controlling what they eat. \n\n\nWith our app, we use Siri to control and send messages to our app. Through Siri and Twilio, we send food information to the app, all in the simple manner of starting a phrase with \"tell tranquility...\".\n\n\nThe user then tells tranquility what they ate that day and our app will parse the message for food times and use a Mashery API to get the nutritional facts. This data can then easily be seen in a chart form in the app by the user.\n\n\n\n\n\n\nBuilt With\n\n\nhardware\nheroku\nios\nmashery\nmongodb\nnode.js\nobjective-c\npebble\nsiri\ntwilio\n\n\n\n\n\n\nTry it out\n\n\n\n\n\n\n\n\nwww.telltranquility.com"
    },
    {
        "url": "https://devpost.com/software/snackhack",
        "content": "You've managed to find a 30-minute window in the middle of your hectic Tuesday afternoon, and you'd like to use that precious time to grab lunch. You don't have time to wait for an hour and a half for your vegan tofu wrap. You need to know where to go to avoid the crowds, so that you can get in, get your food, and still have time to eat before your 2PM lecture. Until now, though, you haven't had much of a choice, other than picking a random restaurant and hoping you were lucky that day.\n\n\nEnter SnackHack:\n a web service and Android app that leverages Twitter's Geolocation API and Wash. U. network data to provide real-time information on just how crowded every restaurant on campus is. No longer is eating at Wash. U. a game of wait-time roulette. Instead, you can find the perfect restaurant in an instant, utilize our robust historical data engine to find the best time to visit a favorite spot – like, for instance, that you can get a Holmes Lounge sandwich with minimal fuss at 10:15, and spend less time waiting in line and more time doing the things you love.\n\n\nInspired by a long-standing hatred of lines and a longer-standing love of food, we've built Snackhack primarily to solve a problem for Wash. U. students, staff, and faculty. That said, Snackhack isn't just a restaurant app. It's also a flexible API for analyzing how busy any location in the world is. Just provide coordinates or an address, and you're ready to go. With complete API docs, and bindings in Ruby and Python, it couldn't easier.\n\n\n\n\n\n\nBuilt With\n\n\nandroid\nheroku\njava\njavascript\njson\nruby\nsinatra\ntwitter\nweb\n\n\n\n\n\n\nTry it out\n\n\n\n\n\n\n\n\nwuhack.herokuapp.com"
    },
    {
        "url": "https://devpost.com/software/quitli",
        "content": "Everybody has habits they'd like to break--and often times, it's as simple as being reminded that you're doing something you'll regret. Quitli brings the quantified self movement to the psychology of habit by recording those moments when you're not at your best and letting you visualize their frequency. Quitli pins onto the shirt and takes data on several events depending on the user's habits and what they want to track.\n\n\n-Using classified accelerometrics, Quitli knows when users take the elevator instead of stairs.\n-With an on-board microphone, it also knows when users crack their knuckles.\n-Quitli even has a smoke detector and keeps track of how often you have a cigarette.\n-Quitli cares a lot about your posture and will notify you when you're slouching.\n-It also keeps track of a user's sedentary state and will notify them if they spend too much time in one place without much activity. Along the same lines, Quitli will tell a user how frequently they stay up too late.\n-By tying into the Plaid API, Quitli knows when you make poor purchases. It records every time you buy fast food, as well as every time you buy anything from a type of store you've blacklisted.\n\n\nQuitli hopes to help make you a better person by letting you know how often you let yourself down. Quitli provides an iOS and a web dashboard for viewing your statistics on the above events, and provides additional features which further incentivize reforming your bad habits. For example, with your permission, Quitli can make a post to Facebook every time you choose the elevator over the stairs.\n\n\n\n\n\n\nBuilt With\n\n\naccelerometer\narduino\nfacebook\nfft\nhardware\nios\nplaid\npython\nweb\n\n\n\n\n\n\nTry it out\n\n\n\n\n\n\n\n\nquit.li"
    },
    {
        "url": "https://devpost.com/software/sirikit-fljjk",
        "content": "We love Siri--Apple's sassy voice assistant has delighted us since day one. But why can't Siri...\n...make one-time and recurring payments to your friends?\n...find your backpack for you?\n...activate your Jambox?\n...control your automated home?\n...interface with all your IoT devices?\n\n\nThough Apple has not released a public API for its voice interface, we have found a workaround that allows third-party apps to grab data from Siri and take actions in response. Importantly, this proceeds without any traffic proxying, data limitations (eg, wifi only), or browser trickery.\n\n\nSiri, plain and simple. And now limitless.\n\n\n\n\n\n\nBuilt With\n\n\narduino\nelectric-imp\nintel-galileo\nios\njawbone\nobjective-c\nsiri\nsmartthings\nsquirrel\nvenmo"
    },
    {
        "url": "https://devpost.com/software/prepark-k3w06",
        "content": "Despite the advancement of technologies such as GPS and Cell Phones, the way we park has remained in the past. Now, using a combination of hardware and software, using phones and an Intel chip, we are able to conveniently integrate the archaic Parking system with our modern technology. We use hardware to identify the state of a parking spot, and use a server to convey that information to a Windows Phone. By doing so, we allow the user a means of viewing vacant nearby parking lots, finding specific spots, and even temporarily reserving them.\n\n\n\n\n\n\nBuilt With\n\n\n.net\nc#\nhardware\nintel\nintel-edison\nmysql\nnode.js\nnokia\nphp\nweb\nwindows-phone"
    },
    {
        "url": "https://devpost.com/software/flare-tm5n9",
        "content": "Flare is a novel communications protocol that encodes data as a sequence of colors, which are then flashed on one phone's screen and read by the camera of another phone. It is a cross-platform and simple alternative to NFC, and also can be used to transfer sensitive information without worrying about snooping on the network. \n\n\nAs each phone sees a flashing message with its camera, it also outputs the same message in real time. This allows daisy chaining of devices, letting Flare support multiple devices receiving the same message at once. \n\n\nAt the Pennapps opening ceremony, our team learned about the new camera API features in iOS 8. This solidified the idea of Flare, and inspired us to explore it further. Although we tinkered with the Android camera API, iOS 8 seemed to be vastly superior and we focused on making an Android send -> iOS receive proof of concept. As a more specific demonstration, we also enabled the iOS app to directly open URLs received. \n\n\nIn addition to being an interesting and simpler alternative to NFC, with growing online privacy concerns we think that certain users may also appreciate having a communication protocol that doesn't require any form of wireless communication, that could be intercepted, revealing passwords and keys.\n\n\nAlthough we were worried the protocol would be unusably slow, we are proud of the fact we ended up capturing the flashes of light at 15 fps. This allows for decent data transfer rates and a huge awe-factor with the speed of the blinking. \n\n\n\n\n\n\nBuilt With\n\n\nandroid\nios\njava\nobjective-c"
    },
    {
        "url": "https://devpost.com/software/cosmos-browser",
        "content": "Cosmos allows you to browse the web without having any data or wifi connection. \nIt uses text messaging to get the html which we then render to display the website.\n\n\n\n\n\n\nBuilt With\n\n\nandroid\nbluemix\nexpress.js\njava\nnode.js\ntwilio"
    },
    {
        "url": "https://devpost.com/software/tug",
        "content": "Existing infrastructure relies on a \"heads-up\" visual awareness; for a society frequently buried in smart devices we believe that pedestrian signaling must be extended to the screens of smart devices.\n\n\nTUG sends localized phone-based alerts to pedestrians approaching crosswalks for which they do not have right of way. The solution specifically targets phone-engaged pedestrians who fail to take notice of pedestrian signaling.\n\n\nBy re-engaging pedestrian attention to traffic signaling we believe TUG will help reduce phone-distraction related collisions.\n\n\nAn Example\n\n\nLet's observe how this works\n\n\nBlue man finds himself walking approaching a crosswalk for which he does not have the right of way. The nearby cross walk post sends a visual SAFETY HAND notification appears temporarily on his screen, serving as a reminder that he does not have right of way.\n\n\n\n\nThe glowing orange circle indicates the affecting range of the pedestrian warning signal. \n\n\nWe like to think of this reminder as a \"friendly tug,\" analogous to a child tugging on the hand of a distracted mother.\n\n\nLikewise, observe Pink person in the same situation, for which Blue man is in the clear.\n\n\n\n\nImplementation Overview\n\n\nTechnically, this solution is implemented using a tailored proximity bluetooth communication between pedestrian traffic posts and mobile phone users. \n\n\nThe \"traffic post\" module syncs with nearby phones and and sends out warning notifications (the SAFETY HAND) that becomes overlain on nearby phone screens.\n\n\nPhones run the TUG application. The application runs in the background awaiting connection to traffic the traffic post module.\n\n\nIn practice our solution has been implemented using phone hardware; one phone runs the \"traffic post\" application and acts as the post with other phones running the \"pedestrian\" application acting as pedestrians. Tailored proximity is handled via physical shielding. \n\n\n\n\n\n\nBuilt With\n\n\nandroid\njava"
    },
    {
        "url": "https://devpost.com/software/android-for-all",
        "content": "Android for All\n\n\nAndroid is currently the biggest mobile platform in the world, but Apple's iOS remains an extremely ubiquitous platform with a huge install base. While competition between different mobile platforms is important, having two or three dominant mobile platforms isn't an ideal solution for developers or users. Our team wanted to bridge the gap between Android and iOS just to see what it would be like. Our initial vision was simple but ambitious: provide users with the ability to run Android apps on iOS.\n\n\nOur eventual solution went a little bit further than apps: we built \na full Android environment running inside your iPhone\n. We even used the newest version of Android possible, the L developer preview!\n\n\nOur solution\n\n\nTo build our solution, we looked at current cloud gaming solutions. Products like NVIDIA's GameStream and Onlive can achieve extremely low latency and a near-native experience. All the user needs is an internet connection. We asked ourselves: what if we replace the game with a mobile phone OS? The controller becomes the touch screen and the phone's sensors. We can bring this near-native experience to mobile platforms.\n\n\nWe did just that, and it works great. \n\n\nPast solutions\n\n\nWe looked at existing solutions to this problem. There was an old port of Android for the iPhone 3G, but this required significant effort on the part of the user: jailbreaking and installing an alternative OS. This is unacceptably difficult for most people. There are ports of Android's Dalvik VM to iOS, but Android changes so rapidly that these apps would become obsolete within half a year. Apple's App Store policy also explicitly prohibits apps such as this, so you'd need to jailbreak your iPhone anyway.\n\n\nFuture vision\n\n\n\n\nHaving a \"phone inside your phone\" is extremely advantageous to developers. We plan to partner with app stores to allow users to try full versions of apps instantly with the push of a button instead of having to download a \"lite\" or free version.\n\n\nWe can allow interested users to \"buy\" a virtual phone of their own, allowing them to download and run any Android app they want to get a full Android experience, even on iOS devices.\n\n\n\n\nLimitations and solutions\n\n\n\n\nCurrently, our solution runs as a native iOS app backed by a server. This may not be practical because of Apple's relatively strict app approval process, so we plan to work around this with a robust HTML5 web client. Later, we will bring Android for All to Windows Phone, desktop and even Android platforms, allowing you to have a phone \"in the cloud\" with full synchronization across all your devices.\n\n\nMany sensors (camera, microphone) don't work just yet. Adding support for more sensors is trivial given more time, but we wanted to push out a polished product in the 36 hours we were allocated. Adding support for telephone is another challenge, but it's certainly possible. Our solution has full support for data, however.\n\n\nOur app requires an internet connection. With the mass proliferation of data plans and WiFi hotspots and further development of LTE technology, we don't imagine this to be an issue for much longer. \n\n\n\n\n\n\n\n\nBuilt With\n\n\nandroid\nios\njava\nobjective-c\npython"
    },
    {
        "url": "https://devpost.com/software/shower-with-friends",
        "content": "Not only is California in the middle of a severe drought, but clean water is a scarce resource worldwide.\n\n\nWe want to influence behavior change for responsible water usage via various mechanisms - quantified data (providing real-time feedback on water consumption), peer influence via social networks, and gamification.\n\n\nFor example, you would be alerted to your how long you were in the shower today, and how many gallons of water were used as compared to the previous day(s). You could enable sharing of this data with your circle of friends, and compete with each other. You could also challenge your friends (a la the Ice Bucket Challenge) to consume less (or as much) water as you did. \n\n\n\n\n\n\nBuilt With\n\n\nadafruit-flow-meter\nc\nd3.js\nduct-tape\nfirebase\nhardware\nintel-galileo\nmiscellaneous-plumbing-supplies\nnode.js\npython\ntwilio"
    },
    {
        "url": "https://devpost.com/software/power-glove-2-0-p4qlc",
        "content": "Power Glove 2.0 is an affordable and customizable virtual reality system, focused on finger and hand movement interaction with the computer. To be honest, this idea came up as I tried to think of an Mechanical Engineering project to start at an otherwise Computer Science hackathon. I was not expecting MHacks to be so CS-based, so I made the most of my knowledge and developed mechanical hardware to be used with CS systems. Target users are gamers searching for cheaper and reliable virtual reality hardware and independent developers seeking to draw more utility from modular hardware. Perhaps the key feature that I am most proud of in this hack is the sheer \"jerry-rigged\" nature of this hardware; the product was literally made from stuff lying around the MHacks hall and functions admirably in spite of that fact. Power Glove 2.0 still has a long way to go before even being a presentable piece of hardware, but with continued improvement and development I feel that being able to afford virtual reality hardware will be within our grasps.\n\n\n\n\n\n\nBuilt With\n\n\narduino\nwindows"
    },
    {
        "url": "https://devpost.com/software/super-smash-bros-64-kinect-controller",
        "content": "Super Smash Bros. 64 is a lot of fun to play with a controller, our project uses your body as a controller. Two Microsoft Kinect cameras are used to [Camera 1] Capture user specific commands (Kick, Punch, Special Move, Jump) and [Camera 2] is used to watch both players fight each other to play the game.\n\n\nPlayers walk up to [Camera 1] and record their moves, next they place a game character token over a camera running OpenCV to select their player. Once players are recognized the player selection field will light up; blue for player 1, red for player 2. Finally, players walk onto the playing floor in view of [Camera 2] and battle against each other in an intense Super Smash Bros. 64 competition.\n\n\n[Computer 1] Runs the Nintendo 64 Emulator and network server. [Computer 2] runs the OpenCV server to detect player tokens and issues commands to the Arduino to turn the lights and control the servos to kill the player that loses. [Computer 3] runs the Kinects to capture player movements and play the game. Game moves are sent from [Computer 3] to [Computer 1] through a TCP connection to control the characters in the game. At the end of each match [Computer 1] uses OpenCV to detect the player that won in order to tell [Computer 2] what player lost and therefore what piece to fling off the character selection board.\n\n\nDemo Video\n\nhttp://youtu.be/S6H04SD3SDI\n\n\nhttp://youtu.be/eUhwQlD0EJY?t=1h57m10s\n\n\n\n\n\n\nBuilt With\n\n\narduino\nbeaglebone-black\nc\nc#\nhardware\nkinect\nled\nopencv\npython\nservo\nspi\nweb\nwindows"
    },
    {
        "url": "https://devpost.com/software/blitz",
        "content": "Tired of burying your face in your phone on the go? What if there was a way to perform all your tedious tasks without having to go digging through each app, time after time. What if there was an easy way to perform these actions, in one tap, from anywhere on your phone! Introducing -- Blitz! \n\n\nBlitz allows you to create shortcuts for specific tasks that can be accessed from the notification centre of your iPhone. This is available for the first time with iOS8, and will allow users who frequently waste time on tedious tasks to use their phone with much more efficiency. Some examples of great shortcuts are calling an Uber to your home and seeing a price estimate, and a quick way to make recurring payments through Venmo.\n\n\n\n\n\n\nBuilt With\n\n\nios\nobjective-c\nsketch\nvenmo"
    },
    {
        "url": "https://devpost.com/software/clipq",
        "content": "If a picture is worth a Thousand Words, a Video is Worth a Million! This is the reason why we created ClipQ, an Android application that allows you to post questions in the form of video clips. By doing so, we hope that you can get more meaningful and contextual answers to your questions.\n\n\nThe app is targeted towards people who finds it increasingly difficult to convey complex questions in text or picture based formats.\n\n\nThe key feature of ClipQ is the ability to take photo, videos or even convert static images to a video clip (using the Moxtra Clip SDK).\n\n\n\n\n\n\nBuilt With\n\n\nandroid\neclipse\nmoxtra\nposgres\nruby-on-rails\ntwilio"
    },
    {
        "url": "https://devpost.com/software/backseat-driver-the-stickshift-learner-s-sidekick-vozxh",
        "content": "Today Roughly half the American population does not know how to drive a stick shift. This same demographic was equivalent to out group. This inspired us to utilize the data we could pull using some of the latest technology to promote learning in a new fashion. I am proud to say that our application \n\n\n\n\n\n\nBuilt With\n\n\nandroid\ngit\ngithub\ngoogle-maps\njava\nobd-ii-data\nopen-xc\nwindows\nwindows-phone\nxml"
    },
    {
        "url": "https://devpost.com/software/sign2line-pofh7",
        "content": "Modern translators can provide ways for people to translate speech of one foreign language into another. We thought it would be cool if we could do this for sign language, specifically American Sign Language (ASL).\n\n\nWe were inspired by the Leap Motion and its new beta version that has added more precision than ever before. We set out to create and identify custom gestures using JavaScript that our web application could recognize and subsequently print to the screen. It's beautiful! Sendgrid provided us with their awesome API so that we could email people with the latest translated sentences. We are proud to say that Sign2Line is a solid example of how the Leap Motion could be used in order to translate ASL into text.\n\n\n\n\n\n\nBuilt With\n\n\ncss\nhardware\nhtml\njavascript\njquery\nleap-motion\nmac\nphp\nsendgrid-api\nweb\nwindows\n\n\n\n\n\n\nTry it out\n\n\n\n\n\n\n\n\nsign2line.devan.im"
    },
    {
        "url": "https://devpost.com/software/project-refund",
        "content": "Receipts have always been a hassle  to carry around, and we thought that in this point in the future no one wants to carry around scraps of paper along with everything else, like phones, wallets, and purses. So we thought its about time to move receipts into the future and allow you to carry them with you in your smartphone and no longer be inconvenienced! \n\n\nGreat part about it, is that this will appeal to anyone who has to purchase anything. So we have a pretty wide variety of users that wouldn't mind to have easier lives.\n\n\nFeatures:\n\n\n\n\nReceipt Recognition - Transcribing text from a receipt to the app on your phone\n\n\nReceipt Storage - History of all previous receipts taken by the application\n\n\nTrends - In the future, the app will use data analytics to use receipts to track personal spending habits\n\n\n\n\n\n\n\n\nBuilt With\n\n\nandroid\nios\njava\nobjective-c\ntesseract"
    },
    {
        "url": "https://devpost.com/software/jump",
        "content": "Leap Motion has an ObjC library for interfacing with the controller, but no way to easily use it on iOS. \nJump\n proxies the existing local websocket server over a bonjour-served socket connection to waiting iOS devices that can then respond to gestures and events.\n\n\nI build a static library that encapsulates this functionality, so integrating the Leap into your iOS game is as easy as linking the library and adding some optional delegate methods for controlling the game.\n\n\nI also build a dynamic library that can be loaded onto jailbroken devices; using this dylib, you can control your phone with gestures. Pan through apps and webpages with a swipe or point and click your way through the interface. Regardless, you're now \nliving in the future\n.\n\n\n\n\n\n\nBuilt With\n\n\nios\nleap-motion\nmac\nobjc"
    },
    {
        "url": "https://devpost.com/software/civic-panda-permit-portal",
        "content": "We got a lot of great feedback from aspiring entrepreneurs, managers, and end-users who were struggling with the process. The key takeaway was clear. Users were looking for a consistent, reliable process that would point them to the resources they needed.\n\n\nWe focused on a restaurant entrepreneur as the primary user, as we understood that to be the most complex use case and therefore the most easily pared down to scale for other users.\n\n\nSee our full presentation\n\n\n\n\n\n\nBuilt With\n\n\nangular.js\nmagic\nweb\n\n\n\n\n\n\nTry it out\n\n\n\n\n\n\n\n\ngithub.com\n\n\n\n\n\n\n\n\ncivic-panda.herokuapp.com\n\n\n\n\n\n\n\n\ndrive.google.com"
    },
    {
        "url": "https://devpost.com/software/myo-music-mixer-3m-77f88c5f-bc45-4c46-ab05-89fd6993b5a0",
        "content": "When we got to HeroHacks, we had a ton of ideas however, we knew that we wanted to use the Myo controller. Out of all the ideas, we decided to go with the Myo Music Mixer project because we personally thought it would be the most fun to make. As this was Ashanks first hackathon, we did not have much experience with coding however, over the course of the hackathon he learned and coded quite a bit. Aakash used assets (images/music) that Ashank found online and put them together into this fluid web application.\n\n\nWe are most proud of the Myo/Leap integration. It was hard using both of the loops especially since the stack usually reached it's limit (a.k.a, it crashed all the time). Eventually, after much perseverance, we managed to get both the Myo and Leap working in the same application asynchronously. Another feature we are proud of us actually getting the Myo Javascript sdk working because there was not too much documentation to work with. We based our entire application on one single line output that was given to us in the Github repository.\n\n\n\n\n\n\nBuilt With\n\n\nhardware\nleap-motion\nmac\nmyo\nthree-js\nweb\nwindows\n\n\n\n\n\n\nTry it out\n\n\n\n\n\n\n\n\naakashadesara.github.io"
    },
    {
        "url": "https://devpost.com/software/findar",
        "content": "findAR is an augmented reality application created to make real-world search easier.\n\n\nfindAR analyzes the user's surrounding to preprocess images before they reach the eyes. The hardware for findAR include the Oculus Rift and a webcam mounted on the front. Video is processed with OpenCV before being fed to the Rift display.\n\n\nKey Features\n\n\n\n\nOculus Rift\n\n\n\n\nTrack and locate moving objects by color.\n\n\nDetect and recognize human faces.\n\n\n\n\nPebble smartwatch\n\n\n\n\nPortable control interface.\n\n\n\n\n\n\nBenefits\n\n\n\n\nSave time by processing only the highlighted visual information that you want.\n\n\nAvoid social faux pas and never forget the name to a face again.\n\n\n\n\nQuirks\n\n\n\n\nSee the world through grayscale, sepia, and various color filters.\n\n\n\n\n\n\n\n\nBuilt With\n\n\nhardware\noculus\nopencv\npebble\nwindows\n\n\n\n\n\n\nTry it out\n\n\n\n\n\n\n\n\ndev.quasi.co"
    },
    {
        "url": "https://devpost.com/software/athelas",
        "content": "We've built a low-cost lens attachment to the smartphone camera that images blood at high magnification. The attachment magnifies/focuses on the sample by means of a 1mm ball lens.  Most importantly, we've implemented computer vision to algorithmically count and identify cells in the bloodstream to automatically diagnose disease/conditions.\n\n\nFor more than 2 centuries, cell morphology - or the practice of viewing/analyzing a person's blood in order to diagnose conditions - has been the primary way to approach medicine. \n\n\nLiterally every facet of the medical world relies on blood cell analysis to diagnose conditions. Malaria, Chronic Diseases, Cancers, and all sorts of Parasites are all first detected when a physician manually recognizes the given cell type in your blood sample.\n\n\nYet, despite the critical nature of blood analysis to the medical industry - the process has hardly changed from its long, expensive form for 150 years: go to the doctor, get a large sample taken, wait for a couple days for a trained professional to analyze the blood, and then receive your report. Athelas changes all that.\n\n\nIn short - a malaria test that requires no expertise, takes a few seconds, and costs next to nothing. All on a smartphone - holding potential to save thousands of lives.\n\n\nFurthermore, through predictive cell counting, Athelas can mimic the process conducted in lab-grade environments in rural areas.\n\n\nThe product can benefit those in rural and suburban areas alike by providing faster and cheaper alternatives to existing diagnostic procedures. In rural areas, the tech will really shine - providing previously unavailable diagnostic skills through the power of artificial intelligence and computer vision.\n\n\n\n\n\n\nBuilt With\n\n\nandroid\nhardware\nios\nmac\nopencv\npython\nweb\n\n\n\n\n\n\nTry it out\n\n\n\n\n\n\n\n\nclipped.me"
    },
    {
        "url": "https://devpost.com/software/evoo",
        "content": "EVOO\n\n\nEVOO was inspired by a particularly harrowing cooking task -- a trip to Lake Tahoe with 13 people. We stayed in the same house, and as the hostess, I cooked every meal the entire time we were there. \n\n\nCooking for 2 is relatively easy. Cooking for 13 was a painstakingly stressful process. It was the pain-points from that trip -- hours of searching scalable/affordable recipes, accommodating for many different dietary restrictions, grocery shopping in large quantities, and complete lack of organization -- that inspired the app.\n\n\nIn cooking, the pairing knife is used for chopping, slicing, dicing, and fine movements across a wide variety of ingredients. It's incredibly versatile. I needed a mobile product that was as versatile as my pairing knife to help me before, during, and after the process of cooking.\n\n\n\"The Problem\"\nRecipe discovery is broken.\n-Search overwhelms the user and lacks direction\n-Browse is inspirational but lacks follow-through\n\n\nThe cooking process is fragmented. \n-There’s no single product to search, store, and take action on your recipe\n-Cooks individually aggregate across Evernote and/or Pinterest, neither of which is dedicated to storing and displaying recipe instructions\n-With recipe locations and types fragmented, the cook loses out on the opportunity to share recipes efficiently\n-Time is lost, ingredients are forgotten, and undoubtedly, side dishes are burned due to lack of streamlined instruction\n\n\n\"The Product\"\nEVOO's feature set is relatively slim, but we are particularly proud of our meal planning tool. Our meal planner allows the cook to search and filter by a variety of criteria to recommend the perfect recipe for you.\nMore of our feature set can be found in the screenshots attached.\n-Recipe outlines\n-Meal Planning tool\n-Curated collections of recipes for fun browsing\n-Step-by-step instructions for each recipe\n-In-app timers\n\n\n\n\n\n\nBuilt With\n\n\nadobe-illustrator\nbourbon\ncss3\nforge\ngithub\nhtml5\nios\njavascript\njson\nkimono\nmailchimp\nnormalizer\nobjective-c\nparse\nsketch\nx-code\n\n\n\n\n\n\nTry it out\n\n\n\n\n\n\n\n\nhttp://www.cookwithevoo.com"
    },
    {
        "url": "https://devpost.com/software/voicepedia",
        "content": "Voicepedia\n\n\nGIthub\n\n\n\n\n\n\nVoicepedia is an open source web application written in Go in the server-side and HTML5 technologies in the client side. You can use your voice to tell which Wikipedia entry to search for. No need to use your eyse, the entry's content will be reading to you!\n\n\nVoicepedia is also a social app, when you can record and hear voices by  people around the world!\n\n\nVoicepedia can run on desktop...\n\n\n\n\nOr on mobile phone...\n\n\n\n\nwith voice commands and fancy gestures...\n\n\n\n\nDraw \nR\n to listen recent articles read by real people\n\n\nDraw \nS\n to start searching\n\n\nDraw \n?\n to listen how to use Voicepedia\n\n\n\n\n\n\nor if you want to read this wikipedia and contribute, just say \"Record\".\n\n\n\n\nHow to install\n\n\nOpen source library used\n\n\n\n\nWikipedia extractor\n\n\nRethinkDB\n\n\nRedis\n\n\nGoji\n\n\nAnnyang\n\n\nRecordmp3js\n\n\nMouse Gesture Recognition\n\n\n\n\nDatabase\n\n\n\n\nStart RethinkDB and Redis\n\n\nDownload lastest wikipedia database at \nWikipedia database dump\n\n\nUncompress using bzip2 (44GB uncompressed)\n\n\nClean database using Wikipedia extractor (about 5 - 6 hours)\n\n\nImport database using scripts/import.go (about 4 hours)\n\n\nIndex database using scripts/index.go (about 4.5 hours)\n\n\n\n\nRunning\n\n\ngo get github.com/tamnd/voicewiki\ncd $GOPATH/src/github.com/tamnd/voicewiki\ngo build && ./voicewiki\n\n\n\nWhat includes in the source code?\n\n\n\n\nA index/search engine write from scratch (scripts/index.go, model/article/search.go)\n\n\nA light HTTP middleware (middleware)\n\n\nA gesture engine (/public/public/js/gesture.js, /public/public/js/pad.js)\n\n\n\n\nAnd much more...\n\n\nLicense\n\n\nUnder BSD license.\n\n\n\n\n\n\nBuilt With\n\n\ngestures\ngo\nrethinkdb\ntext-to-speech\nvoice-recognition\nweb\nwikipedia\n\n\n\n\n\n\nTry it out\n\n\n\n\n\n\n\n\ngithub.com"
    },
    {
        "url": "https://devpost.com/software/papyrus-natural-note-taking",
        "content": "Wouldn’t it be nice if you could give a presentation or show a drawing to others on a nearby TV or projector right from your phone or tablet? How about annotate on that presentation or draw while your audience is watching on the big screen? Now you can with \nPresentation Mode\n for Papyrus!\n\n\nPapyrus is a powerful and intuitive natural handwriting note-taking app with support for importing and marking up PDFs. With Papyrus you can take handwritten notes on your Android device and present them using HDMI®, MHL®, WirelessHD®, Miracast™, or any other display technology with dual screen capabilities introduced in Android 4.2 Jellybean.\n\n\nCreate a note from a PDF (slides, lecture template, mind maps, etc.) or various paper types. When you’re ready to present, connect your device to a secondary display and start a note presentation. Walk around, annotate, and even allow others to participate by passing around your device. Now anyone can write on the board right from their seat! With Presentation Mode in Papyrus, presenting is a snap.\n\n\nPresentation Mode\n\n\n\n\nPresent a note on a secondary display\n\n\nShow only the note’s content on the secondary display, without any of the app controls or \"chrome\"\n\n\nShow a splash screen while preparing a note, setting up a presentation, or switching between notes and other apps\n\n\nDisplay a note even while switching between apps (e.g. check email)\n\n\nUse Papyrus Presentation Mode in combination with Samsung Multi-Window mode to have multiple apps showing on your device screen at once, while only your Papyrus note is showing on the secondary display\n\n\n\n\nExample Presentation Scenarios\n\n\n\n\nIn a classroom, a teacher or student can import their presentation slides and walk around the room while presenting on the projector. They can highlight, fill in, and draw on slides with a pen or their finger during a lecture, then share a PDF of the presentation at the end of class. Increase student engagement by passing around the tablet to answer the teacher’s prompts.\n\n\nIn a business meeting, a team can pass around a tablet connected to the conference room TV and brainstorm ideas together. A work document or presentation can be annotated and marked-up during a meeting for all to see. At the end of the meeting, the note can be easily exported and shared with everyone.\n\n\nAt home, family and friends can play word and drawing games on the big screen. Play Pictionary® or Hangman without needing a giant white board or having to cram around a small piece of paper. \n\n\n\n\nKey Features\n\n\n\n\nTake notes naturally with a pen on active-pen enabled devices\n\n\nTake notes with your finger or passive stylus (non-active pen)\n\n\nVector graphics engine\n\n\nMultiple paper types and sizes\n\n\nImport a PDF as a note\n\n\nVarious tools for rich note-taking and markup (pen, highlighter, eraser, shapes, text)\n\n\nUndo/redo, select, move, and resize\n\n\nChange the color and weight of selected items\n\n\nCut, copy, and paste items between notes\n\n\nTwo finger scroll and pinch-to-zoom\n\n\nTwo finger double tap to quickly jump to a specific zoom level\n\n\nOrganize notes within notebooks (long press, drag and drop)\n\n\nSort notes and notebooks\n\n\nImage import, crop, and resize\n\n\nExport notes to PDF, PNG, or JPEG for printing, archiving, or sharing\n\n\nShare notes with friends and colleagues via email, Evernote, and other services\n\n\nBackup/restore and export notes as PDFs to cloud storage providers Dropbox and Box\n\n\nSamsung Multi-Window support\n\n\nShortcuts to create a new note or open a notebook\n\n\nPresent notes on a secondary display\n\n\n\n\nPapyrus takes special advantage of active pens on capable devices (e.g. Samsung Note) to provide natural, pressure sensitive handwriting as well as allow users to write with the pen and erase with their finger!\n\n\nGo beyond paper with \nPapyrus\n.\n\n\n\n\n\n\nBuilt With\n\n\nandroid\nandroid-studio\nwindows-phone\n\n\n\n\n\n\nTry it out\n\n\n\n\n\n\n\n\npapyrusapp.com\n\n\n\n\n\n\n\n\nplay.google.com"
    },
    {
        "url": "https://devpost.com/software/de-mobo-slides",
        "content": "de Mobo Slides\n uses the user’s smartphone as a remote control for Google Drive Presentations. Slides are shown on the phone along with the speaker notes while the presentation is displayed on the TV screen. This way, the user will not have to look back at the TV screen which diverts attention from the audience. The app includes a timer as well as multiple between-page/in-page transitions to choose from.\n\n\nUse volume controls on your wireless headset to navigate through the slides remotely.\nGoogle Drive Spreadsheet, dropbox and box.net supports are coming soon.\n\n\nde Mobo Slides\n is a presentation software application developed by \nde Mobo LLC.\n and is currently only available for MHL ready devices using a MHL/HDMI adapter.\n\n\nVisit \nhttp://www.demobo.com/product.html\n to see more of our multi-screen games and apps.\n\n\n\n\n\n\nBuilt With\n\n\nandroid\nfamo.us\ngoogle-drive-api\nios\npresentation-api\nweb\n\n\n\n\n\n\nTry it out\n\n\n\n\n\n\n\n\nwww.demobo.com"
    },
    {
        "url": "https://devpost.com/software/teacher-mod",
        "content": "This is a mobile application to help teachers find specific strategies for specific special needs in their classes, and to keep track of what works and what doesn’t work for each child. This application provides an innovative portfolio that builds on teachers’ wisdom over time around a student’s individual learning needs.  The app has three main functions- The Students, Strategies, and Classroom Assignments. Teachers are greeted with a list of students in their classrooms, and then can drill down to individual student information including which strategies have been used before by anyone working with this student.  The app provides data on the efficacy of each strategy both on an individual student basis as well as which strategies work best for which special needs.  It thus helps to increase effectiveness for the entire profession, and will help to reinvent teaching, so teachers can stop reinventing the wheel. \n\n\n\n\n\n\nBuilt With\n\n\nandroid\nangular.js\nionic\nios\nmac\nsalesforce\nwindows\n\n\n\n\n\n\nTry it out\n\n\n\n\n\n\n\n\ngoogle.com"
    },
    {
        "url": "https://devpost.com/software/tic-tac-close--3",
        "content": "What do we want on mobile devices? Social media and games! Thanks to Chatter social media is already available within Salesforce. Now it is time to engage the gamers and their competitive spirit! An unscientific observation of Subway riders in NYC indicated that users are listening to headphones and playing games on their mobile devices.\nThis insightful new game Tic Tac Close(TM) creates a new App Exchange category - CPG - Competition Performance Games(TM) for mobile users. CPG helps to make the sales the talk of the office! Imagine a bracket of competition just like the NCAA basketball tournament, World Cup, or Cricket that can be played each sales cycle. This can inspire competitive and team spirit within the organization!\n\n\nOptimized for the Salesforce1 Mobile Platform, pick a colleague and compete! Focus on closing opportunities with a friendly competition of a game! Imagine champions being announced in meetings rather than the once exciting dashboard discussions. \n\n\nCalled the game within the game, users upon closing opportunities get the opportunity to place an X or an O on the Tic Tac Close(TM) board against their competition. By winning the game, bragging rights are earned for the winner! In addition the management then can put together incentives for winning the game – gift certificates, shows, or incentive cash. \n\n\nThe premise is based on people becoming more engaged and motivated if there is a competition. With the Salesforce1 platform, now they can be engaged and informed throughout the sales cycle with the focus on immediate opportunity closure. Salesforce does a great job of focusing on closing the sale now it can be more fun and spur some rivalry!\n\n\nCheck out our Dashboards and Reports to see how the organization is doing at Tic Tac Close!\n\n\n\n\n\n\nBuilt With\n\n\nandroid\nios\nsalesforce"
    },
    {
        "url": "https://devpost.com/software/calligraphr",
        "content": "Our project improves the Chinese calligraphy learning experience with augmented reality. We project interactive character animations on a piece of real paper. User can simply trace down the character with a brush pen.\n\n\n\n\n\n\nBuilt With\n\n\nandroid\ngoogle-glass\nhardware\nopencv\nvuforia"
    },
    {
        "url": "https://devpost.com/software/deja-vu",
        "content": "The Story\n\n\nDeja Vu for iOS enhances your video experience to help you quickly find those memorable past times. Instead of scrolling through a long list of unrecognisable thumbnails, you can simply type in the name of the friend, words from your last bad jokes or even colors/landmarks you remember seeing. Unlike other services, Deja Vu does the tagging for you. \n\n\nWe achieve this by accessing various voice/image/facial recognition services, ranging from REST-ful APIs to open-source libraries. Facebook friends' faces are machine recognized, their words captioned, and even simple objects identified to provide search terms. Think of it as a personal search engine for everything in your phone. So, keep on shooting, and we'll be here to help you relive your best memories instantly.\n\n\nBoth the iOS app and server code is open source, they can be found at \nhttps://github.com/IRIS089/DejaVu\n and \nhttps://github.com/algobunny/djv\n respectively.\n\n\n\n\n\n\nBuilt With\n\n\ncelery\ndjango\nface++\nfacebook\nios\nkaltura\nrekognition\nstockpodium\nvoicebase\nweb\n\n\n\n\n\n\nTry it out\n\n\n\n\n\n\n\n\ngithub.com"
    },
    {
        "url": "https://devpost.com/software/watchpoint",
        "content": "WatchPoint was inspired by personal experiences from team members losing their expensive possessions such as their phones. We wanted to create an application that would remind you you were leaving your phone behind BEFORE you left it. This app will save average Americans hundreds of dollars and prevent them from losing sentimental things such as photos and videos. Our target user group is technologically inclined middle aged people, and more specifically males from the age of 16-35. We believe this group of people would use and benefit from our app the most because of previous research conducted by Business Insider. As a whole I can say that our team is most proud of the fact that our application will be able to solve a frustrating and time consuming problem with a simple and user friendly application for their smartphone. \n\n\n\n\n\n\nBuilt With\n\n\nandroid\nhardware\njava\npebble\ntizen\n\n\n\n\n\n\nTry it out\n\n\n\n\n\n\n\n\nteamwatchpoint.co"
    },
    {
        "url": "https://devpost.com/software/bar-cart",
        "content": "I have spent more than my share of time and money sipping drinks at dimly lit cocktail bars all over New York. I have entered speakeasies through phone booths and barbershops. I have made reservations weeks in advance and occasionally, I've waited hours for a coveted seat. Like many others, I love cocktails.\n\n\nHowever, I wanted an alternative to the hassle and expense of going to cocktail bars. So, I started studying the art and history of cocktails in my spare time and became a \"home mixologist\". The result of that was a large home bar, loads of cocktail books, and one huge spreadsheet. \n\n\nI realized that there are others like me who want to enjoy great cocktails at home but may not have the interest in doing all the legwork.\n\n\nBring the cocktail bar to you\n\n\nSo I created Bar Cart to make it easy to discover and make great cocktails at home. Browse our curated list of cocktails and easily order the necessary ingredients from liquor stores near you. You could be enjoying a high quality cocktail from the comfort of your own home in a matter of minutes.\n\n\n\n\n\n\nBuilt With\n\n\nafnetworking\ndelivery.com\nios\nobjective-c\n\n\n\n\n\n\nTry it out\n\n\n\n\n\n\n\n\nwww.barcartapp.com"
    },
    {
        "url": "https://devpost.com/software/fitmunch",
        "content": "Eating right is tough\n\n\nIt's so easy to grab a slice of pizza on the way home from work.  That daily slice of pizza will quickly add up to pounds around the waist.  Experts say that eating healthy is 75% of the equation to losing weight.  But how do you eat healthy?  \n\n\nWho has time to cook?\n\n\nCooking with healthy ingredients has always been the surefire way to a good diet.  But who has time for that?  There just isn't enough time to cook while staying late at work, going to the gym, seeing friends, taking your dog for a walk, and watching the latest episodes of your favorite show.  And even if there is time, who feels like putting in the effort?\n\n\nFitMunch brings healthy food with the ease of delivery\n\n\nFitMunch uses a mixture of nutritionist knowledge and advanced machine learning algorithms to find delicious healthy meals at local restaurants.   Users can select some simple search criteria and will receive a list of curated healthy meals.  Each meal option comes with information as to why it is healthy, such as low carb, low sugar, low calorie, etc..  These meals can be ordered directly through FitMunch using delivery.com's API.  It shouldn't be this easy to eat healthy!\n\n\n\n\n\n\nBuilt With\n\n\nbootstrap\njquery\nweb\n\n\n\n\n\n\nTry it out\n\n\n\n\n\n\n\n\nm.fitmunch.com"
    },
    {
        "url": "https://devpost.com/software/basket",
        "content": "The laundry scheduling experience at delivery.com is backwards. Users must first pick a store, then pick items, and finally figure out scheduling.\n\n\nThis forces users to work around a particular store’s timings and quirks. \n\n\nBasket is an iOS app that makes laundry scheduling quick and simple by working around the user’s needs.\n\n\nCheck out the demo video to see how it works.\n\n\n\n\n\n\nBuilt With\n\n\ndelivery.com\nfacebook-pop\nios\nobjective-c"
    },
    {
        "url": "https://devpost.com/software/incentivizr-make-likes-matter",
        "content": "Advertising as the sole revenue source for content platforms creates a culture of “clickbait” articles, encouraging content creators and editors to create stories that will attract a lot of attention and clicks, but not necessarily by virtue of the story's quality. Separate from that, the advertising model provides no attachment to your content for consumers, as the advertising is rarely, if ever, related to the content platform owner itself.\n\n\nEnter: Incentivizr\n\n\nIncentivizr \nextends\n this model with a layer of user engagement that rewards content creators directly. It is designed for content platforms that have existing (paid) subscribers, but can be used with a recurring donation model as well.\n\n\nIncentivizr repurposes a fraction of a user's paid subscription as \nreward currency\n which the user can then give to pieces of content she/he want to encourage and reward.\n\n\nThe key benefit is more psychological than monetary: the financial amount being awarded to content creators is more a “bonus” to their salary than a source of income, but it helps give them reliable insight into what content their readers/listeners \ntruly\n appreciate. Similarly, the user feels a stronger sense of ‘ownership’ over the publication as she/he gets to have a say over which pieces of content are rewarded financially.\n\n\nAdditionally, users are given the opportunity to voluntarily \nincrease\n the amount of money that goes into their subscription for distribution to content creators, creating an opportunity for them to express stronger appreciation for great content. This further helps encourage content creators to focus on writing great content\n\n\nThe result is that (paid) users are more engaged with the content because they'll have a sense of ownership over what gets rewarded, and content creators are encouraged to focus on quality more, and clickbait, less.\n\n\nIncentivizr facilitates newsrooms to exercise the integrity and editorial quality they possess by reducing their dependency on ad revenues, by helping them understand what users truly value, and by helping them focus on creating \ngreat content\n.\n\n\n\n\n\n\nBuilt With\n\n\njquery\nweb\n\n\n\n\n\n\nTry it out\n\n\n\n\n\n\n\n\nrjihacks.farukat.es"
    },
    {
        "url": "https://devpost.com/software/smiley-slider",
        "content": "Super simple photo based feedback aimed at younger children.\n\n\nThe process for using our app would be as follows.\n\n\n\n\nhospital staff can take pictures of things they want feedback on around the hospital.\n\n\nthen then upload these pictures to the app.\n\n\nchildren can use a super simple mechanism to share their feelings on each picture\n\n\n\n\nCheck it out on github here => \nhttps://github.com/markdurrant/Smiley-Slider\n\n\n\n\n\n\nBuilt With\n\n\nbrowser\njquery\nweb\n\n\n\n\n\n\nTry it out\n\n\n\n\n\n\n\n\ngithub.com"
    },
    {
        "url": "https://devpost.com/software/fast-food-runner",
        "content": "When we first got together as a group, we had no real solid plan on what to program.  The first idea that we had was an app/web application which would cater to the musically inclined audience, specifically DJ's and their listeners at a live set.  However, towards midnight, we had settled on an idea for a web application which would be silly, and somewhat useless, but awesome.  That idea is what we eventually stuck with, and when the application was finished, we had something which would allow the user to put in the type of nutrition info they were craving and their budget, and would spit out a restaurant and a compiled menu which packs the most of that specific \"nutrition\" for the person's budget.  In addition to the compiled menu, we also included a feature which would map the user to the restaurant, with it's location based off of the amount of calories they would have to burn to even out after eating all the menu items.\n\n\nWe have no target user, as everyone has food cravings.  As for our web application's interface and inner workings, we are proud of it's conceptual simplicity and user interface design, as well as the algorithm which was able to compile a list of the menu items which would maximize on the specific \"nutrition\", while keeping within the budget of the user.  \n\n\n\n\n\n\nBuilt With\n\n\nfast-food-nutrition-data\ngoogle-maps\njavascript\njson\npython\nweb\n\n\n\n\n\n\nTry it out\n\n\n\n\n\n\n\n\ncalvinlin.github.io"
    },
    {
        "url": "https://devpost.com/software/which",
        "content": "We wanted to help people easily make decisions. We wanted to make it as most interactive with great user experience. We worked really hard on making mock ups on an app people will fall in love.\n\n\n\n\n\n\nBuilt With\n\n\nandroid\nparse\n\n\n\n\n\n\nTry it out\n\n\n\n\n\n\n\n\nwww.google.com"
    },
    {
        "url": "https://devpost.com/software/click-shot",
        "content": "Control your camera with the remote V.BTTN! Take group photos, action shots, and videos. Trim and merge your clips to create seamless movies! \n\n\nComing out soon for iOS.\n\n\n\n\n\n\nBuilt With\n\n\nhardware\nios\n\n\n\n\n\n\nTry it out\n\n\n\n\n\n\n\n\nwww.testflightapp.com"
    },
    {
        "url": "https://devpost.com/software/tira",
        "content": "Hello Appcelerator team,\n\n\nMy name is Misha Peric, and I am the owner/developer from Byteout.com that created the Tira app we are submitting. \n\n\nWe have been using Titanium for native mobile development for couple of years now and created many applications with it. Recently we decided to start using more animations in our apps so we tested the Titanium.\n\n\nAnd it performed beautifully! As you will see from video demo the animations we accomplished are quite advanced and creating them surprisingly was not that hard.\n\n\nWhen we realized we cant do easings in Android animations, we searched the Titanium marketplace and found the free module that does them. And thats quite awesome.\n\n\nBesides a lot of animations, we had to integrate with Jira API, and as always Titanium was quite powerful in that regard too.\n\n\nCouple of years ago I used Cocoa Touch for iOS development, but since I switched to Titanium I never looked back. For example, layout manager in Titanium is something that keeps impressing me, and in this project I again was astonished how complex layouts are easy to accomplish with Titanium/Alloy views.\n\n\nWe plan to use this application ourself and to expand it with many more features. But we will of course put it on both stores and see how community likes it.\n\n\nThank you,\nMisha Peric\n\n\n\n\n\n\nBuilt With\n\n\nalloy\nandroid\nios\ntitanium\n\n\n\n\n\n\nTry it out\n\n\n\n\n\n\n\n\nwww.byteout.com"
    },
    {
        "url": "https://devpost.com/software/it-gon-rain",
        "content": "Enter your zip code and Ollie Williams will read you the forecast. If you can't understand Ollie, we convert the amount of rain to a comparable funding round from Crunchbase.\n\n\n\n\n\n\nBuilt With\n\n\ncrunchbase\nmailjet\nweather-underground\n\n\n\n\n\n\nTry it out\n\n\n\n\n\n\n\n\nitgonrain.herokuapp.com"
    },
    {
        "url": "https://devpost.com/software/vrban",
        "content": "VRBAN allows users to explore urban environments using the Oculus Rift. Using Esri's CityEngine, 3D models of cities can be generated, edited, and updated using 2D GIS data as input. These 3D models can then be viewed and explored in VRBAN using the Oculus Rift. VRBAN also allows for collaborative planning of proposed buildings by allowing external users to  vary settings such as sunlight, point of view, and location of proposed buildings. This allows the user with the Oculus to observe the impact of different decisions in an urban plan, while his or her teammates collaboratively work on the plan. \n\n\n\n\n\n\nBuilt With\n\n\ncityengine\nesri\nfirebase\nnode.js\noculus\nweb\n\n\n\n\n\n\nTry it out\n\n\n\n\n\n\n\n\nvrban.io"
    },
    {
        "url": "https://devpost.com/software/indulge",
        "content": "Indulge helps you experience colors by organizing your nail polish adventures. The goal is simple: never forget another nail polish name. Just Mark. Scan. Snap. Share. Indulge makes your nail experience effortless, simple and fun. \n\n\n\n\n\n\nBuilt With\n\n\nfoursquare\nios\n\n\n\n\n\n\nTry it out\n\n\n\n\n\n\n\n\nbit.ly\n\n\n\n\n\n\n\n\nindulgeapp.net"
    },
    {
        "url": "https://devpost.com/software/shoomai",
        "content": "Shoomai is a iPhone application that helps you find a delicious dimsum dish.\n\n\n\n\n\n\nBuilt With\n\n\nios\nsketch"
    },
    {
        "url": "https://devpost.com/software/tred-virtual-reality-immersion-experience",
        "content": "Our project is a platform that enters you into the virtual reality experience of whatever alternate reality you're experiencing - walk onto the platform and close the restraining wall to begin. Put on any virtual reality headset like the Oculus VR and start experiencing true virtual reality - walk without a controller. Tred has a layer of sensors that picks up pressure differentials and sends this to a Windows machine.\n\n\nThe accompanying software receives sensor data from a microcontroller and runs a k-means clustering algorithm with 2 clusters on the pressure points to approximate the location of the feet, then takes a low pass filter on the movement of the feet to get rid of noise, and uses this vector as the movement vector. Coupled with sensor data from the Rift, Tred looks like a keyboard to your Windows computer, and so works with any virtual reality experience that uses the Rift. \n\n\nHacked by 3 Terrapin Hackers from the University of Maryland.\n\n\n\n\n\n\nBuilt With\n\n\narduino\nhardware\noculus\npython\nunity\nwindows"
    },
    {
        "url": "https://devpost.com/software/colorchase",
        "content": "Do you think playing games will help you think faster? If so, our game will put your brain flexibility and speed to the test. ColorChase is a brain training game that incorporates the concept of matching colors with their meaning and the universally-recognized game of Pac-Man. Brain training games can improve teens' cognitive function. Since Pac-Man is a very popular game, we have decided to bring it back with a twist--our game will bring Pac-Man back and get your brain on track. (Roxy Anita Gabby Niqeel Tim)\n\n\n\n\n\n\nBuilt With\n\n\nadobe-illustrator\njavascript\nphotoshop\ntge\ntresensa\nwindows\n\n\n\n\n\n\nTry it out\n\n\n\n\n\n\n\n\ntimotius02.github.io"
    },
    {
        "url": "https://devpost.com/software/nextwave",
        "content": "A microwave of things! We created a WiFi controlled microwave that runs off an Android app with crowdsourced cooking times. A user scans a barcode (or searches if no barcode is present), and is given a cooking time, which they can adjust to personal preference. The microwave automatically opens and the user deposits the food. After closing the door, the microwave cooking time is automatically set, and promptly opens the doors upon finishing. \n\n\n\n\n\n\nBuilt With\n\n\nandroid\nc#\nfirebase\nhardware\nmicrowave\nparticle\npebble\nservo\nweb\nzxing\n\n\n\n\n\n\nTry it out\n\n\n\n\n\n\n\n\nkashev.github.io"
    },
    {
        "url": "https://devpost.com/software/sphinx",
        "content": "Built With\n\n\nweb"
    },
    {
        "url": "https://devpost.com/software/the-homework-machine",
        "content": "We've made every kid's dream -- a machine that does your homework for you! We set out on our first hardware hack to create a system that mimics your handwriting, can understand worksheets, and solves your homework with a pen in your handwriting.\n\n\n\n\n\n\nBuilt With\n\n\narduino\nhardware\nmac\nopencv\npython\ntesseract\nwindows"
    },
    {
        "url": "https://devpost.com/software/workflow",
        "content": "Workflow brings the productivity of a desktop computer to your iPad. Both power users and new users alike can drag and drop to create powerful workflows and automate daily tasks.\n\n\n\n\n\n\nBuilt With\n\n\nblood\nfilepicker.io\nios\nobjective-c\nsweat\ntears\n\n\n\n\n\n\nTry it out\n\n\n\n\n\n\n\n\nmy.workflow.is"
    },
    {
        "url": "https://devpost.com/software/smart-send",
        "content": "Our API allows users to send text messages to anyone in the world and specify a time that all of the recipients will receive the message at their local time. This is beneficial because if you are texting someone on the other side of the world. They will most likely receive it at midnight and they will not notice the message or advertisement. We created an app that demonstrates the various features of the API.\n\n\n\n\n\n\nBuilt With\n\n\nallareacodes\nandroid\nhardware\nios\nmac\nmongodb\nnexmo\npython\nweb\nwindows\nwindows-phone"
    },
    {
        "url": "https://devpost.com/software/postpushr",
        "content": "https://github.com/yasyf/PostPushr\n\n\nWhat?\n\n\nYou can have 9 email accounts all at once, but there's still something so wonderfully refreshing about getting an envelope addressed to you in the mail. On the other hand, email is just so damn convenient. That's where our app comes in.\n\n\nWhy?\n\n\nPostPushr combines postal delivery and cloud printing in one simplified service for general personal use. Bid farewell to fumbling through cluttered drawers for stamps and making errant stops at the post office. With PostPushr, all you have to do is open up your email, type your message in the body, and hit send. BOOM! With the power of Lob, Sendgrid, Stripe, Google Maps, MongoDB, and NameCheap, your email will end up as a letter in your recipient's mailbox!\n\n\nWhen?\n\n\nImagine: you're here at MHacks and left your spouse/partner/significant other back at home. How romantic it would be to send them a love letter, from amidst the chaos of sleep deprivation and junk food galore, without having to brave the cold and walk to a post office! But if that prospect left you swooning, check out the second part of our project. With our mobile app, just take a picture and your selfie can be immediately sent...as a postcard! Our mobile app will generate a digital postcard with the picture, then print it out and mail it--like a physical, tangible Snapchat. Now people don't even have to feel bad about screenshotting your beautiful face, they can just frame the postcard they get. Your loved one can see you \"Wish they were here!\" as you drown in your StackOverflow and snack overflows.\n\n\nHow?\n\n\nInstructions for Using PostPushr Web:\n\n\n\n\nwww.postpushr.com\n \n\n\nRegister an account. \n\n\nOpening up a new message, enter the name of your recipient in front of @letter.postpushr.com, with underscores substituted for spaces. For example, if I wanted to send a letter to David, I would email \nDavid@letter.postpushr.com\n. If I wanted to mail Aneesh Agrawal, I would email \nAneesh_Agrawal@letter.postpushr.com\n. \n\n\nType a valid address in the subject line. ex: 318 Memorial Street, Cambridge, MA 12169 \n\n\nEnter the text of your letter in the email body. \n\n\nHit send, and wait for your lucky recipient to get a delightful surprise in the mail!\n\n\n\n\n\n\n\n\nBuilt With\n\n\nandroid\nflask\ngoogle-maps\nios\nlob\nmac\nmongodb\nnamecheap\npython\nsendgrid\nstripe\nweb\nwindows\nwindows-phone\n\n\n\n\n\n\nTry it out\n\n\n\n\n\n\n\n\nwww.postpushr.com"
    },
    {
        "url": "https://devpost.com/software/greencan",
        "content": "A trashcan that sorts items into recyclables and non-recyclables based on the sound that the item makes as it enters the trashcan."
    }
]